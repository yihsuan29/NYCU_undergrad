{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8de7e718",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-12-16T17:22:14.892506Z",
     "iopub.status.busy": "2023-12-16T17:22:14.891836Z",
     "iopub.status.idle": "2023-12-16T17:22:15.934586Z",
     "shell.execute_reply": "2023-12-16T17:22:15.933810Z",
     "shell.execute_reply.started": "2023-12-16T07:09:01.891940Z"
    },
    "papermill": {
     "duration": 1.091216,
     "end_time": "2023-12-16T17:22:15.934748",
     "exception": false,
     "start_time": "2023-12-16T17:22:14.843532",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import glob\n",
    "import os\n",
    "import gc\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "from sklearn import preprocessing, model_selection\n",
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler,LabelEncoder\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "import numpy.matlib\n",
    "\n",
    "\n",
    "path_submissions = '/'\n",
    "\n",
    "target_name = 'target'\n",
    "scores_folds = {}\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0df649e1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-16T17:22:16.016579Z",
     "iopub.status.busy": "2023-12-16T17:22:16.005374Z",
     "iopub.status.idle": "2023-12-16T17:22:45.609243Z",
     "shell.execute_reply": "2023-12-16T17:22:45.608664Z",
     "shell.execute_reply.started": "2023-12-16T07:09:37.624071Z"
    },
    "papermill": {
     "duration": 29.641873,
     "end_time": "2023-12-16T17:22:45.609425",
     "exception": false,
     "start_time": "2023-12-16T17:22:15.967552",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.7.10\r\n",
      "\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!python --version\n",
    "!pip -q install /kaggle/input/d/ryati131457/pytorchtabnet/pytorch_tabnet-2.0.1-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45372d50",
   "metadata": {
    "papermill": {
     "duration": 0.032139,
     "end_time": "2023-12-16T17:22:45.674966",
     "exception": false,
     "start_time": "2023-12-16T17:22:45.642827",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "* ### I just tried different parameters and weights of 2  models. Cross validation with different folds was also attempted, but failed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d952fb45",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-16T17:22:45.774162Z",
     "iopub.status.busy": "2023-12-16T17:22:45.757916Z",
     "iopub.status.idle": "2023-12-16T17:22:45.822274Z",
     "shell.execute_reply": "2023-12-16T17:22:45.822729Z",
     "shell.execute_reply.started": "2023-12-15T05:18:56.295420Z"
    },
    "papermill": {
     "duration": 0.112471,
     "end_time": "2023-12-16T17:22:45.822880",
     "exception": false,
     "start_time": "2023-12-16T17:22:45.710409",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# data directory\n",
    "data_dir = '../input/optiver-realized-volatility-prediction/'\n",
    "\n",
    "# Function to calculate first WAP\n",
    "def calc_wap1(df):\n",
    "    wap = (df['bid_price1'] * df['ask_size1'] + df['ask_price1'] * df['bid_size1']) / (df['bid_size1'] + df['ask_size1'])\n",
    "    return wap\n",
    "\n",
    "# Function to calculate second WAP\n",
    "def calc_wap2(df):\n",
    "    wap = (df['bid_price2'] * df['ask_size2'] + df['ask_price2'] * df['bid_size2']) / (df['bid_size2'] + df['ask_size2'])\n",
    "    return wap\n",
    "\n",
    "def calc_wap3(df):\n",
    "    wap = (df['bid_price1'] * df['bid_size1'] + df['ask_price1'] * df['ask_size1']) / (df['bid_size1'] + df['ask_size1'])\n",
    "    return wap\n",
    "\n",
    "def calc_wap4(df):\n",
    "    wap = (df['bid_price2'] * df['bid_size2'] + df['ask_price2'] * df['ask_size2']) / (df['bid_size2'] + df['ask_size2'])\n",
    "    return wap\n",
    "\n",
    "# Function to calculate the log of the return\n",
    "# Remember that logb(x / y) = logb(x) - logb(y)\n",
    "def log_return(series):\n",
    "    return np.log(series).diff()\n",
    "\n",
    "# Calculate the realized volatility\n",
    "def realized_volatility(series):\n",
    "    return np.sqrt(np.sum(series**2))\n",
    "\n",
    "#我加的===================================================================\n",
    "\n",
    "def RSI(close,n):\n",
    "    delta = close.diff()\n",
    "    delta = delta[1:]\n",
    "    pricesUp = delta.copy()\n",
    "    pricesDown = delta.copy()\n",
    "    pricesUp[pricesUp < 0] = 0\n",
    "    pricesDown[pricesDown > 0] = 0\n",
    "    rollUp = pricesUp.rolling(n).mean()\n",
    "    rollDown = pricesDown.abs().rolling(n).mean()\n",
    "    rs = rollUp / rollDown\n",
    "    rsi = 100.0 - (100.0 / (1.0 + rs))\n",
    "    return rsi\n",
    "\n",
    "#========================================================================\n",
    "\n",
    "# Function to count unique elements of a series\n",
    "def count_unique(series):\n",
    "    return len(np.unique(series))\n",
    "\n",
    "# Function to read our base train and test set\n",
    "def read_train_test():\n",
    "    train = pd.read_csv('../input/optiver-realized-volatility-prediction/train.csv')\n",
    "    test = pd.read_csv('../input/optiver-realized-volatility-prediction/test.csv')\n",
    "#     train = train[:100]\n",
    "#     test=test[:100]\n",
    "    # Create a key to merge with book and trade data\n",
    "    train['row_id'] = train['stock_id'].astype(str) + '-' + train['time_id'].astype(str)\n",
    "    test['row_id'] = test['stock_id'].astype(str) + '-' + test['time_id'].astype(str)\n",
    "    \n",
    "    print(f'Our training set has {train.shape[0]} rows')\n",
    "    return train, test\n",
    "\n",
    "# Function to preprocess book data (for each stock id)\n",
    "def book_preprocessor(file_path):\n",
    "    df = pd.read_parquet(file_path)\n",
    "    # Calculate Wap\n",
    "    df['wap1'] = calc_wap1(df)\n",
    "    df['wap2'] = calc_wap2(df)\n",
    "    df['wap3'] = calc_wap3(df)\n",
    "    df['wap4'] = calc_wap4(df)\n",
    "    # Calculate log returns\n",
    "    df['log_return1'] = df.groupby(['time_id'])['wap1'].apply(log_return)\n",
    "    df['log_return2'] = df.groupby(['time_id'])['wap2'].apply(log_return)\n",
    "    df['log_return3'] = df.groupby(['time_id'])['wap3'].apply(log_return)\n",
    "    df['log_return4'] = df.groupby(['time_id'])['wap4'].apply(log_return)\n",
    "    # Calculate wap balance\n",
    "    df['wap_balance'] = abs(df['wap1'] - df['wap2'])\n",
    "    # Calculate spread\n",
    "    df['price_spread'] = (df['ask_price1'] - df['bid_price1']) / ((df['ask_price1'] + df['bid_price1']) / 2)\n",
    "    df['price_spread2'] = (df['ask_price2'] - df['bid_price2']) / ((df['ask_price2'] + df['bid_price2']) / 2)\n",
    "    df['bid_spread'] = df['bid_price1'] - df['bid_price2']\n",
    "    df['ask_spread'] = df['ask_price1'] - df['ask_price2']\n",
    "    df[\"bid_ask_spread\"] = abs(df['bid_spread'] - df['ask_spread'])\n",
    "    df['total_volume'] = (df['ask_size1'] + df['ask_size2']) + (df['bid_size1'] + df['bid_size2'])\n",
    "    df['volume_imbalance'] = abs((df['ask_size1'] + df['ask_size2']) - (df['bid_size1'] + df['bid_size2']))\n",
    "    \n",
    "    # Dict for aggregations\n",
    "    create_feature_dict = {\n",
    "        'wap1': [np.sum, np.mean, np.std],\n",
    "        'wap2': [np.sum, np.mean, np.std],\n",
    "        'wap3': [np.sum, np.mean, np.std],\n",
    "        'wap4': [np.sum, np.mean, np.std],\n",
    "        'log_return1': [np.sum, realized_volatility, np.mean, np.std],\n",
    "        'log_return2': [np.sum, realized_volatility, np.mean, np.std],\n",
    "        'log_return3': [np.sum, realized_volatility, np.mean, np.std],\n",
    "        'log_return4': [np.sum, realized_volatility, np.mean, np.std],\n",
    "        'wap_balance': [np.sum, np.mean, np.std],\n",
    "        'price_spread':[np.sum, np.mean, np.std],\n",
    "        'price_spread2':[np.sum, np.mean, np.std],\n",
    "        'bid_spread':[np.sum, np.mean, np.std],\n",
    "        'ask_spread':[np.sum, np.mean, np.std],\n",
    "        'total_volume':[np.sum, np.mean, np.std],\n",
    "        'volume_imbalance':[np.sum, np.mean, np.std],\n",
    "        \"bid_ask_spread\":[np.sum, np.mean, np.std],\n",
    "    }\n",
    "    create_feature_dict_time = {\n",
    "        'log_return1': [realized_volatility],\n",
    "        'log_return2': [realized_volatility],\n",
    "        'log_return3': [realized_volatility],\n",
    "        'log_return4': [realized_volatility],\n",
    "    }\n",
    "    \n",
    "    # Function to get group stats for different windows (seconds in bucket)\n",
    "    def get_stats_window(fe_dict,seconds_in_bucket, add_suffix = False):\n",
    "        # Group by the window\n",
    "        df_feature = df[df['seconds_in_bucket'] >= seconds_in_bucket].groupby(['time_id']).agg(fe_dict).reset_index()\n",
    "        # Rename columns joining suffix\n",
    "        df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n",
    "        # Add a suffix to differentiate windows\n",
    "        if add_suffix:\n",
    "            df_feature = df_feature.add_suffix('_' + str(seconds_in_bucket))\n",
    "        return df_feature\n",
    "    \n",
    "    # Get the stats for different windows\n",
    "    df_feature = get_stats_window(create_feature_dict,seconds_in_bucket = 0, add_suffix = False)\n",
    "    df_feature_500 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 500, add_suffix = True)\n",
    "    df_feature_400 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 400, add_suffix = True)\n",
    "    df_feature_300 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 300, add_suffix = True)\n",
    "    df_feature_200 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 200, add_suffix = True)\n",
    "    df_feature_100 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 100, add_suffix = True)\n",
    "\n",
    "    # Merge all\n",
    "    df_feature = df_feature.merge(df_feature_500, how = 'left', left_on = 'time_id_', right_on = 'time_id__500')\n",
    "    df_feature = df_feature.merge(df_feature_400, how = 'left', left_on = 'time_id_', right_on = 'time_id__400')\n",
    "    df_feature = df_feature.merge(df_feature_300, how = 'left', left_on = 'time_id_', right_on = 'time_id__300')\n",
    "    df_feature = df_feature.merge(df_feature_200, how = 'left', left_on = 'time_id_', right_on = 'time_id__200')\n",
    "    df_feature = df_feature.merge(df_feature_100, how = 'left', left_on = 'time_id_', right_on = 'time_id__100')\n",
    "    # Drop unnecesary time_ids\n",
    "    df_feature.drop(['time_id__500','time_id__400', 'time_id__300', 'time_id__200','time_id__100'], axis = 1, inplace = True)\n",
    "    \n",
    "    #我加的===================================================================\n",
    "    df_new_features = df[['time_id']].copy()\n",
    "    df['mid'] = (df['ask_price1']+ df['bid_price1'])/2\n",
    "    #STD(price)\n",
    "    for window in [100,200,300,400,500]:\n",
    "        df[f'mid_ma_{window}'] = df['mid'].rolling(window=window).mean()\n",
    "        #|price-MA|\n",
    "        df[f'price_to_ma_{window}'] = abs(df['mid'] - df[f'mid_ma_{window}'])\n",
    "        df[f'price_to_ma_{window}'].bfill(inplace=True)\n",
    "        df_new_features[f'price_to_ma_{window}'] = df.groupby('time_id')[f'price_to_ma_{window}'].transform('mean')\n",
    "    for n in [30,60,100]:\n",
    "        df[f'rsi_{n}']=RSI(df['mid'],n=n)\n",
    "        df[f'rsi_{n}'].bfill(inplace=True)\n",
    "        df_new_features[f'rsi_{n}'] = df.groupby('time_id')[f'rsi_{n}'].transform('mean')\n",
    "    df_new_features.drop_duplicates(subset='time_id', keep='first', inplace=True)     \n",
    "    df_feature = df_feature.merge(df_new_features, how = 'left', left_on = 'time_id_', right_on = 'time_id')\n",
    "    df_feature.drop(['time_id'], axis = 1, inplace = True)\n",
    "    #我加的===================================================================\n",
    "    \n",
    "    # Create row_id so we can merge\n",
    "    stock_id = file_path.split('=')[1]\n",
    "    df_feature['row_id'] = df_feature['time_id_'].apply(lambda x: f'{stock_id}-{x}')\n",
    "    df_feature.drop(['time_id_'], axis = 1, inplace = True)\n",
    "    return df_feature\n",
    "\n",
    "# Function to preprocess trade data (for each stock id)\n",
    "def trade_preprocessor(file_path):\n",
    "    df = pd.read_parquet(file_path)\n",
    "    df['log_return'] = df.groupby('time_id')['price'].apply(log_return)\n",
    "    df['amount']=df['price']*df['size']\n",
    "    # Dict for aggregations\n",
    "    create_feature_dict = {\n",
    "        'log_return':[realized_volatility],\n",
    "        'seconds_in_bucket':[count_unique],\n",
    "        'size':[np.sum, np.max, np.min],\n",
    "        'order_count':[np.sum,np.max],\n",
    "        'amount':[np.sum,np.max,np.min],\n",
    "    }\n",
    "    create_feature_dict_time = {\n",
    "        'log_return':[realized_volatility],\n",
    "        'seconds_in_bucket':[count_unique],\n",
    "        'size':[np.sum],\n",
    "        'order_count':[np.sum],\n",
    "    }\n",
    "    # Function to get group stats for different windows (seconds in bucket)\n",
    "    def get_stats_window(fe_dict,seconds_in_bucket, add_suffix = False):\n",
    "        # Group by the window\n",
    "        df_feature = df[df['seconds_in_bucket'] >= seconds_in_bucket].groupby(['time_id']).agg(fe_dict).reset_index()\n",
    "        # Rename columns joining suffix\n",
    "        df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n",
    "        # Add a suffix to differentiate windows\n",
    "        if add_suffix:\n",
    "            df_feature = df_feature.add_suffix('_' + str(seconds_in_bucket))\n",
    "        return df_feature\n",
    "    \n",
    "\n",
    "    # Get the stats for different windows\n",
    "    df_feature = get_stats_window(create_feature_dict,seconds_in_bucket = 0, add_suffix = False)\n",
    "    df_feature_500 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 500, add_suffix = True)\n",
    "    df_feature_400 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 400, add_suffix = True)\n",
    "    df_feature_300 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 300, add_suffix = True)\n",
    "    df_feature_200 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 200, add_suffix = True)\n",
    "    df_feature_100 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 100, add_suffix = True)\n",
    "    \n",
    "    def tendency(price, vol):    \n",
    "        df_diff = np.diff(price)\n",
    "        val = (df_diff/price[1:])*100\n",
    "        power = np.sum(val*vol[1:])\n",
    "        return(power)\n",
    "    \n",
    "    lis = []\n",
    "    for n_time_id in df['time_id'].unique():\n",
    "        df_id = df[df['time_id'] == n_time_id]        \n",
    "        tendencyV = tendency(df_id['price'].values, df_id['size'].values)      \n",
    "        f_max = np.sum(df_id['price'].values > np.mean(df_id['price'].values))\n",
    "        f_min = np.sum(df_id['price'].values < np.mean(df_id['price'].values))\n",
    "        df_max =  np.sum(np.diff(df_id['price'].values) > 0)\n",
    "        df_min =  np.sum(np.diff(df_id['price'].values) < 0)\n",
    "        # new\n",
    "        abs_diff = np.median(np.abs( df_id['price'].values - np.mean(df_id['price'].values)))        \n",
    "        energy = np.mean(df_id['price'].values**2)\n",
    "        iqr_p = np.percentile(df_id['price'].values,75) - np.percentile(df_id['price'].values,25)\n",
    "        \n",
    "        # vol vars\n",
    "        \n",
    "        abs_diff_v = np.median(np.abs( df_id['size'].values - np.mean(df_id['size'].values)))        \n",
    "        energy_v = np.sum(df_id['size'].values**2)\n",
    "        iqr_p_v = np.percentile(df_id['size'].values,75) - np.percentile(df_id['size'].values,25)\n",
    "        \n",
    "        lis.append({'time_id':n_time_id,'tendency':tendencyV,'f_max':f_max,'f_min':f_min,'df_max':df_max,'df_min':df_min,\n",
    "                   'abs_diff':abs_diff,'energy':energy,'iqr_p':iqr_p,'abs_diff_v':abs_diff_v,'energy_v':energy_v,'iqr_p_v':iqr_p_v})\n",
    "    \n",
    "    df_lr = pd.DataFrame(lis)\n",
    "        \n",
    "   \n",
    "    df_feature = df_feature.merge(df_lr, how = 'left', left_on = 'time_id_', right_on = 'time_id')\n",
    "    \n",
    "    # Merge all\n",
    "    df_feature = df_feature.merge(df_feature_500, how = 'left', left_on = 'time_id_', right_on = 'time_id__500')\n",
    "    df_feature = df_feature.merge(df_feature_400, how = 'left', left_on = 'time_id_', right_on = 'time_id__400')\n",
    "    df_feature = df_feature.merge(df_feature_300, how = 'left', left_on = 'time_id_', right_on = 'time_id__300')\n",
    "    df_feature = df_feature.merge(df_feature_200, how = 'left', left_on = 'time_id_', right_on = 'time_id__200')\n",
    "    df_feature = df_feature.merge(df_feature_100, how = 'left', left_on = 'time_id_', right_on = 'time_id__100')\n",
    "    # Drop unnecesary time_ids\n",
    "    df_feature.drop(['time_id__500','time_id__400', 'time_id__300', 'time_id__200','time_id','time_id__100'], axis = 1, inplace = True)\n",
    "    \n",
    "    \n",
    "    df_feature = df_feature.add_prefix('trade_')\n",
    "    stock_id = file_path.split('=')[1]\n",
    "    df_feature['row_id'] = df_feature['trade_time_id_'].apply(lambda x:f'{stock_id}-{x}')\n",
    "    df_feature.drop(['trade_time_id_'], axis = 1, inplace = True)\n",
    "    return df_feature\n",
    "\n",
    "# Function to get group stats for the stock_id and time_id\n",
    "def get_time_stock(df):\n",
    "    vol_cols = ['log_return1_realized_volatility', 'log_return2_realized_volatility', 'log_return1_realized_volatility_400', 'log_return2_realized_volatility_400', \n",
    "                'log_return1_realized_volatility_300', 'log_return2_realized_volatility_300', 'log_return1_realized_volatility_200', 'log_return2_realized_volatility_200', \n",
    "                'trade_log_return_realized_volatility', 'trade_log_return_realized_volatility_400', 'trade_log_return_realized_volatility_300', 'trade_log_return_realized_volatility_200']\n",
    "\n",
    "\n",
    "    # Group by the stock id\n",
    "    df_stock_id = df.groupby(['stock_id'])[vol_cols].agg(['mean', 'std', 'max', 'min', ]).reset_index()\n",
    "    # Rename columns joining suffix\n",
    "    df_stock_id.columns = ['_'.join(col) for col in df_stock_id.columns]\n",
    "    df_stock_id = df_stock_id.add_suffix('_' + 'stock')\n",
    "\n",
    "    # Group by the stock id\n",
    "    df_time_id = df.groupby(['time_id'])[vol_cols].agg(['mean', 'std', 'max', 'min', ]).reset_index()\n",
    "    # Rename columns joining suffix\n",
    "    df_time_id.columns = ['_'.join(col) for col in df_time_id.columns]\n",
    "    df_time_id = df_time_id.add_suffix('_' + 'time')\n",
    "    \n",
    "    # Merge with original dataframe\n",
    "    df = df.merge(df_stock_id, how = 'left', left_on = ['stock_id'], right_on = ['stock_id__stock'])\n",
    "    df = df.merge(df_time_id, how = 'left', left_on = ['time_id'], right_on = ['time_id__time'])\n",
    "    df.drop(['stock_id__stock', 'time_id__time'], axis = 1, inplace = True)\n",
    "    return df\n",
    "    \n",
    "# Funtion to make preprocessing function in parallel (for each stock id)\n",
    "def preprocessor(list_stock_ids, is_train = True):\n",
    "    \n",
    "    # Parrallel for loop\n",
    "    def for_joblib(stock_id):\n",
    "        # Train\n",
    "        if is_train:\n",
    "            file_path_book = data_dir + \"book_train.parquet/stock_id=\" + str(stock_id)\n",
    "            file_path_trade = data_dir + \"trade_train.parquet/stock_id=\" + str(stock_id)\n",
    "        # Test\n",
    "        else:\n",
    "            file_path_book = data_dir + \"book_test.parquet/stock_id=\" + str(stock_id)\n",
    "            file_path_trade = data_dir + \"trade_test.parquet/stock_id=\" + str(stock_id)\n",
    "    \n",
    "        # Preprocess book and trade data and merge them\n",
    "        df_tmp = pd.merge(book_preprocessor(file_path_book), trade_preprocessor(file_path_trade), on = 'row_id', how = 'left')\n",
    "        \n",
    "        # Return the merge dataframe\n",
    "        return df_tmp\n",
    "    \n",
    "    # Use parallel api to call paralle for loop\n",
    "    df = Parallel(n_jobs = -1, verbose = 1)(delayed(for_joblib)(stock_id) for stock_id in list_stock_ids)\n",
    "    # Concatenate all the dataframes that return from Parallel\n",
    "    df = pd.concat(df, ignore_index = True)\n",
    "    return df\n",
    "\n",
    "# Function to calculate the root mean squared percentage error\n",
    "def rmspe(y_true, y_pred):\n",
    "    return np.sqrt(np.mean(np.square((y_true - y_pred) / y_true)))\n",
    "\n",
    "# Function to early stop with root mean squared percentage error\n",
    "def feval_rmspe(y_pred, lgb_train):\n",
    "    y_true = lgb_train.get_label()\n",
    "    return 'RMSPE', rmspe(y_true, y_pred), False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0fafe20f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-16T17:22:45.894905Z",
     "iopub.status.busy": "2023-12-16T17:22:45.894124Z",
     "iopub.status.idle": "2023-12-16T18:05:00.306054Z",
     "shell.execute_reply": "2023-12-16T18:05:00.305289Z",
     "shell.execute_reply.started": "2023-12-15T05:18:56.378351Z"
    },
    "papermill": {
     "duration": 2534.450709,
     "end_time": "2023-12-16T18:05:00.306245",
     "exception": false,
     "start_time": "2023-12-16T17:22:45.855536",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our training set has 428932 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed: 16.3min\n",
      "[Parallel(n_jobs=-1)]: Done 112 out of 112 | elapsed: 42.0min finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    0.4s finished\n"
     ]
    }
   ],
   "source": [
    "# Read train and test\n",
    "train, test = read_train_test()\n",
    "\n",
    "# Get unique stock ids \n",
    "train_stock_ids = train['stock_id'].unique()\n",
    "# Preprocess them using Parallel and our single stock id functions\n",
    "train_ = preprocessor(train_stock_ids, is_train = True)\n",
    "train = train.merge(train_, on = ['row_id'], how = 'left')\n",
    "\n",
    "# Get unique stock ids \n",
    "test_stock_ids = test['stock_id'].unique()\n",
    "# Preprocess them using Parallel and our single stock id functions\n",
    "test_ = preprocessor(test_stock_ids, is_train = False)\n",
    "test = test.merge(test_, on = ['row_id'], how = 'left')\n",
    "\n",
    "# Get group stats of time_id and stock_id\n",
    "train = get_time_stock(train)\n",
    "test = get_time_stock(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc081237",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-16T18:05:00.394360Z",
     "iopub.status.busy": "2023-12-16T18:05:00.393655Z",
     "iopub.status.idle": "2023-12-16T18:05:00.411479Z",
     "shell.execute_reply": "2023-12-16T18:05:00.410813Z",
     "shell.execute_reply.started": "2023-12-15T06:01:53.907369Z"
    },
    "papermill": {
     "duration": 0.065203,
     "end_time": "2023-12-16T18:05:00.411614",
     "exception": false,
     "start_time": "2023-12-16T18:05:00.346411",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# replace by order sum (tau)\n",
    "train['size_tau'] = np.sqrt( 1/ train['trade_seconds_in_bucket_count_unique'] )\n",
    "test['size_tau'] = np.sqrt( 1/ test['trade_seconds_in_bucket_count_unique'] )\n",
    "#train['size_tau_450'] = np.sqrt( 1/ train['trade_seconds_in_bucket_count_unique_450'] )\n",
    "#test['size_tau_450'] = np.sqrt( 1/ test['trade_seconds_in_bucket_count_unique_450'] )\n",
    "train['size_tau_400'] = np.sqrt( 1/ train['trade_seconds_in_bucket_count_unique_400'] )\n",
    "test['size_tau_400'] = np.sqrt( 1/ test['trade_seconds_in_bucket_count_unique_400'] )\n",
    "train['size_tau_300'] = np.sqrt( 1/ train['trade_seconds_in_bucket_count_unique_300'] )\n",
    "test['size_tau_300'] = np.sqrt( 1/ test['trade_seconds_in_bucket_count_unique_300'] )\n",
    "#train['size_tau_150'] = np.sqrt( 1/ train['trade_seconds_in_bucket_count_unique_150'] )\n",
    "#test['size_tau_150'] = np.sqrt( 1/ test['trade_seconds_in_bucket_count_unique_150'] )\n",
    "train['size_tau_200'] = np.sqrt( 1/ train['trade_seconds_in_bucket_count_unique_200'] )\n",
    "test['size_tau_200'] = np.sqrt( 1/ test['trade_seconds_in_bucket_count_unique_200'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1abcafd1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-16T18:05:00.490592Z",
     "iopub.status.busy": "2023-12-16T18:05:00.489897Z",
     "iopub.status.idle": "2023-12-16T18:05:00.510710Z",
     "shell.execute_reply": "2023-12-16T18:05:00.510114Z",
     "shell.execute_reply.started": "2023-12-15T06:01:53.935717Z"
    },
    "papermill": {
     "duration": 0.0634,
     "end_time": "2023-12-16T18:05:00.510840",
     "exception": false,
     "start_time": "2023-12-16T18:05:00.447440",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train['size_tau2'] = np.sqrt( 1/ train['trade_order_count_sum'] )\n",
    "test['size_tau2'] = np.sqrt( 1/ test['trade_order_count_sum'] )\n",
    "#train['size_tau2_450'] = np.sqrt( 0.25/ train['trade_order_count_sum'] )\n",
    "#test['size_tau2_450'] = np.sqrt( 0.25/ test['trade_order_count_sum'] )\n",
    "train['size_tau2_400'] = np.sqrt( 0.33/ train['trade_order_count_sum'] )\n",
    "test['size_tau2_400'] = np.sqrt( 0.33/ test['trade_order_count_sum'] )\n",
    "train['size_tau2_300'] = np.sqrt( 0.5/ train['trade_order_count_sum'] )\n",
    "test['size_tau2_300'] = np.sqrt( 0.5/ test['trade_order_count_sum'] )\n",
    "#train['size_tau2_150'] = np.sqrt( 0.75/ train['trade_order_count_sum'] )\n",
    "#test['size_tau2_150'] = np.sqrt( 0.75/ test['trade_order_count_sum'] )\n",
    "train['size_tau2_200'] = np.sqrt( 0.66/ train['trade_order_count_sum'] )\n",
    "test['size_tau2_200'] = np.sqrt( 0.66/ test['trade_order_count_sum'] )\n",
    "\n",
    "# delta tau\n",
    "train['size_tau2_d'] = train['size_tau2_400'] - train['size_tau2']\n",
    "test['size_tau2_d'] = test['size_tau2_400'] - test['size_tau2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "908dfcef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-16T18:05:00.591546Z",
     "iopub.status.busy": "2023-12-16T18:05:00.590788Z",
     "iopub.status.idle": "2023-12-16T18:05:00.594885Z",
     "shell.execute_reply": "2023-12-16T18:05:00.594382Z",
     "shell.execute_reply.started": "2023-12-15T06:01:53.962300Z"
    },
    "papermill": {
     "duration": 0.045984,
     "end_time": "2023-12-16T18:05:00.595002",
     "exception": false,
     "start_time": "2023-12-16T18:05:00.549018",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "226"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "colNames = [col for col in list(train.columns)\n",
    "            if col not in {\"stock_id\", \"time_id\", \"target\", \"row_id\"}]\n",
    "len(colNames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e1f9e7d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-16T18:05:00.675253Z",
     "iopub.status.busy": "2023-12-16T18:05:00.674620Z",
     "iopub.status.idle": "2023-12-16T18:05:03.221084Z",
     "shell.execute_reply": "2023-12-16T18:05:03.220456Z",
     "shell.execute_reply.started": "2023-12-15T06:01:53.974201Z"
    },
    "papermill": {
     "duration": 2.591261,
     "end_time": "2023-12-16T18:05:03.221276",
     "exception": false,
     "start_time": "2023-12-16T18:05:00.630015",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 4 2 1 1 2 4 6 2 1 0 4 4 1 1 1 2 4 4 4 0 1 1 3 1 1 4 3 4 3 4 4 1 3 3 4\n",
      " 3 4 1 4 1 4 4 1 0 4 4 1 0 0 3 3 3 2 0 2 4 1 4 4 1 4 1 0 3 3 0 3 0 6 5 3 3\n",
      " 0 1 2 0 3 3 3 4 1 1 0 2 3 3 1 0 1 4 4 4 4 4 1 3 1 0 1 4 1 0 1 4 1 0 4 0 4\n",
      " 0]\n",
      "[1, 11, 22, 50, 55, 56, 62, 73, 76, 78, 84, 87, 96, 101, 112, 116, 122, 124, 126]\n",
      "(0, 227)\n",
      "[0, 4, 5, 10, 15, 16, 17, 23, 26, 28, 29, 36, 42, 44, 48, 53, 66, 69, 72, 85, 94, 95, 100, 102, 109, 111, 113, 115, 118, 120]\n",
      "(3, 227)\n",
      "[3, 6, 9, 18, 61, 63, 86, 97]\n",
      "(0, 227)\n",
      "[27, 31, 33, 37, 38, 40, 58, 59, 60, 74, 75, 77, 82, 83, 88, 89, 90, 98, 99, 110]\n",
      "(0, 227)\n",
      "[2, 7, 13, 14, 19, 20, 21, 30, 32, 34, 35, 39, 41, 43, 46, 47, 51, 52, 64, 67, 68, 70, 93, 103, 104, 105, 107, 108, 114, 119, 123, 125]\n",
      "(0, 227)\n",
      "[81]\n",
      "(0, 227)\n",
      "[8, 80]\n",
      "(0, 227)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "# making agg features\n",
    "\n",
    "train_p = pd.read_csv('../input/optiver-realized-volatility-prediction/train.csv')\n",
    "train_p = train_p.pivot(index='time_id', columns='stock_id', values='target')\n",
    "\n",
    "corr = train_p.corr()\n",
    "\n",
    "ids = corr.index\n",
    "\n",
    "kmeans = KMeans(n_clusters=7, random_state=0).fit(corr.values)\n",
    "print(kmeans.labels_)\n",
    "\n",
    "l = []\n",
    "for n in range(7):\n",
    "    l.append ( [ (x-1) for x in ( (ids+1)*(kmeans.labels_ == n)) if x > 0] )\n",
    "    \n",
    "\n",
    "mat = []\n",
    "matTest = []\n",
    "\n",
    "n = 0\n",
    "for ind in l:\n",
    "    print(ind)\n",
    "    newDf = train.loc[train['stock_id'].isin(ind) ]\n",
    "    newDf = newDf.groupby(['time_id']).agg(np.nanmean)\n",
    "    newDf.loc[:,'stock_id'] = str(n)+'c1'\n",
    "    mat.append ( newDf )\n",
    "    \n",
    "    newDf = test.loc[test['stock_id'].isin(ind) ]    \n",
    "    newDf = newDf.groupby(['time_id']).agg(np.nanmean)\n",
    "    newDf.loc[:,'stock_id'] = str(n)+'c1'\n",
    "    matTest.append ( newDf )\n",
    "    print(newDf.shape)\n",
    "    n+=1\n",
    "    \n",
    "mat1 = pd.concat(mat).reset_index()\n",
    "mat1.drop(columns=['target'],inplace=True)\n",
    "\n",
    "mat2 = pd.concat(matTest).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "771021ba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-16T18:05:03.306608Z",
     "iopub.status.busy": "2023-12-16T18:05:03.305334Z",
     "iopub.status.idle": "2023-12-16T18:05:03.514759Z",
     "shell.execute_reply": "2023-12-16T18:05:03.514017Z",
     "shell.execute_reply.started": "2023-12-15T06:01:56.505776Z"
    },
    "papermill": {
     "duration": 0.254101,
     "end_time": "2023-12-16T18:05:03.514913",
     "exception": false,
     "start_time": "2023-12-16T18:05:03.260812",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:3: FutureWarning: Index.ravel returning ndarray is deprecated; in a future version this will return a view on self.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:7: FutureWarning: Index.ravel returning ndarray is deprecated; in a future version this will return a view on self.\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "mat2 = pd.concat([mat2,mat1.loc[mat1.time_id==5]])\n",
    "mat1 = mat1.pivot(index='time_id', columns='stock_id')\n",
    "mat1.columns = [\"_\".join(x) for x in mat1.columns.ravel()]\n",
    "mat1.reset_index(inplace=True)\n",
    "\n",
    "mat2 = mat2.pivot(index='time_id', columns='stock_id')\n",
    "mat2.columns = [\"_\".join(x) for x in mat2.columns.ravel()]\n",
    "mat2.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f82eab5c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-16T18:05:03.600923Z",
     "iopub.status.busy": "2023-12-16T18:05:03.600337Z",
     "iopub.status.idle": "2023-12-16T18:05:22.308136Z",
     "shell.execute_reply": "2023-12-16T18:05:22.307492Z",
     "shell.execute_reply.started": "2023-12-15T06:01:56.714790Z"
    },
    "papermill": {
     "duration": 18.753057,
     "end_time": "2023-12-16T18:05:22.308311",
     "exception": false,
     "start_time": "2023-12-16T18:05:03.555254",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      time_id  wap1_sum_0c1  wap1_sum_1c1  wap1_sum_2c1  wap1_sum_3c1  \\\n",
      "0           5    454.259460    450.296722    410.733185    350.107635   \n",
      "1          11    338.439728    320.296692    278.055267    233.907928   \n",
      "2          16    352.636505    360.082367    347.626770    256.215057   \n",
      "3          31    312.892395    302.954620    298.578033    231.109253   \n",
      "4          62    327.286407    314.466064    315.001099    210.724350   \n",
      "...       ...           ...           ...           ...           ...   \n",
      "3825    32751    419.968994    395.118561    361.749359    297.389282   \n",
      "3826    32753    372.915985    347.497681    303.217072    275.159363   \n",
      "3827    32758    311.283264    327.945435    290.676208    234.331451   \n",
      "3828    32763    443.204437    451.020905    408.540771    354.972992   \n",
      "3829    32767    357.900848    373.544617    305.278687    252.666580   \n",
      "\n",
      "      wap1_sum_4c1  wap1_sum_5c1  wap1_sum_6c1  wap1_mean_0c1  wap1_mean_1c1  \\\n",
      "0       506.105530    537.815308    476.310425       1.003184       1.002290   \n",
      "1       358.379181    281.177979    423.141602       1.000956       1.000588   \n",
      "2       420.425507    355.364594    284.772736       1.000108       0.999730   \n",
      "3       343.913910    449.594482    366.892456       0.999307       0.999252   \n",
      "4       355.398346    218.791824    407.402344       0.999301       0.999558   \n",
      "...            ...           ...           ...            ...            ...   \n",
      "3825    467.723602    338.286774    451.196350       0.999238       0.999701   \n",
      "3826    410.243500    281.197662    362.701538       1.000772       0.999747   \n",
      "3827    381.826019    268.066437    214.089966       1.001127       1.000425   \n",
      "3828    494.760376    467.120056    392.955444       1.001051       1.001303   \n",
      "3829    419.389618    313.938019    423.653992       0.999563       1.000014   \n",
      "\n",
      "      ...  size_tau2_200_4c1  size_tau2_200_5c1  size_tau2_200_6c1  \\\n",
      "0     ...           0.044431           0.044926           0.041189   \n",
      "1     ...           0.063868           0.051484           0.037159   \n",
      "2     ...           0.060912           0.036405           0.065579   \n",
      "3     ...           0.076921           0.037837           0.045045   \n",
      "4     ...           0.070177           0.076425           0.037661   \n",
      "...   ...                ...                ...                ...   \n",
      "3825  ...           0.050041           0.047871           0.033436   \n",
      "3826  ...           0.050737           0.048901           0.050758   \n",
      "3827  ...           0.073582           0.060722           0.097900   \n",
      "3828  ...           0.041723           0.047706           0.033103   \n",
      "3829  ...           0.064281           0.064226           0.043765   \n",
      "\n",
      "      size_tau2_d_0c1  size_tau2_d_1c1  size_tau2_d_2c1  size_tau2_d_3c1  \\\n",
      "0           -0.024915        -0.024369        -0.027101        -0.033393   \n",
      "1           -0.034569        -0.033599        -0.037332        -0.052039   \n",
      "2           -0.033426        -0.037183        -0.027899        -0.049481   \n",
      "3           -0.042717        -0.038160        -0.032851        -0.045086   \n",
      "4           -0.037143        -0.037902        -0.035379        -0.047943   \n",
      "...               ...              ...              ...              ...   \n",
      "3825        -0.027396        -0.027238        -0.027789        -0.038157   \n",
      "3826        -0.031339        -0.028047        -0.035711        -0.036298   \n",
      "3827        -0.041982        -0.039200        -0.041714        -0.052872   \n",
      "3828        -0.021978        -0.023091        -0.023470        -0.028788   \n",
      "3829        -0.032925        -0.033441        -0.036363        -0.046284   \n",
      "\n",
      "      size_tau2_d_4c1  size_tau2_d_5c1  size_tau2_d_6c1  \n",
      "0           -0.023273        -0.023533        -0.021575  \n",
      "1           -0.033454        -0.026968        -0.019464  \n",
      "2           -0.031906        -0.019069        -0.034351  \n",
      "3           -0.040292        -0.019820        -0.023595  \n",
      "4           -0.036759        -0.040032        -0.019727  \n",
      "...               ...              ...              ...  \n",
      "3825        -0.026212        -0.025075        -0.017514  \n",
      "3826        -0.026576        -0.025615        -0.026587  \n",
      "3827        -0.038543        -0.031807        -0.051281  \n",
      "3828        -0.021855        -0.024989        -0.017340  \n",
      "3829        -0.033671        -0.033642        -0.022924  \n",
      "\n",
      "[3830 rows x 1583 columns]\n"
     ]
    }
   ],
   "source": [
    "nnn = ['time_id',\n",
    "     'log_return1_realized_volatility_0c1',\n",
    "     'log_return1_realized_volatility_1c1',     \n",
    "     'log_return1_realized_volatility_3c1',\n",
    "     'log_return1_realized_volatility_4c1',     \n",
    "     'log_return1_realized_volatility_6c1',\n",
    "     'total_volume_sum_0c1',\n",
    "     'total_volume_sum_1c1', \n",
    "     'total_volume_sum_3c1',\n",
    "     'total_volume_sum_4c1', \n",
    "     'total_volume_sum_6c1',\n",
    "     'trade_size_sum_0c1',\n",
    "     'trade_size_sum_1c1', \n",
    "     'trade_size_sum_3c1',\n",
    "     'trade_size_sum_4c1', \n",
    "     'trade_size_sum_6c1',\n",
    "     'trade_order_count_sum_0c1',\n",
    "     'trade_order_count_sum_1c1',\n",
    "     'trade_order_count_sum_3c1',\n",
    "     'trade_order_count_sum_4c1',\n",
    "     'trade_order_count_sum_6c1',      \n",
    "     'price_spread_sum_0c1',\n",
    "     'price_spread_sum_1c1',\n",
    "     'price_spread_sum_3c1',\n",
    "     'price_spread_sum_4c1',\n",
    "     'price_spread_sum_6c1',   \n",
    "     'bid_spread_sum_0c1',\n",
    "     'bid_spread_sum_1c1',\n",
    "     'bid_spread_sum_3c1',\n",
    "     'bid_spread_sum_4c1',\n",
    "     'bid_spread_sum_6c1',       \n",
    "     'ask_spread_sum_0c1',\n",
    "     'ask_spread_sum_1c1',\n",
    "     'ask_spread_sum_3c1',\n",
    "     'ask_spread_sum_4c1',\n",
    "     'ask_spread_sum_6c1',   \n",
    "     'volume_imbalance_sum_0c1',\n",
    "     'volume_imbalance_sum_1c1',\n",
    "     'volume_imbalance_sum_3c1',\n",
    "     'volume_imbalance_sum_4c1',\n",
    "     'volume_imbalance_sum_6c1',       \n",
    "     'bid_ask_spread_sum_0c1',\n",
    "     'bid_ask_spread_sum_1c1',\n",
    "     'bid_ask_spread_sum_3c1',\n",
    "     'bid_ask_spread_sum_4c1',\n",
    "     'bid_ask_spread_sum_6c1',\n",
    "     'size_tau2_0c1',\n",
    "     'size_tau2_1c1',\n",
    "     'size_tau2_3c1',\n",
    "     'size_tau2_4c1',\n",
    "     'size_tau2_6c1'] \n",
    "print(mat1)\n",
    "train = pd.merge(train,mat1[nnn],how='left',on='time_id')\n",
    "test = pd.merge(test,mat2[nnn],how='left',on='time_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dac047ad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-16T18:05:22.480504Z",
     "iopub.status.busy": "2023-12-16T18:05:22.479651Z",
     "iopub.status.idle": "2023-12-16T18:05:22.483452Z",
     "shell.execute_reply": "2023-12-16T18:05:22.482858Z",
     "shell.execute_reply.started": "2023-12-15T06:02:15.449262Z"
    },
    "papermill": {
     "duration": 0.135087,
     "end_time": "2023-12-16T18:05:22.483588",
     "exception": false,
     "start_time": "2023-12-16T18:05:22.348501",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "del mat1,mat2\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9bedd2d",
   "metadata": {
    "papermill": {
     "duration": 0.03862,
     "end_time": "2023-12-16T18:05:22.560328",
     "exception": false,
     "start_time": "2023-12-16T18:05:22.521708",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Tabnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "681ec43f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-16T18:05:22.643626Z",
     "iopub.status.busy": "2023-12-16T18:05:22.643005Z",
     "iopub.status.idle": "2023-12-16T18:05:22.646309Z",
     "shell.execute_reply": "2023-12-16T18:05:22.645692Z",
     "shell.execute_reply.started": "2023-12-15T06:02:15.557370Z"
    },
    "papermill": {
     "duration": 0.048025,
     "end_time": "2023-12-16T18:05:22.646435",
     "exception": false,
     "start_time": "2023-12-16T18:05:22.598410",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "USE_PRECOMPUTE_FEATURES = True  # Load precomputed features for train.csv from private dataset (just for speed up)\n",
    "\n",
    "# model & ensemble configurations\n",
    "PREDICT_CNN = True\n",
    "PREDICT_MLP = True\n",
    "PREDICT_GBDT = True\n",
    "PREDICT_TABNET = False\n",
    "\n",
    "GBDT_NUM_MODELS = 5 #3\n",
    "GBDT_LR = 0.02  # 0.1\n",
    "\n",
    "NN_VALID_TH = 0.185\n",
    "NN_MODEL_TOP_N = 3\n",
    "TAB_MODEL_TOP_N = 3\n",
    "ENSEMBLE_METHOD = 'mean'\n",
    "NN_NUM_MODELS = 10\n",
    "TABNET_NUM_MODELS = 5\n",
    "\n",
    "# for saving quota\n",
    "IS_1ST_STAGE = False\n",
    "SHORTCUT_NN_IN_1ST_STAGE = True  # early-stop training to save GPU quota\n",
    "SHORTCUT_GBDT_IN_1ST_STAGE = False\n",
    "MEMORY_TEST_MODE = False\n",
    "\n",
    "# for ablation studies\n",
    "CV_SPLIT = 'time'  # 'time': time-series KFold 'group': GroupKFold by stock-id\n",
    "USE_PRICE_NN_FEATURES = True  # Use nearest neighbor features that rely on tick size\n",
    "USE_VOL_NN_FEATURES = True  # Use nearest neighbor features that can be calculated without tick size\n",
    "USE_SIZE_NN_FEATURES = True  # Use nearest neighbor features that can be calculated without tick size\n",
    "USE_RANDOM_NN_FEATURES = False  # Use random index to aggregate neighbors\n",
    "\n",
    "USE_TIME_ID_NN = True  # Use time-id based neighbors\n",
    "USE_STOCK_ID_NN = True  # Use stock-id based neighbors\n",
    "\n",
    "ENABLE_RANK_NORMALIZATION = True  # Enable rank-normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "58cdc14e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-16T18:05:22.762533Z",
     "iopub.status.busy": "2023-12-16T18:05:22.726073Z",
     "iopub.status.idle": "2023-12-16T18:05:24.369286Z",
     "shell.execute_reply": "2023-12-16T18:05:24.368635Z",
     "shell.execute_reply.started": "2023-12-15T06:02:15.568463Z"
    },
    "papermill": {
     "duration": 1.685268,
     "end_time": "2023-12-16T18:05:24.369426",
     "exception": false,
     "start_time": "2023-12-16T18:05:22.684158",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import random\n",
    "from typing import List, Tuple, Optional, Union\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn.decomposition import PCA\n",
    "from pytorch_tabnet.metrics import Metric\n",
    "from pytorch_tabnet.tab_model import TabNetRegressor\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "\n",
    "null_check_cols = [\n",
    "    'book.log_return1.realized_volatility',\n",
    "    'book_150.log_return1.realized_volatility',\n",
    "    'book_300.log_return1.realized_volatility',\n",
    "    'book_450.log_return1.realized_volatility',\n",
    "    'trade.log_return.realized_volatility',\n",
    "    'trade_150.log_return.realized_volatility',\n",
    "    'trade_300.log_return.realized_volatility',\n",
    "    'trade_450.log_return.realized_volatility'\n",
    "] # might be problems in these columns\n",
    "\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "def rmspe_metric(y_true, y_pred):\n",
    "    rmspe = np.sqrt(np.mean(np.square((y_true - y_pred) / y_true)))\n",
    "    return rmspe\n",
    "\n",
    "\n",
    "def rmspe_loss(y_true, y_pred):\n",
    "    rmspe = torch.sqrt(torch.mean(torch.square((y_true - y_pred) / y_true)))\n",
    "    return rmspe\n",
    "\n",
    "\n",
    "class RMSPE(Metric):\n",
    "    def __init__(self):\n",
    "        self._name = \"rmspe\"\n",
    "        self._maximize = False\n",
    "\n",
    "    def __call__(self, y_true, y_score):\n",
    "        return np.sqrt(np.mean(np.square((y_true - y_score) / y_true)))\n",
    "\n",
    "def RMSPELoss_Tabnet(y_pred, y_true):\n",
    "    return torch.sqrt(torch.mean( ((y_true - y_pred) / y_true) ** 2 )).clone()\n",
    "\n",
    "\n",
    "class AverageMeter:\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "class TabularDataset(Dataset):\n",
    "    def __init__(self, x_num: np.ndarray, x_cat: np.ndarray, y: Optional[np.ndarray]):\n",
    "        super().__init__()\n",
    "        self.x_num = x_num\n",
    "        self.x_cat = x_cat\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x_num)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.y is None:\n",
    "            return self.x_num[idx], torch.LongTensor(self.x_cat[idx])\n",
    "        else:\n",
    "            return self.x_num[idx], torch.LongTensor(self.x_cat[idx]), self.y[idx]\n",
    "\n",
    "\n",
    "class MLP(nn.Module): \n",
    "    def __init__(self,\n",
    "                 src_num_dim: int,\n",
    "                 n_categories: List[int],\n",
    "                 dropout: float = 0.0,\n",
    "                 hidden: int = 50,\n",
    "                 emb_dim: int = 10,\n",
    "                 dropout_cat: float = 0.2,\n",
    "                 bn: bool = False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embs = nn.ModuleList([\n",
    "            nn.Embedding(x, emb_dim) for x in n_categories])\n",
    "        self.cat_dim = emb_dim * len(n_categories)\n",
    "        self.dropout_cat = nn.Dropout(dropout_cat)\n",
    "\n",
    "        if bn:\n",
    "            self.sequence = nn.Sequential(\n",
    "                nn.Linear(src_num_dim + self.cat_dim, hidden),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.BatchNorm1d(hidden),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden, hidden),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.BatchNorm1d(hidden),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden, 1)\n",
    "            )\n",
    "        else:\n",
    "            self.sequence = nn.Sequential(\n",
    "                nn.Linear(src_num_dim + self.cat_dim, hidden),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden, hidden),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden, 1)\n",
    "            )\n",
    "\n",
    "    def forward(self, x_num, x_cat):\n",
    "        embs = [embedding(x_cat[:, i]) for i, embedding in enumerate(self.embs)]\n",
    "        x_cat_emb = self.dropout_cat(torch.cat(embs, 1))\n",
    "        x_all = torch.cat([x_num, x_cat_emb], 1)\n",
    "        x = self.sequence(x_all)\n",
    "        return torch.squeeze(x)\n",
    "\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_features: int,\n",
    "                 hidden_size: int,\n",
    "                 n_categories: List[int],\n",
    "                 emb_dim: int = 10,\n",
    "                 dropout_cat: float = 0.2,\n",
    "                 channel_1: int = 256,\n",
    "                 channel_2: int = 512,\n",
    "                 channel_3: int = 512,\n",
    "                 dropout_top: float = 0.1,\n",
    "                 dropout_mid: float = 0.3,\n",
    "                 dropout_bottom: float = 0.2,\n",
    "                 weight_norm: bool = True,\n",
    "                 two_stage: bool = True,\n",
    "                 celu: bool = True,\n",
    "                 kernel1: int = 5,\n",
    "                 leaky_relu: bool = False):\n",
    "        super().__init__()\n",
    "\n",
    "        num_targets = 1\n",
    "\n",
    "        cha_1_reshape = int(hidden_size / channel_1)\n",
    "        cha_po_1 = int(hidden_size / channel_1 / 2)\n",
    "        cha_po_2 = int(hidden_size / channel_1 / 2 / 2) * channel_3\n",
    "\n",
    "        self.cat_dim = emb_dim * len(n_categories)\n",
    "        self.cha_1 = channel_1\n",
    "        self.cha_2 = channel_2\n",
    "        self.cha_3 = channel_3\n",
    "        self.cha_1_reshape = cha_1_reshape\n",
    "        self.cha_po_1 = cha_po_1\n",
    "        self.cha_po_2 = cha_po_2\n",
    "        self.two_stage = two_stage\n",
    "\n",
    "        self.expand = nn.Sequential(\n",
    "            nn.BatchNorm1d(num_features + self.cat_dim),\n",
    "            nn.Dropout(dropout_top),\n",
    "            nn.utils.weight_norm(nn.Linear(num_features + self.cat_dim, hidden_size), dim=None),\n",
    "            nn.CELU(0.06) if celu else nn.ReLU()\n",
    "        )\n",
    "\n",
    "        def _norm(layer, dim=None):\n",
    "            return nn.utils.weight_norm(layer, dim=dim) if weight_norm else layer\n",
    "\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.BatchNorm1d(channel_1),\n",
    "            nn.Dropout(dropout_top),\n",
    "            _norm(nn.Conv1d(channel_1, channel_2, kernel_size=kernel1, stride=1, padding=kernel1 // 2, bias=False)),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool1d(output_size=cha_po_1),\n",
    "            nn.BatchNorm1d(channel_2),\n",
    "            nn.Dropout(dropout_top),\n",
    "            _norm(nn.Conv1d(channel_2, channel_2, kernel_size=3, stride=1, padding=1, bias=True)),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        if self.two_stage:\n",
    "            self.conv2 = nn.Sequential(\n",
    "                nn.BatchNorm1d(channel_2),\n",
    "                nn.Dropout(dropout_mid),\n",
    "                _norm(nn.Conv1d(channel_2, channel_2, kernel_size=3, stride=1, padding=1, bias=True)),\n",
    "                nn.ReLU(),\n",
    "                nn.BatchNorm1d(channel_2),\n",
    "                nn.Dropout(dropout_bottom),\n",
    "                _norm(nn.Conv1d(channel_2, channel_3, kernel_size=5, stride=1, padding=2, bias=True)),\n",
    "                nn.ReLU()\n",
    "            )\n",
    "\n",
    "        self.max_po_c2 = nn.MaxPool1d(kernel_size=4, stride=2, padding=1)\n",
    "\n",
    "        self.flt = nn.Flatten()\n",
    "\n",
    "        if leaky_relu:\n",
    "            self.dense = nn.Sequential(\n",
    "                nn.BatchNorm1d(cha_po_2),\n",
    "                nn.Dropout(dropout_bottom),\n",
    "                _norm(nn.Linear(cha_po_2, num_targets), dim=0),\n",
    "                nn.LeakyReLU()\n",
    "            )\n",
    "        else:\n",
    "            self.dense = nn.Sequential(\n",
    "                nn.BatchNorm1d(cha_po_2),\n",
    "                nn.Dropout(dropout_bottom),\n",
    "                _norm(nn.Linear(cha_po_2, num_targets), dim=0)\n",
    "            )\n",
    "\n",
    "        self.embs = nn.ModuleList([nn.Embedding(x, emb_dim) for x in n_categories])\n",
    "        self.cat_dim = emb_dim * len(n_categories)\n",
    "        self.dropout_cat = nn.Dropout(dropout_cat)\n",
    "\n",
    "    def forward(self, x_num, x_cat):\n",
    "        embs = [embedding(x_cat[:, i]) for i, embedding in enumerate(self.embs)]\n",
    "        x_cat_emb = self.dropout_cat(torch.cat(embs, 1))\n",
    "        x = torch.cat([x_num, x_cat_emb], 1)\n",
    "\n",
    "        x = self.expand(x)\n",
    "\n",
    "        x = x.reshape(x.shape[0], self.cha_1, self.cha_1_reshape)\n",
    "\n",
    "        x = self.conv1(x)\n",
    "\n",
    "        if self.two_stage:\n",
    "            x = self.conv2(x) * x\n",
    "\n",
    "        x = self.max_po_c2(x)\n",
    "        x = self.flt(x)\n",
    "        x = self.dense(x)\n",
    "\n",
    "        return torch.squeeze(x)\n",
    "\n",
    "\n",
    "def preprocess_nn(\n",
    "        X: pd.DataFrame,\n",
    "        scaler: Optional[StandardScaler] = None,\n",
    "        scaler_type: str = 'standard',\n",
    "        n_pca: int = -1,\n",
    "        na_cols: bool = True):\n",
    "    if na_cols:\n",
    "        #for c in X.columns:\n",
    "        for c in null_check_cols:\n",
    "            if c in X.columns:\n",
    "                X[f\"{c}_isnull\"] = X[c].isnull().astype(int)\n",
    "\n",
    "    cat_cols = [c for c in X.columns if c in ['time_id', 'stock_id']]\n",
    "    num_cols = [c for c in X.columns if c not in cat_cols]\n",
    "\n",
    "    X_num = X[num_cols].values.astype(np.float32)\n",
    "    X_cat = np.nan_to_num(X[cat_cols].values.astype(np.int32))\n",
    "\n",
    "    def _pca(X_num_):\n",
    "        if n_pca > 0:\n",
    "            pca = PCA(n_components=n_pca, random_state=0)\n",
    "            return pca.fit_transform(X_num)\n",
    "        return X_num\n",
    "\n",
    "    if scaler is None:\n",
    "        scaler = StandardScaler()\n",
    "        X_num = scaler.fit_transform(X_num)\n",
    "        X_num = np.nan_to_num(X_num, posinf=0, neginf=0)\n",
    "        return _pca(X_num), X_cat, cat_cols, scaler\n",
    "    else:\n",
    "        X_num = scaler.transform(X_num) #TODO: infでも大丈夫？\n",
    "        X_num = np.nan_to_num(X_num, posinf=0, neginf=0)\n",
    "        return _pca(X_num), X_cat, cat_cols\n",
    "\n",
    "\n",
    "def train_epoch(data_loader: DataLoader,\n",
    "                model: nn.Module,\n",
    "                optimizer,\n",
    "                scheduler,\n",
    "                device,\n",
    "                clip_grad: float = 1.5):\n",
    "    model.train()\n",
    "    losses = AverageMeter()\n",
    "    step = 0\n",
    "\n",
    "    for x_num, x_cat, y in tqdm(data_loader, position=0, leave=True, desc='Training'):\n",
    "        batch_size = x_num.size(0)\n",
    "        x_num = x_num.to(device, dtype=torch.float)\n",
    "        x_cat = x_cat.to(device)\n",
    "        y = y.to(device, dtype=torch.float)\n",
    "\n",
    "        loss = rmspe_loss(y, model(x_num, x_cat))\n",
    "        losses.update(loss.detach().cpu().numpy(), batch_size)\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip_grad)\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "        step += 1\n",
    "\n",
    "    return losses.avg\n",
    "\n",
    "\n",
    "def evaluate(data_loader: DataLoader, model, device):\n",
    "    model.eval()\n",
    "\n",
    "    losses = AverageMeter()\n",
    "\n",
    "    final_targets = []\n",
    "    final_outputs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x_num, x_cat, y in tqdm(data_loader, position=0, leave=True, desc='Evaluating'):\n",
    "            batch_size = x_num.size(0)\n",
    "            x_num = x_num.to(device, dtype=torch.float)\n",
    "            x_cat = x_cat.to(device)\n",
    "            y = y.to(device, dtype=torch.float)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                output = model(x_num, x_cat)\n",
    "\n",
    "            loss = rmspe_loss(y, output)\n",
    "            # record loss\n",
    "            losses.update(loss.detach().cpu().numpy(), batch_size)\n",
    "\n",
    "            targets = y.detach().cpu().numpy()\n",
    "            output = output.detach().cpu().numpy()\n",
    "\n",
    "            final_targets.append(targets)\n",
    "            final_outputs.append(output)\n",
    "\n",
    "    final_targets = np.concatenate(final_targets)\n",
    "    final_outputs = np.concatenate(final_outputs)\n",
    "\n",
    "    try:\n",
    "        metric = rmspe_metric(final_targets, final_outputs)\n",
    "    except:\n",
    "        metric = None\n",
    "\n",
    "    return final_outputs, final_targets, losses.avg, metric\n",
    "\n",
    "\n",
    "def predict_nn(X: pd.DataFrame,\n",
    "               model: Union[List[MLP], MLP],\n",
    "               scaler: StandardScaler,\n",
    "               device,\n",
    "               ensemble_method='mean'):\n",
    "    if not isinstance(model, list):\n",
    "        model = [model]\n",
    "\n",
    "    for m in model:\n",
    "        m.eval()\n",
    "    X_num, X_cat, cat_cols = preprocess_nn(X.copy(), scaler=scaler)\n",
    "    valid_dataset = TabularDataset(X_num, X_cat, None)\n",
    "    valid_loader = torch.utils.data.DataLoader(valid_dataset,\n",
    "                                               batch_size=512,\n",
    "                                               shuffle=False,\n",
    "                                               num_workers=4)\n",
    "\n",
    "    final_outputs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x_num, x_cat in tqdm(valid_loader, position=0, leave=True, desc='Evaluating'):\n",
    "            x_num = x_num.to(device, dtype=torch.float)\n",
    "            x_cat = x_cat.to(device)\n",
    "\n",
    "            outputs = []\n",
    "            with torch.no_grad():\n",
    "                for m in model:\n",
    "                    output = m(x_num, x_cat)\n",
    "                    outputs.append(output.detach().cpu().numpy())\n",
    "\n",
    "            if ensemble_method == 'median':\n",
    "                pred = np.nanmedian(np.array(outputs), axis=0)\n",
    "            else:\n",
    "                pred = np.array(outputs).mean(axis=0)\n",
    "            final_outputs.append(pred)\n",
    "\n",
    "    final_outputs = np.concatenate(final_outputs)\n",
    "    return final_outputs\n",
    "\n",
    "\n",
    "def predict_tabnet(X: pd.DataFrame,\n",
    "                   model: Union[List[TabNetRegressor], TabNetRegressor],\n",
    "                   scaler: StandardScaler,\n",
    "                   ensemble_method='mean'):\n",
    "    if not isinstance(model, list):\n",
    "        model = [model]\n",
    "\n",
    "    X_num, X_cat, cat_cols = preprocess_nn(X.copy(), scaler=scaler)\n",
    "    X_processed = np.concatenate([X_cat, X_num], axis=1)\n",
    "\n",
    "    predicted = []\n",
    "    for m in model:\n",
    "        predicted.append(m.predict(X_processed))\n",
    "\n",
    "    if ensemble_method == 'median':\n",
    "        pred = np.nanmedian(np.array(predicted), axis=0)\n",
    "    else:\n",
    "        pred = np.array(predicted).mean(axis=0)\n",
    "\n",
    "    return pred\n",
    "\n",
    "\n",
    "def train_tabnet(X: pd.DataFrame,\n",
    "                 y: pd.DataFrame,\n",
    "                 #folds: List[Tuple],\n",
    "                 batch_size: int = 1024,\n",
    "                 lr: float = 1e-3,\n",
    "                 model_path: str = 'fold_{}.pth',\n",
    "                 scaler_type: str = 'standard',\n",
    "                 output_dir: str = 'artifacts',\n",
    "                 epochs: int = 250,\n",
    "                 seed: int = 42,\n",
    "                 n_pca: int = -1,\n",
    "                 na_cols: bool = True,\n",
    "                 patience: int = 10,\n",
    "                 factor: float = 0.5,\n",
    "                 gamma: float = 2.0,\n",
    "                 lambda_sparse: float = 8.0,\n",
    "                 n_steps: int = 2,\n",
    "                 scheduler_type: str = 'cosine',\n",
    "                 n_a: int = 16):\n",
    "    seed_everything(seed)\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    y = y.values.astype(np.float32)\n",
    "    X_num, X_cat, cat_cols, scaler = preprocess_nn(X.copy(), scaler_type=scaler_type, n_pca=n_pca, na_cols=na_cols)\n",
    "\n",
    "    best_losses = []\n",
    "    best_predictions = []\n",
    "    kfold = KFold(n_splits = 3, random_state = 2021, shuffle = True)\n",
    "    for cv_idx, (train_idx, valid_idx) in enumerate(kfold.split(train)):\n",
    "        X_tr, X_va = X_num[train_idx], X_num[valid_idx]\n",
    "        X_tr_cat, X_va_cat = X_cat[train_idx], X_cat[valid_idx]\n",
    "        y_tr, y_va = y[train_idx], y[valid_idx]\n",
    "        y_tr = y_tr.reshape(-1,1)\n",
    "        y_va = y_va.reshape(-1,1)\n",
    "        X_tr = np.concatenate([X_tr_cat, X_tr], axis=1)\n",
    "        X_va = np.concatenate([X_va_cat, X_va], axis=1)\n",
    "\n",
    "        cat_idxs = [0]\n",
    "        cat_dims = [128]\n",
    "\n",
    "        if scheduler_type == 'cosine':\n",
    "            scheduler_params = dict(T_0=200, T_mult=1, eta_min=1e-4, last_epoch=-1, verbose=False)\n",
    "            scheduler_fn = CosineAnnealingWarmRestarts\n",
    "        else:\n",
    "            scheduler_params = {'mode': 'min', 'min_lr': 1e-7, 'patience': patience, 'factor': factor, 'verbose': True}\n",
    "            scheduler_fn = torch.optim.lr_scheduler.ReduceLROnPlateau\n",
    "\n",
    "        model = TabNetRegressor(\n",
    "            cat_idxs=cat_idxs,\n",
    "            cat_dims=cat_dims,\n",
    "            cat_emb_dim=1,\n",
    "            n_d=n_a,\n",
    "            n_a=n_a,\n",
    "            n_steps=n_steps,\n",
    "            gamma=gamma,\n",
    "            n_independent=2,\n",
    "            n_shared=2,\n",
    "            lambda_sparse=lambda_sparse,\n",
    "            optimizer_fn=torch.optim.Adam,\n",
    "            optimizer_params={'lr': lr},\n",
    "            mask_type=\"entmax\",\n",
    "            scheduler_fn=scheduler_fn,\n",
    "            scheduler_params=scheduler_params,\n",
    "            seed=seed,\n",
    "            verbose=10\n",
    "            #device_name=device,\n",
    "            #clip_value=1.5\n",
    "        )\n",
    "\n",
    "        model.fit(X_tr, y_tr, eval_set=[(X_va, y_va)], max_epochs=epochs, patience=50, batch_size=1024*20,\n",
    "                  virtual_batch_size=batch_size, num_workers=4, drop_last=False, eval_metric=[RMSPE], loss_fn=RMSPELoss_Tabnet)\n",
    "\n",
    "        path = os.path.join(output_dir, model_path.format(cv_idx))\n",
    "        model.save_model(path)\n",
    "\n",
    "        predicted = model.predict(X_va)\n",
    "\n",
    "        rmspe = rmspe_metric(y_va, predicted)\n",
    "        best_losses.append(rmspe)\n",
    "        best_predictions.append(predicted)\n",
    "\n",
    "    return best_losses, best_predictions, scaler, model\n",
    "\n",
    "\n",
    "def train_nn(X: pd.DataFrame,\n",
    "             y: pd.DataFrame,\n",
    "             #folds: List[Tuple],\n",
    "             device,\n",
    "             emb_dim: int = 25,\n",
    "             batch_size: int = 1024,\n",
    "             model_type: str = 'mlp',\n",
    "             mlp_dropout: float = 0.0,\n",
    "             mlp_hidden: int = 64,\n",
    "             mlp_bn: bool = False,\n",
    "             cnn_hidden: int = 64,\n",
    "             cnn_channel1: int = 32,\n",
    "             cnn_channel2: int = 32,\n",
    "             cnn_channel3: int = 32,\n",
    "             cnn_kernel1: int = 5,\n",
    "             cnn_celu: bool = False,\n",
    "             cnn_weight_norm: bool = False,\n",
    "             dropout_emb: bool = 0.0,\n",
    "             lr: float = 1e-3,\n",
    "             weight_decay: float = 0.0,\n",
    "             model_path: str = 'fold_{}.pth',\n",
    "             scaler_type: str = 'standard',\n",
    "             output_dir: str = 'artifacts',\n",
    "             scheduler_type: str = 'onecycle',\n",
    "             optimizer_type: str = 'adam',\n",
    "             max_lr: float = 0.01,\n",
    "             epochs: int = 30,\n",
    "             seed: int = 42,\n",
    "             n_pca: int = 100,\n",
    "             batch_double_freq: int = 50,\n",
    "             cnn_dropout: float = 0.1,\n",
    "             na_cols: bool = True,\n",
    "             cnn_leaky_relu: bool = False,\n",
    "             patience: int = 8,\n",
    "             factor: float = 0.5):\n",
    "    seed_everything(seed)\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    y = y.values.astype(np.float32)\n",
    "    X_num, X_cat, cat_cols, scaler = preprocess_nn(X.copy(), scaler_type=scaler_type, n_pca=n_pca, na_cols=na_cols)\n",
    "\n",
    "    best_losses = []\n",
    "    best_predictions = []\n",
    "    kfold = KFold(n_splits = 5, random_state = 2021, shuffle = True)\n",
    "    # Iterate through each fold\n",
    "    \n",
    "    for cv_idx, (train_idx, valid_idx) in enumerate(kfold.split(train)):\n",
    "        X_tr, X_va = X_num[train_idx], X_num[valid_idx]\n",
    "        X_tr_cat, X_va_cat = X_cat[train_idx], X_cat[valid_idx]\n",
    "        y_tr, y_va = y[train_idx], y[valid_idx]\n",
    "\n",
    "        cur_batch = batch_size\n",
    "        best_loss = 1e10\n",
    "        best_prediction = None\n",
    "\n",
    "        print(f\"fold {cv_idx} train: {X_tr.shape}, valid: {X_va.shape}\")\n",
    "\n",
    "        train_dataset = TabularDataset(X_tr, X_tr_cat, y_tr)\n",
    "        valid_dataset = TabularDataset(X_va, X_va_cat, y_va)\n",
    "        train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=cur_batch, shuffle=True,\n",
    "                                                   num_workers=4)\n",
    "        valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=cur_batch, shuffle=False,\n",
    "                                                   num_workers=4)\n",
    "\n",
    "        if model_type == 'mlp':\n",
    "            model = MLP(X_tr.shape[1],\n",
    "                        n_categories=[128],\n",
    "                        dropout=mlp_dropout, hidden=mlp_hidden, emb_dim=emb_dim,\n",
    "                        dropout_cat=dropout_emb, bn=mlp_bn)\n",
    "        elif model_type == 'cnn':\n",
    "            model = CNN(X_tr.shape[1],\n",
    "                        hidden_size=cnn_hidden,\n",
    "                        n_categories=[128],\n",
    "                        emb_dim=emb_dim,\n",
    "                        dropout_cat=dropout_emb,\n",
    "                        channel_1=cnn_channel1,\n",
    "                        channel_2=cnn_channel2,\n",
    "                        channel_3=cnn_channel3,\n",
    "                        two_stage=False,\n",
    "                        kernel1=cnn_kernel1,\n",
    "                        celu=cnn_celu,\n",
    "                        dropout_top=cnn_dropout,\n",
    "                        dropout_mid=cnn_dropout,\n",
    "                        dropout_bottom=cnn_dropout,\n",
    "                        weight_norm=cnn_weight_norm,\n",
    "                        leaky_relu=cnn_leaky_relu)\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "        model = model.to(device)\n",
    "\n",
    "        if optimizer_type == 'adamw':\n",
    "            opt = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        elif optimizer_type == 'adam':\n",
    "            opt = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "\n",
    "        scheduler = epoch_scheduler = None\n",
    "        if scheduler_type == 'onecycle':\n",
    "            scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer=opt, pct_start=0.1, div_factor=1e3,\n",
    "                                                            max_lr=max_lr, epochs=epochs,\n",
    "                                                            steps_per_epoch=len(train_loader))\n",
    "        elif scheduler_type == 'reduce':\n",
    "            epoch_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer=opt,\n",
    "                                                                         mode='min',\n",
    "                                                                         min_lr=1e-7,\n",
    "                                                                         patience=patience,\n",
    "                                                                         verbose=True,\n",
    "                                                                         factor=factor)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            if epoch > 0 and epoch % batch_double_freq == 0:\n",
    "                cur_batch = cur_batch * 2\n",
    "                print(f'batch: {cur_batch}')\n",
    "                train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                                           batch_size=cur_batch,\n",
    "                                                           shuffle=True,\n",
    "                                                           num_workers=4)\n",
    "            train_loss = train_epoch(train_loader, model, opt, scheduler, device)\n",
    "            predictions, valid_targets, valid_loss, rmspe = evaluate(valid_loader, model, device=device)\n",
    "            print(f\"epoch {epoch}, train loss: {train_loss:.3f}, valid rmspe: {rmspe:.3f}\")\n",
    "\n",
    "            if epoch_scheduler is not None:\n",
    "                epoch_scheduler.step(rmspe)\n",
    "\n",
    "            if rmspe < best_loss:\n",
    "                print(f'new best:{rmspe}')\n",
    "                best_loss = rmspe\n",
    "                best_prediction = predictions\n",
    "                torch.save(model, os.path.join(output_dir, model_path.format(cv_idx)))\n",
    "\n",
    "        best_predictions.append(best_prediction)\n",
    "        best_losses.append(best_loss)\n",
    "        del model, train_dataset, valid_dataset, train_loader, valid_loader, X_tr, X_va, X_tr_cat, X_va_cat, y_tr, y_va, opt\n",
    "        if scheduler is not None:\n",
    "            del scheduler\n",
    "        gc.collect()\n",
    "\n",
    "    return best_losses, best_predictions, scaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "caaf0190",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-16T18:05:24.450910Z",
     "iopub.status.busy": "2023-12-16T18:05:24.450286Z",
     "iopub.status.idle": "2023-12-16T18:05:24.453187Z",
     "shell.execute_reply": "2023-12-16T18:05:24.452622Z",
     "shell.execute_reply.started": "2023-12-15T06:02:17.285321Z"
    },
    "papermill": {
     "duration": 0.045794,
     "end_time": "2023-12-16T18:05:24.453316",
     "exception": false,
     "start_time": "2023-12-16T18:05:24.407522",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import torch\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# print(device)\n",
    "\n",
    "# def get_X(df_src):\n",
    "#     cols = [c for c in df_src.columns if c not in ['time_id', 'target', 'tick_size', 'row_id']]\n",
    "#     return df_src[cols]\n",
    "\n",
    "# X = get_X(train)\n",
    "# y = train['target']\n",
    "\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# print(device)\n",
    "\n",
    "# gc.collect()\n",
    "\n",
    "# def get_top_n_models(models, scores, top_n):\n",
    "#     if len(models) <= top_n:\n",
    "#         print('number of models are less than top_n. all models will be used')\n",
    "#         return models\n",
    "#     sorted_ = [(y, x) for y, x in sorted(zip(scores, models), key=lambda pair: pair[0])]\n",
    "#     print(f'scores(sorted): {[y for y, _ in sorted_]}')\n",
    "#     return [x for _, x in sorted_][:top_n]\n",
    "\n",
    "\n",
    "# if True:\n",
    "#     tab_model = []\n",
    "#     scores = []\n",
    "        \n",
    "#     if SHORTCUT_NN_IN_1ST_STAGE and IS_1ST_STAGE:\n",
    "#         print('shortcut to save quota...')\n",
    "#         epochs = 10\n",
    "#         valid_th = 1000\n",
    "#     else:\n",
    "#         print('train full')\n",
    "#         epochs = 250\n",
    "#         valid_th = 1000\n",
    "\n",
    "#     for i in range(TABNET_NUM_MODELS):\n",
    "#         nn_losses, nn_preds, scaler, model = train_tabnet(X, y,  \n",
    "#                                                           #[folds[-1]], \n",
    "#                                                           batch_size=1280,\n",
    "#                                                           epochs=epochs, #epochs,\n",
    "#                                                           lr=0.04,\n",
    "#                                                           patience=50,\n",
    "#                                                           factor=0.5,\n",
    "#                                                           gamma=1.6,\n",
    "#                                                           lambda_sparse=3.55e-6,\n",
    "#                                                           seed=i,\n",
    "#                                                           n_a=36)\n",
    "#         if nn_losses[0] < valid_th:\n",
    "#             tab_model.append(model)\n",
    "#             scores.append(nn_losses[0])\n",
    "#             np.save(f'pred_tab_seed{i}.npy', nn_preds[0])\n",
    "#             model.save_model(f'artifacts/tabnet_fold_0_seed{i}')\n",
    "            \n",
    "#     tab_model = get_top_n_models(tab_model, scores, TAB_MODEL_TOP_N)\n",
    "#     print(f'total {len(tab_model)} models will be used.')\n",
    "    \n",
    "# # about 1 hr\n",
    "# # after pca to 100: about 45 min, weaker performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6c1ac7d0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-16T18:05:24.531307Z",
     "iopub.status.busy": "2023-12-16T18:05:24.530713Z",
     "iopub.status.idle": "2023-12-16T18:05:24.533582Z",
     "shell.execute_reply": "2023-12-16T18:05:24.533051Z",
     "shell.execute_reply.started": "2023-12-15T11:50:32.532600Z"
    },
    "papermill": {
     "duration": 0.043072,
     "end_time": "2023-12-16T18:05:24.533701",
     "exception": false,
     "start_time": "2023-12-16T18:05:24.490629",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# X_test = get_X(test)\n",
    "# tab_preds = predict_tabnet(X_test, tab_model, scaler, ensemble_method=ENSEMBLE_METHOD).flatten() \n",
    "# print(tab_preds)\n",
    "# 0.20593"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a708f58",
   "metadata": {
    "papermill": {
     "duration": 0.037272,
     "end_time": "2023-12-16T18:05:24.609383",
     "exception": false,
     "start_time": "2023-12-16T18:05:24.572111",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## LGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2b8ff508",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-16T18:05:24.702532Z",
     "iopub.status.busy": "2023-12-16T18:05:24.698059Z",
     "iopub.status.idle": "2023-12-16T18:29:27.560535Z",
     "shell.execute_reply": "2023-12-16T18:29:27.561028Z",
     "shell.execute_reply.started": "2023-12-15T11:50:38.974622Z"
    },
    "papermill": {
     "duration": 1442.914191,
     "end_time": "2023-12-16T18:29:27.561234",
     "exception": false,
     "start_time": "2023-12-16T18:05:24.647043",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type='text/css'>\n",
       ".datatable table.frame { margin-bottom: 0; }\n",
       ".datatable table.frame thead { border-bottom: none; }\n",
       ".datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n",
       ".datatable .bool    { background: #DDDD99; }\n",
       ".datatable .object  { background: #565656; }\n",
       ".datatable .int     { background: #5D9E5D; }\n",
       ".datatable .float   { background: #4040CC; }\n",
       ".datatable .str     { background: #CC4040; }\n",
       ".datatable .time    { background: #40CC40; }\n",
       ".datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n",
       ".datatable .frame tbody td { text-align: left; }\n",
       ".datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n",
       ".datatable th:nth-child(2) { padding-left: 12px; }\n",
       ".datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n",
       ".datatable .sp {  opacity: 0.25;}\n",
       ".datatable .footer { font-size: 9px; }\n",
       ".datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n",
       ".datatable .frame thead tr.colnames {  background-image: url('data:image/png;base64,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');  background-repeat: repeat-x;  background-size: 14px;  height: 28px;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training fold 1\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[250]\ttraining's rmse: 0.000429005\ttraining's RMSPE: 0.198406\tvalid_1's rmse: 0.000441736\tvalid_1's RMSPE: 0.205029\n",
      "[500]\ttraining's rmse: 0.000406641\ttraining's RMSPE: 0.188063\tvalid_1's rmse: 0.000427725\tvalid_1's RMSPE: 0.198526\n",
      "[750]\ttraining's rmse: 0.000392987\ttraining's RMSPE: 0.181748\tvalid_1's rmse: 0.000420488\tvalid_1's RMSPE: 0.195167\n",
      "[1000]\ttraining's rmse: 0.000382851\ttraining's RMSPE: 0.177061\tvalid_1's rmse: 0.000416324\tvalid_1's RMSPE: 0.193234\n",
      "[1250]\ttraining's rmse: 0.000374682\ttraining's RMSPE: 0.173283\tvalid_1's rmse: 0.000413803\tvalid_1's RMSPE: 0.192064\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1300]\ttraining's rmse: 0.000373224\ttraining's RMSPE: 0.172609\tvalid_1's rmse: 0.000413174\tvalid_1's RMSPE: 0.191772\n",
      "Training fold 2\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[250]\ttraining's rmse: 0.000428514\ttraining's RMSPE: 0.198526\tvalid_1's rmse: 0.000440305\tvalid_1's RMSPE: 0.20294\n",
      "[500]\ttraining's rmse: 0.000406233\ttraining's RMSPE: 0.188203\tvalid_1's rmse: 0.000425532\tvalid_1's RMSPE: 0.196131\n",
      "[750]\ttraining's rmse: 0.000392085\ttraining's RMSPE: 0.181648\tvalid_1's rmse: 0.00041752\tvalid_1's RMSPE: 0.192438\n",
      "[1000]\ttraining's rmse: 0.000381894\ttraining's RMSPE: 0.176927\tvalid_1's rmse: 0.000412993\tvalid_1's RMSPE: 0.190351\n",
      "[1250]\ttraining's rmse: 0.000373694\ttraining's RMSPE: 0.173128\tvalid_1's rmse: 0.000410175\tvalid_1's RMSPE: 0.189053\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1300]\ttraining's rmse: 0.000372205\ttraining's RMSPE: 0.172438\tvalid_1's rmse: 0.000409671\tvalid_1's RMSPE: 0.18882\n",
      "Training fold 3\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[250]\ttraining's rmse: 0.000428116\ttraining's RMSPE: 0.197988\tvalid_1's rmse: 0.000469098\tvalid_1's RMSPE: 0.217759\n",
      "[500]\ttraining's rmse: 0.000406189\ttraining's RMSPE: 0.187847\tvalid_1's rmse: 0.000454434\tvalid_1's RMSPE: 0.210952\n",
      "[750]\ttraining's rmse: 0.000392273\ttraining's RMSPE: 0.181412\tvalid_1's rmse: 0.000447918\tvalid_1's RMSPE: 0.207927\n",
      "[1000]\ttraining's rmse: 0.000382141\ttraining's RMSPE: 0.176726\tvalid_1's rmse: 0.000443544\tvalid_1's RMSPE: 0.205896\n",
      "Early stopping, best iteration is:\n",
      "[1130]\ttraining's rmse: 0.000377742\ttraining's RMSPE: 0.174692\tvalid_1's rmse: 0.00044219\tvalid_1's RMSPE: 0.205268\n",
      "Training fold 4\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[250]\ttraining's rmse: 0.000427888\ttraining's RMSPE: 0.198236\tvalid_1's rmse: 0.000445434\tvalid_1's RMSPE: 0.205302\n",
      "[500]\ttraining's rmse: 0.000405576\ttraining's RMSPE: 0.187899\tvalid_1's rmse: 0.000430323\tvalid_1's RMSPE: 0.198337\n",
      "[750]\ttraining's rmse: 0.000392129\ttraining's RMSPE: 0.181669\tvalid_1's rmse: 0.000422881\tvalid_1's RMSPE: 0.194907\n",
      "[1000]\ttraining's rmse: 0.000381803\ttraining's RMSPE: 0.176885\tvalid_1's rmse: 0.000418265\tvalid_1's RMSPE: 0.19278\n",
      "[1250]\ttraining's rmse: 0.000373708\ttraining's RMSPE: 0.173135\tvalid_1's rmse: 0.00041568\tvalid_1's RMSPE: 0.191588\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1300]\ttraining's rmse: 0.000372255\ttraining's RMSPE: 0.172462\tvalid_1's rmse: 0.0004153\tvalid_1's RMSPE: 0.191413\n",
      "Training fold 5\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[250]\ttraining's rmse: 0.000429063\ttraining's RMSPE: 0.19846\tvalid_1's rmse: 0.000443587\tvalid_1's RMSPE: 0.205775\n",
      "[500]\ttraining's rmse: 0.000407102\ttraining's RMSPE: 0.188302\tvalid_1's rmse: 0.00043016\tvalid_1's RMSPE: 0.199546\n",
      "[750]\ttraining's rmse: 0.000393288\ttraining's RMSPE: 0.181913\tvalid_1's rmse: 0.000422968\tvalid_1's RMSPE: 0.19621\n",
      "[1000]\ttraining's rmse: 0.00038307\ttraining's RMSPE: 0.177186\tvalid_1's rmse: 0.000418925\tvalid_1's RMSPE: 0.194334\n",
      "[1250]\ttraining's rmse: 0.000374749\ttraining's RMSPE: 0.173338\tvalid_1's rmse: 0.000415775\tvalid_1's RMSPE: 0.192873\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1300]\ttraining's rmse: 0.000373259\ttraining's RMSPE: 0.172648\tvalid_1's rmse: 0.00041566\tvalid_1's RMSPE: 0.19282\n",
      "Our out of folds RMSPE is 0.19410481639395613\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAh8AAAEWCAYAAADGuvWEAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8rg+JYAAAACXBIWXMAAAsTAAALEwEAmpwYAACAN0lEQVR4nO2deZhUxdWH3x8IoiIgi0ZQBET2ZRQkEpEMqLhvkWgSI4saFRc0iYhJjIASRXEX1C8i4i6IIsSdIOOCiqKyCIqiooKETUFAwAHO90dVz1ya7pkepmemp6n3efqZe+vWck7PQJ+uOvUrmRmBQCAQCAQC5UWVijYgEAgEAoHArkUIPgKBQCAQCJQrIfgIBAKBQCBQroTgIxAIBAKBQLkSgo9AIBAIBALlSgg+AoFAIBAIlCsh+AgEAoEMRdLfJY2paDsCgXSjoPMRCASyEUmLgf2ArZHiFmb2XSn7vMDM/ls66yofkoYCzc3sjxVtS6DyE2Y+AoFANnOKmdWMvHY68EgHknaryPF3lspqdyBzCcFHIBDYpZBUW9KDkpZJWippuKSq/tnBkl6TtFrSKkmPS6rjnz0KNAb+I2m9pKsl5UpaEtf/YknH+OuhkiZKekzSj0C/osZPYOtQSY/56yaSTFJ/Sd9K+kHSxZIOlzRX0hpJoyJt+0maIWmUpLWSPpV0dOR5Q0lTJH0vaZGkP8WNG7X7YuDvwNne9zm+Xn9Jn0haJ+lLSRdF+siVtETSXyWt8P72jzzfQ9Jtkr729r0laQ//7AhJb3uf5kjK3YlfdSCDCcFHIBDY1RgHbAGaA4cCvYAL/DMBNwENgdbAgcBQADM7F/iGwtmUW1Ic7zRgIlAHeLyY8VPhl8AhwNnAncA/gGOAtsBZkn4dV/cLoD4wBHhWUl3/7Clgife1N3CjpJ5J7H4QuBEY733v6OusAE4GagH9gTskHRbp4xdAbaARcD4wWtI+/tmtQCfgV0Bd4Gpgm6RGwAvAcF9+FfCMpAYleI8CGU4IPgKBQDbznP/2vEbSc5L2A04ErjSzDWa2ArgD+B2AmS0ys6lmttnMVgK3A79O3n1KvGNmz5nZNtyHdNLxU+QGM9tkZq8CG4AnzWyFmS0F3sQFNDFWAHeaWb6ZjQcWAidJOhA4Ehjs+5oNjAH6JLLbzDYmMsTMXjCzL8zxOvAqcFSkSj5wvR//RWA90FJSFeA84AozW2pmW83sbTPbDPwReNHMXvRjTwVm+fctkCWEdbxAIJDNnB5NDpXUBagGLJMUK64CfOuf7wfchfsA3ds/+6GUNnwbuT6oqPFTZHnkemOC+5qR+6W2/a6Cr3EzHQ2B781sXdyzzknsToikE3AzKi1wfuwJzItUWW1mWyL3P3n76gM1cLMy8RwE/FbSKZGyasD04uwJVB5C8BEIBHYlvgU2A/XjPhRj3AgY0N7Mvpd0OjAq8jx+e+AG3AcuAD53I355INqmuPHTTSNJigQgjYEpwHdAXUl7RwKQxsDSSNt4X7e7l7Q78AxutmSymeVLeg63dFUcq4BNwMHAnLhn3wKPmtmfdmgVyBrCsksgENhlMLNluKWB2yTVklTFJ5nGllb2xi0NrPW5B4PiulgONIvcfwbUkHSSpGrAtcDupRg/3ewLDJRUTdJvcXksL5rZt8DbwE2SakjqgMvJeKyIvpYDTfySCUB1nK8rgS1+FqRXKkb5JaixwO0+8bWqpK4+oHkMOEXScb68hk9ePaDk7gcylRB8BAKBXY0+uA/OBbgllYnA/v7ZMOAwYC0u6fHZuLY3Adf6HJKrzGwtcAkuX2IpbiZkCUVT1PjpZiYuOXUV8C+gt5mt9s9+DzTBzYJMAoYUo1/ytP+5WtKHfsZkIDAB58cfcLMqqXIVbonmfeB74Gagig+MTsPtrlmJmwkZRPi8yiqCyFggEAhkIZL64QTRulW0LYFAPCGSDAQCgUAgUK6E4CMQCAQCgUC5EpZdAoFAIBAIlCth5iMQCAQCgUC5EnQ+AoEUqFOnjjVv3ryizUgLGzZsYK+99qpoM9JGNvkTfMlcssmf8vTlgw8+WGVmO0jjh+AjEEiB/fbbj1mzZlW0GWkhLy+P3NzcijYjbWSTP8GXzCWb/ClPXyR9nag8LLsEAoFAIBAoV0LwEQgEAoFAoFwJwUcgEAgEAoFyJQQfgUAgEAgEypUQfAQCgUAgEChXQvARCAQCgcAuQJMmTWjfvj0XXHABnTt3BmDOnDl07dqV9u3bc8opp/Djjz8CMHXqVDp16kT79u3p1KkTr732WkE/ubm5tGzZkpycHHJyclixYkWJbQnBR6BCkXSlpD13su1QSVelWPd6ScckKM+V9PzOjB8IBAKVjenTpzNmzJgC6YALLriAESNGMG/ePM444wxGjhwJQP369fnPf/7DvHnzePjhhzn33HO36+fxxx9n9uzZzJ49m3333bfEdoTgI1DRXAnsVPBREszsumKOCw8EAoFdjs8++4zu3bsDcOyxx/LMM88AcOihh9KwYUMA2rZty8aNG9m8eXPaxg0iY4FyQ9JewATgAKAq8DTQEJguaZWZ9ZD0e+DvgIAXzGywb3s8cKNvt8rMjo7r+0/Ab4DfmNnGBGOPA543s4m+rzuBn4C3UrF9Y/5WmlzzQsmdzkD+2n4L/bLEF8guf4IvmUtl9mfxiJMAkESvXr1Yv349V111FRdeeCFt27Zl8uTJnH766Tz99NN8++23O7R/5plnOOyww9h9990Lyvr370/VqlU588wzufbaa5FUIpvCwXKBckPSmcDxZvYnf18bmAN0NrNVkhoC7wKdgB+AV4G7gRnAh0B3M/tKUl0z+17SUGA9sAk4FjjLzBKG5rHgw78+B3oCi4DxwJ5mdnKCNhcCFwLUr9+g03V3PpCW96Gi2W8PWL5DeFZ5ySZ/gi+ZS2X2p32j2gCsXLmSBg0asGTJEoYMGcLAgQPZZ599uOeee1i7di1HHnkkzz77LJMnTy5o+9VXX3Httddyyy230KhRo+36+emnnxgyZAjHHHMMxx13XMKxe/To8YGZdY4vDzMfgfJkHnCbpJtxsxBvxkXLhwN5ZrYSQNLjQHdgK/CGmX0FYGbfR9r0Ab4FTjez/BRsaAV8ZWaf+zEewwcY8ZjZv4F/A7Rs2dIuP+e0lB3NZPLy8jgrS2SiIbv8Cb5kLtnkT15eHueeey75+fn06dOHPn36AG4JZv78+QXS60uWLOHCCy9kwoQJHHnkkQn7WrFiBbNmzSqxXHvI+QiUG2b2GXAYLggZLum6NHQ7D2iCW8oJBAKBQAI2bNjAunXrANi4cSOvvvoq7dq1K9ipsm3bNoYPH87FF18MwJo1azjppJMYMWLEdoHHli1bWLVqFQD5+fk8//zztGvXrsT2hOAjUG74ZZWfzOwxYCQuEFkH7O2rvAf8WlJ9SVWB3wOv45Ziuktq6vupG+n2I+AiYIrvvzg+BZpIOtjf/76UbgUCgUDGs3z5crp160bHjh0ZMGAAJ510EscffzxPPvkkLVq0oFWrVjRs2JD+/fsDMGrUKBYtWsT111+/3ZbazZs3c9xxx9GhQwdycnJo1KgRf/rTn0psT1h2CZQn7YGRkrYB+cAAoCvwsqTvfMLpNcB0ChNOJ0NB/sWzkqoAK3A5HgCY2Vt+y+0Lko41s1XJDDCzTb6vFyT9BLxJYfATCAQCWUmzZs2YM2cOsP2ptldccQVXXHHFDvWvvfZarr322oR9ffDBB6W2JwQfgXLDzF4BXokrngXcE6nzJPBkgrYvAS/FlQ0tpu9o3X6R65dxuR+BQCAQqADCsksgkIFs3bqVQw89lJNPdptwXnvtNQ477DDatWtH37592bJlCwA//PADZ5xxBh06dKBLly58/PHHFWl2IBAIpEQIPgJZhaTRkmbHvfpXtF0l5a677qJ169aASwTr27cvTz31FB9//DEHHXQQDz/8MAA33ngjOTk5zJ07l0ceeSTh9GkgEAhkGiH4SAOS6ki6pJg6TST9IYW+mkhK29dXSf0kjUpXf5mIpL6SPpf0OfCemeXEvR6K1G0l6R1Jm1OVZi9vlixZwgsvvMAFF1wAwOrVq6levTotWrQAtlchXLBgAT179gSgVatWLF68mOXLl1eM4YFAIJAiIecjPdQBLgHuLaJOE+APwBPlYM8ug9/5MgToDBjwgaQpZvZDkibfAwOB00syTnkpnC4ecRJXXnklt9xyS8G2uPr167NlyxZmzZpF586dmThxYoEKYceOHXn22Wc56qijeO+99/j6669ZsmQJ++23X5nbGggEAjtLCD7SwwjgYEmzgam+7ATch+FwMxvv67T2dR4GJgGPAnv5+peZ2dvFDSTpXeB8M5vv7/OAq4AvgbFAM5xs+IVmNjeu7Ti8xLi/X29mNSXlAsOANbgdKRNw+hlXAHvgBLy+kNQAuB9o7Lu80sxmJLHz18Bd/tZwYmGdgKtiaqJ+RmaWmY2TtBiXaHoCsAUn/HUT0BwYaWb3J3lLjgOmxoTHJE0FjgeeTCTJbmYrgBWSTkrSX9SHqMIp17XfUlyTUnPTTTeRn5/PunXrmD17NqtXr+b111/n6quv5rzzziM/P5/OnTuzceNG8vLyOPLIIxk1ahTNmzenWbNmNG/enI8++qggcEnE+vXrycvLK3Nfyots8if4krlkkz8Z4YuZhVcpX7hZjY/99Zm4AKQqsB/wDbA/kIv74I+12ROo4a8PwX0Ib9dXkrH+DAzz1/sDC/31PcAQf90TmO2v+wGj/PU4oHekr/X+Zy4u8Ngf2B1YGhnjCuBOf/0E0M1fNwY+KcLO/wBH+uuauEA3/j0YBfTz14uBAf76DmAubgtsA2B5EeNcBVwbuf+nL2uAUz5t6svrxrUbiguEUvodt2jRwsqDa665xho1amQHHXSQ7bfffrbHHnvYOeecs12dV155xX7729/u0Hbbtm120EEH2dq1a4scY/r06ek0ucLJJn+CL5lLNvlTnr7EPtviXyHnI/10A540s61mthwnknV4gnrVgAckzcMdsNYmxf4nAL399VnAxMi4jwKY2WtAPUm1SmD3+2a2zNzZKF/gzlWBQgVRgGOAUX72ZgpQS1LNJP3NAG6XNBCoY2apTBtMiYw508zWmZNa3yypTgl8ATiC5JLsGctNN93EkiVLWLx4MU899RQ9e/bkscceK1Ah3Lx5MzfffPN2KoQ///wzAGPGjKF79+7UqlWSX3sgEAiUP2HZpeL4M7Ac6IhL/N2USiMzWypptaQOwNnAxSUYc4sfCy/WVT3yLHog27bI/TYK/06qAEeYWbG2mtkISS8AJwIzJB0XHd9TI65ZdMx4e5L9rS7FzajEOADIK86+ysbIkSN5/vnn2bZtGwMGDChIMv3kk0/o27cvkmjbti0PPvhgBVsaCAQCxRNmPtJDVCL8TeBsSVV9jkR3nGx4tA5AbWCZmW0DzsUt06TKeOBqoLYV5nW8CZwD4HM4VpnZj3HtFuPyLgBOxc2+lIRXgctjN5JyklWUdLCZzTOzm4H3caJeXwNtJO3uZzKOLuH4iXgF6CVpH0n7AL18WVGS7JWC3Nxcnn/+ecAFH5988gkLFy7kyiuvLKjTtWtXPvvsMxYuXMizzz7LPvvsU0HWBgKBQOqEmY80YGarJc3wW2RfwuUrzMElWl5tZv+TtBrYKmkOLvfiXuAZSX2Al4ENJRhyIi6Z84ZI2VBgrKS5uITTvgnaPQBM9jaUdExwu0RG+zF2A94g+czLlZJ64GYt5gMvmdlmSROAj4GvcOeylAoz+17SDbgAB+B6K0w+3UGSXdIvcKqqtYBtkq4E2iQI1AKBQCBQRoTgI02YWbyGx6C45/m4RNAoHSLXg329xUCRRwT6XJLd4sq+J8H2UTMbhwt2Yu2OSDBmHpGlCjPLjVwXPDN3ZsrZRdkWaXd5kvKrcbM28eVNEtkc/yxJn2NxO33iyxNJsv+PDD0Bd+vWrXTu3JlGjRrx/PPPc/755zNr1qxYwivjxo2jZs2abN68mT59+vDBBx9Qr149xo8fT5MmTSra/EAgEEiZsOwSCGQIUVVTgDvuuIM5c+Ywd+5cGjduzKhRTivuwQcfZJ999mHRokX8+c9/ZvDgwRVlciAQCOwUIfjIUCQdl0AmfFIZjjdGUqo7bqLt+iewc1qKx9sn6/NYSR9Imud/9pTUPsE4M339lyXNkTRf0v2Sisyf8fXXSHp+Z21MN/GqpkDBrhUzY+PGjUgCYPLkyfTt61bVevfuzbRp02LbhwOBQKBSEJZdMhQr5pTWMhjvguJrJWz3EPBQtMwLnzUEvttJc1YBp5jZd5LaAa+YWSMgJ0n9s8zsR7lP54nAb4Gniuh/JE5n5aJUDSorhdPFI5zWWbyqaYz+/fvz4osv0qZNG2677TYAli5dyoEHHgjAbrvtRu3atVm9ejX169dPu32BQCBQFoTgYxdE0l44vZADcLtsbgAG4MS5GgLX+6p7ANXNrKmkTsDtOMGwVThxsGUJ+u6Nkzp/XNJGoCsu/+UU39/bwEVmZjF1VjObJak+ToymiZlFE1HnA3tI2t1rkOxAJFl0N9z2YfO2NMcpsjYAtgK/NbMvzGya3xFU3PtU5gqneXl5vPPOOzuomsbUB/v27csf//hH7r77boYNG8YJJ5zAhg0beOedd2jQoAEAmzZtYsaMGdSuXTulMTNC3TCNZJM/wZfMJZv8yQhfEimPhVd2v3AqrA9E7mvjkko7x9WbAFyK25L7NtDAl58NjC2i/+36IqIuihNCOyW+HlAfWJygr97Af1Pw6RXgB5wKa1VfNhM4w1/XAPaM1M8lorZa3KssFU5TUTV9/fXX7aSTTjIzs169etnbb79tZmb5+flWr14927ZtW8rjZZNSo1l2+RN8yVyyyZ+gcBqoKObhtp3eLOkoM1sbX0HS1cBGMxsNtMTtwJnq1U2vpWQ7RnpImunVXHsCbVNpJKktcDMpLI+Y2XEUysP3lLQ30MjMJvnnm8zspxLYXG4kUjV99NFHWbRoEeC+IEyZMoVWrVoBcOqpp/Lwww8DMHHiRHr27FmQDxIIBAKVgbDssgtiZp9JOgynPjpc0rToc0nH4PImuseKgPlm1rWkY0mqgdM06Wxm30oaSqGyaVTxtEZcuwNwh+/1MbMvUvRrk6TJwGk4kbFKi5nRt29ffvzxR8yMjh07ct999wFw/vnnc+6559K8eXPq1q3LU08Vld4SCAQCmUcIPnZB/E6U783sMUlrgAsizw4CRgPHmdlGX7wQaCCpq5m9I6ka0ML8yboJiKq5xoKKVf4cmN4UnkezGKe4+h6F59Xg1U9fAK6xJKfmRurWBPY2s2WSdgNOAt40s3WSlkg63cyek7Q7bjkmI2c/YuTm5pKbmwvAjBmJXa9RowZPP/10OVoVCAQC6SUsu+yatAfe80soQ4DhkWf9gHrAc34764tm9jMuOLjZq6POBn5VRP/jgPt9/5txyqof4/Iy3o/UuxUYIOkjXM5HjMuA5sB1kW21+yYZay9gilddnY1TMr3fPzsXGOifvQ38AkDSm7jD/I72AcpxRfgSCAQCgTQTZj52QSzxNt5c/3MWMCxBm9kULsMU1/8zwDORomv9K77ep2yv8nqtLx/O9gFRUWMtJ/GpwZjZ5+yoKouZHZVK34FAIBAoG8LMRyBQQWzatIkuXbrQsWNH2rZty5AhQwCYNm0ahx12GDk5OXTr1q0g8fSbb76hR48eHHrooXTo0IEXX3yxIs0PBAKBnSbMfAR2GkmjgSPjiu8yJzxWFuPNxO1miXKumc0ri/HKmt13353XXnuNmjVrkp+fT7du3TjhhBMYMGAAkydPpnXr1tx7770MHz6ccePGMXz4cM466ywGDBjAggULOPHEE1m8eHFFuxEIBAIlJgQfacAnSP7BzO4tok4T4Fdm9kQxfTXB6U8UebhcCWzrh9tpclk6+otiZpemu89ixvtlonJJL+MOzHvLzE4uqg9J9XAJr4cD48rifUkVSdSsWROA/Px88vPzkYQkfvzR6aatXbuWhg0bFtRPVB4IBAKVjRB8pIc6wCW4LaXJaAL8ASeCFUgvJZFL3wT8E6dbknKAl2559Zis+tatW+nUqROLFi3i0ksv5Ze//CVjxozhxBNPZI899qBWrVq8+67bNTx06FB69erFPffcw4YNG/jvf/+bNnsCgUCgPJETIAuUBklP4bQlFgJTffEJOJnv4WY2XtK7QGvgK+BhnIbFo7jdGgCXmdnbxc18+H7Oj21zjUmUA1/ijpVvBvwEXGhmc6MzH5LG+b4n+rbrzaymlxofBqzB7YSZgBMiuwIniX66mX0hqQFuJ0ljb86VybbCSvo1cJe/NVyyaiecnPrJvs4onPrdOEmLgSf9+7YFJ2t+E27Xy0gzu58i8D5cFZ35kHS4t2Ev3K6bo81snX9W8L4U0WdUXr3TdXc+UJQJJaJ9o+2l0NevX88///lPBg4cyEMPPcTvfvc72rRpw1NPPcW3337LoEGDmDBhAgBnnXUW8+fPZ+TIkYwdO5YqVUqWurV+/fqCGZdsIJv8Cb5kLtnkT3n60qNHjw/MrPMODxLJnoZXieXKmwAf++szcQFIVWA/4Buc8mYuETlv3Df1Gv76ELwEbbSvJGP9GRjmr/cHFvrre4Ah/ronMNtf9wNG+etxQO9IX+utUGp8DYUKoUsjY1wB3OmvnwC6+evGwCdF2Pkf4Eh/XRM3yxb/HozCnREDTvNjgL++A5iL0wppACxP4XcQ33d1XEB2uL+vBewWeV7wvqTyKkt59RjDhg2zW265xZo1a1ZQ9vXXX1vr1q3NzKxNmzb2zTffFDxr2rSpLV++vMTjZJNMtFl2+RN8yVyyyZ8gr56ddAOeNLOt5raBvk7iraDVgAe85PjTQKrH2U+gUJDrLAoFu7rhZlIws9eAepJqlcDu981smbnD274AXvXl83ABEcAxwCiv3zEFqOVFvhIxA7hd0kCgjpmlcirblMiYM81snZmtBDb7vJqS0BJYZmbvgzt8LkUbyo2VK1eyZs0aADZu3MjUqVNp3bo1a9eu5bPPPgMoKANo3Lgx06Y5MdpPPvmETZs2FRwuFwgEApWJkPNRcfwZWA50xG153pRKIzNbKmm1pA64A94uLsGYBXLmkqrgZgdiRE+M3Ra530bh30kV4AgzK9ZWMxsh6QWchPsML+QVlVOHOEn1uDHj7cm6v9Vly5bRt29ftm7dyrZt2zjrrLM4+eSTeeCBBzjzzDOpUqUK++yzD2PHjgXgtttu409/+hN33HEHkhg3blw40yUQCFRKsu4/9AoiKif+JnCRpIeBurhch0FAo0gdcCfJLjGzbZL64pZpUmU8cDVQ28zmRsY9B7jB5z+sMrMf4z6cFuPyLiYAp+JmX0rCq8DluARPJOWYEx/bAUkHm9sCO8/nXrQCPgDaeKnzPYCjgbdKaEOqLAT2l3S4mb3vD5rbmEmzHx06dOCjjz7aofyMM87gjDPO2KG8TZs2SSXXA4FAoDIRgo80YGarJc2Q9DHwEi5fYQ4u0fJqM/ufpNXAVi9PPg63M+YZSX2Al4ENJRhyIi6R8oZI2VBgrJcS/wnom6DdA8Bkb0NJxwQYCIz2Y+wGvEHymZcrJfXAzVrMB14ys82SJuCk1r8Cdvzk3Qm8XHoroKakJbiE3FcknQ3cI2kPYCNu2Wi9T26tBVSXdDrQy8wWpMOWQCAQCBRPCD7ShJn9Ia5oUNzzfHaU+o5Kiw/29RZTzBZQn0uyW1zZ98DpCeqOwwU7sXZHJBgzD8iLtMmNXBc8M7NVuKWeYjGzy5OUX42btYkvb5LI5vhnSfpMKJfu8z2OSFBeZH/lwaZNm+jevTubN29my5Yt9O7dm2HDhjFt2jQGDRrEtm3bqFmzJuPGjaN58+a88cYbXHnllcydO5ennnqK3r17Fz9IIBAIZCgh4TQQqABi6qZz5sxh9uzZvPzyy7z77rsMGDCAxx9/nNmzZ/OHP/yB4cPdETeNGzdm3Lhx/OEP8TFuIBAIVD7CzEeG4hM0b44r/srMdkwGSM94Y4DbS7r8IKk/bjtulNU42fPvdtKWY4ERuITYn3GzSCvxu3kibDazX0qqjtu2m4tb5vmHucPtkvU/FjgZWGFpUpItKSVVN23SpAlAiTU9AoFAIBMJwUeGYolPni3L8S7YyXYPAdud5eKFzxoCOxV8AKuAU8zsO0ntgFfMrBGQk6T+P3CBRAu/i6duMf2PwwUrj6RqUFkonJZE3TQQCASyiaBwugsiaS/cjpcDcLtsbgAG4JRSGwLX+6p7ANXNrKmkTsDtOMGwVThxsGUJ+u6N+3Bfikvy7IqbuTjF9/c2cJGZWUyd1cxmSaqPE6NpEtefcDMp+3sNkkT+fAu0MrMNceX74RRZm/miAWb2tn/WhGLO0CkvhdNU1E1jjBgxgq5du/LrX/96p8fOJqVGyC5/gi+ZSzb5ExROw6tCXjgV1gci97VxSaWd4+pNAC7Fbcl9G2jgy88GxhbR/3Z9AXUj14/iZjW2qwfUBxYn6Ks38N8ixqoDfIsLjD7ECbbt55+Nx0nAgwuyakfaNaEIJdn4V1krnBanbhqjb9++9vTTT5dqrGxSajTLLn+CL5lLNvkTFE4DFcU84FhJN0s6yszWxleQdDVOF2M0Ti20HTDVq5tei5s1SZUekmZ6NdeeQNtUGklqi8t7KerAuN28LW+b2WHAO8Ct/llP4D4Ac4qzO/hZUZRU3TQQCASyiZDzsQtiZp9JOgynPjpc0rToc0nHAL/FCaQBCJhvZl1LOpakGjhNk85m9q2koRQqm0YVT2vEtTsAd/heHzP7ooghVuN0TZ71908D55fUzvKmpOqm77//PmeccQY//PAD//nPfxgyZAjz58+vYC8CgUBg5wjBxy6IpIbA92b2mKQ1wAWRZwcBo4HjzGyjL14INJDU1czekVQNaGH+ZN0ERBVfY0HFKn8OTG8Kz6NZjFNcfY/C82rw57i8AFxjSU7NjWFmJuk/uJ0ur+FUU2M7dqbhclnulFQVqJkpsx8lVTc9/PDDWbJkSXmYFggEAmVOWHbZNWkPvOeXUIYAwyPP+gH1gOckzZb0opn9jAsObvbqqLOBXxXR/zjgft//Zpyy6se43TvvR+rdCgyQ9BEu5yPGZUBz4Dpvw2xJ+xYx3mBgqFdePRf4qy+/ArfkMw8v7Q4g6Unc8kxLSUskZfxMSSAQCGQTYeZjF8QSb+PN9T9nAcMStJlN4TJMcf0/A0R1Nq71r/h6n7K9yuu1vnw42wdExY33dSLbzCm6npag/Pep9l1WJFM4NTOuvfZann76aapWrcqAAQMYOHAgP/zwA+eddx5ffPEFNWrUYOzYsbRrVyESJYFAIFBqQvARCFQAMYXTmjVrkp+fT7du3TjhhBP45JNP+Pbbb/n000+pUqUKK1asAODGG28kJyeHSZMm8emnn3LppZcybdq0YkYJBAKBzCQsu1RCJF3vk0Ir2o7RkWWR2Kt/CftYX4K6MxOM177kllc8yRRO77vvPq677roCJdN993WrTQsWLKBnT3c0UKtWrVi8eDHLly+vGOMDgUCglISZj0qGpKpmdl1F2+G5wsrxiHoz+2V5jRVPOhVOF484CSChwukXX3zB+PHjmTRpEg0aNODuu+/mkEMOoWPHjjz77LMcddRRvPfee3z99dcsWbKE/fbbLy02BQKBQHkSFE4zCK+6+TIuOfIw3FH0fXC7N8YDxwK3AMfj1DknSjocuAvYC5fceTRu6+kIXB7H7sBoM/u/JGPu7/uuhQtGB5jZm35G4gGgF/A/4HdmttKrks4GugFP4oTCdlA+lfQnnDpodWAR7qyXnyQ1BZ7w9SfjRMASSu1JysXln6zBJclOwGmUXIFTSz3dzL6Q1ACnZNrYN73SzGZI6uLfmxo4tdX+ZrZQUj/gVGBP4GBgkrnTduPHLxOF06i6KWyvcHrJJZfQv39/zjrrLN544w0mTpzI3XffzYYNGxg1ahSff/45zZo145tvvuGqq66iefPmJR4/m5QaIbv8Cb5kLtnkTyYonIbgI4PwwcdXQDf/4TkWF3hcBtxrZrf4euOA54EpwKfA2Wb2vqRauMDjPGBfMxsuaXdgBvBbM/sqwZh/BWqY2b/8dtQ9zWydJAP+aGaPS7rO93eZDz4WmNklfsvt68BpPjA5G7dF9zxJ9cxstR9jOLDczO6RNAWYaGaPSLoUuLmY4OM5oDXwPfAlMMbMhki6AmhqZldKesK/P29Jaow7C6Z17P0wsy1+mWqAmZ3pg4/rgENxAdtC/55/m+x307hZc6ty1l1Jf3clITbzEeX6669nzz33ZMyYMbz00ks0bdoUM6NOnTqsXbv97mAzo2nTpsydO5datWqVePy8vDxyc3N31vyMI5v8Cb5kLtnkT3n6Iilh8BGWXTKPbyPaFo8BA/31+AR1WwLLzOx9ADP7EUBSL6CDP2cFnHz6IbjAJp73gbE+kHjO72oBdzpsbMzHKBTxitoSVT4FJ2EeO++lnQ866uBmOWK7a47EybuDk1qPP7l3B/vMnyEj6QvgVV8+D+jhr48B2ngbAGp5TZHawMOSDgEMJxMfY1pM80PSAuAgnEx7QvaoVpWFCYKGnWXlypVUq1aNOnXqFCicDh48mNNPP53p06fTtGlTXn/9dVq0aAHAmjVr2HPPPalevTpjxoyhe/fuOxV4BAKBQCYQgo/MI34qKna/Ib5iEQi43G+pLXowszckdQdOAsZJut3MEp32GrUrZktRyqfjcMsic/xMQ26Svoojepjctsj9Ngr/fqsAR5jZpmhDSaOA6WZ2hp9VykvS71bK+d9CMoXTbt26cc4553DHHXdQs2ZNxowZA8Ann3xC3759kUTbtm158MEHy9PcQCAQSCsh+Mg8GseURIE/AG/hlgcSsRDYX9Lhftllb1xuwys48a7XzCxfUgtgqcWd+goFiqZLzOwBv0RzGO6o+So4YbGnInYkGj+Z8unewDJfdg7ulFtwS0C/w82mnFPSNycJrwKXAyO9Tzl+Bqd2ZNx+aRorLSRTOK1Tpw4vvLBjYmvXrl0LznwJBAKByk7Yapt5LAQulfQJsA/+YLREeOXRs4F7vPLoVFxy5RhcrsiHkj4G/o/kgWYuMMerjJ6NS9AEN7vRxbfvCVyfZPxkyqf/BGbigo1PI82u8P7NAxolfRdKxkCgs6S5fgnlYl9+C3CT9y0E2oFAIJAhhP+QM48tZvbHuLIm0Rsz6xe5fh84IkE/f/evIjGzh4GHkzz7S4Ky3Lj72SRWF72PBIGTT3qNLtPsoHwaqZtHZKkkOnb0mZmtwgVO8e3fAVrEj2Vm43DLQrF6JyezIRAIBALpJ8x8BAIVwKZNm+jSpQsdO3akbdu2DBkyBHA7Wf7xj3/QokULWrduzd133w3AyJEjycnJIScnh3bt2lG1alW+//77inQhEAgEdpow85FBmNli3O6RtOOVQB+NK96cTLgr2fbXsqCktmUDJZVXHzRoEIMGDQLgP//5D3fccQd169atSBcCgUBgpwkzH7sIZjbPzHLiXgUf7pLGSGqTjrEk9ZPUcGdtAwYBu0maJ+kDST2LGe9fkr5NVapd0lhJK3w+S4VQUnn1KE8++SS//32Fn40XCAQCO02Y+QgAYGYXpLG7fsDHwHc72X4VcIqZfSepHW73TlHJqf8BRgGfp9j/OF8/0ZbihFS0vHqMn376iZdffplRo0alxZZAIBCoCELwsQsiaS+cVPkBOGGwG4ABwFVAQwp3tuwBVDezppI6kUBGPUHfvYHOwOOSNuKSSwcBp/j+3gYuMjPzaqlXmdksSfWBWWbWxMyie1DnA3tI2t3MNpMAM3vXjx1vy3442fVmvmiAmb3ttU2apPA+ReXVua59eo6xycvLK7i+8847C+TVW7VqxU8//cTSpUu59dZbeeONNzjzzDML8j4AXnvtNVq1asXcuXN3evz169dvZ0NlJ5v8Cb5kLtnkT0b4YmbhtYu9cAqjD0Tua+N2jnSOqzcBuBSnDPo20MCXnw2MLaL/7foC6kauH8XNamxXD6gPLE7QV2/gvyn6tT7ufjzunBdwQVbtyLMmwMepvmctWrSwsmTYsGE2cuRIa9mypX355ZdmZrZt2zarVavWdvVOP/10e/zxx0s11vTp00vVPtPIJn+CL5lLNvlTnr7gvlTu8H9qyPnYNZkHHCvpZklHmZcZjyLpamCjmY1mexn12bgtqweUYLwekmZ6bY+eQNtUGklqi5Nfv6gEY0Xpid/ua2ZbE/lZUaxcuZI1a9YAFMirt2rVqkBeHdhOXh1g7dq1vP7665x22mkVYXIgEAikjbDssgtiZp9JOgw4ERguaVr0uT+E7bcU6ncUJaNeJJJqAPfiZji+lTQUJ4QGsIXCpOcace0OACYBfczsi5KOm+mUVF4dYNKkSfTq1Yu99tqrAi0PBAKB0hOCj10QvxPlezN7TNIa4ILIs4OA0bjTaTf64qJk1BOxDievDoVBxSp/2FtvYKIvWwx0At7z5TEb6gAvANdY4SF7O8M0XC7Lnf7E3pqZMvtRUnl1gH79+tGvX78ytiwQCATKnrDssmvSHnjPL6EMAYZHnvUD6gHPSZot6UUrWkY9EeOA+33/m4EHcLtfXsGdohvjVtwZNB/hcj5iXAY0B67zNsyWtOOeU4+kWyQtAfaUtMTProCTcu/hl3s+ANr4+k8C7wAtff3zi/AlEAgEAmkmzHzsgpg77Tb+xNtc/3MWMCxBm9kkkFFP0v8zwDORomtJIKNuZp8CHeLqYWbD2T4gKm68q4GrE5QvB3ZIkDCzChfJ2LRpE927d2fz5s1s2bKF3r17M2zYMMyMa6+9lqeffpqqVasyYMAABg4cCLhdMldeeSX5+fnUr1+f119/vYK9CAQCgZ0jBB+BQAVQUoXTNWvWcMkll/Dyyy/TuHHjgvJAIBCojIRllzQgqY6kS4qp00TSH1Loq0k6lTe92miZKFJJGh1ZFom9+pfFWH68mQnGa++f1fJLKEX6KqmVpHckbZZ0VVnZWhwlVTh94okn+M1vfkPjxo23Kw8EAoHKSJj5SA91gEtwuzqS0QT4A/BEOdhTLpjZpeU8XlFnvdwAvJFCN98DA4HTSzJ2RSucfvbZZ+Tn55Obm8u6deu44oor6NOnT1rsCQQCgfImBB/pYQRwsE+wnOrLTgAMGG5m432d1r7Ow7htpI8CsX2Tl5nZ28UNJOld4PzYTpOYSijwJTAWp+b5E3Chmc2NazsOeN7MJvr79WZWU1IuLs9jDS4ZdQJOC+QKnCrp6Wb2haQGOMXQxr7LK5PtRpH0a+Auf2u4fJFOOEXTk32dUTgBmnGSFgNP+vdtC05Z9CZc4ulIM7u/iPekE7Af8DJOXTVWfjxwI05gbJWZHW1mK4AVkk5K1l+kfcYonH799dcsXLiQ2267jZ9//plLL70USRx44IElHj8j1A3TSDb5E3zJXLLJn4zwJZHyWHiVWDG0CV4tE6ceOhX3gbcf8A2wPy6h8/lImz2BGv76ELwKHMUobwJ/Bob56/2Bhf76HmCIv+4JzPbX/YBR/noc0DvS13r/MxcXeOwP7A4sjYxxBXCnv34C6OavGwOfFGHnf4Aj/XVNXKAb/x6Mwsm0g9t2O8Bf3wHMxW3XbQAsL2KcKjil1APifG0AfAs09fd149oNxQVCKf2OK1rh9KabbrLrrruuoP55551nEyZM2Kmxskmp0Sy7/Am+ZC7Z5E9QOM1OugFPmlPUXA68DhyeoF414AG/DfRp/DbQFJhAoSbGWRRqZnTDH0tvZq8B9STVKoHd75vZMnPnp3wBvOrL5+ECIoBjgFF+9mYKUMtrdyRiBnC7pIFAHTNLZdpgSmTMmWa2zsxWApu99kciLgFeNLMlceVHAG+Y2VcAZvZ9CuOXGyVVOD3ttNN466232LJlCz/99BMzZ86kdevWFWV+IBAIlIqw7FJx/BlYDnTEfXvflEojM1sqabWkDrgzVi4uwZgFiqKSqgDVI8+ih7Zti9xvo/DvpApwhJkVa6uZjZD0Ak5FdYak49he0RTiVE3jxoy3J9nfalfgKJ/wWxOoLmk9LvjJWEqqcNq6dWuOP/54OnToQJUqVbjgggto165dBXsRCAQCO0cIPtJDVNHzTeAiSQ8DdXG5DoNwR8LvHWlTG1hiZtsk9cUt06TKeJyuRW0rzOt4EzgHuMHncKwysx/jTnpdjMu7mACcipt9KQmvApcDIwEk5ZjT/9gBSQeb2TxgnqTDgVZ4oS9Ju+NySY4G3iqhDdthZudExuyHk3G/xuen3CupqZl9JaluJs1+7IzC6aBBgxg0aFBZmxYIBAJlTgg+0oCZrZY0w2+RfQmXrzAHl2h5tZn9T9JqYKtXCB2H2xnzjKQ+uETJDSUYciIumfOGSNlQYKykubiE074J2j0ATPY2lHRMcLtERvsxdsPtLkk283KlpB64WYv5wEtmtlnSBJza6VfAjp++acLMVvqE0Wf9LM8K3GF6v8AJqdUCtkm6EmhjZj+WlS2BQCAQiCNRIkh4hVd4bf9KZ8Lpxo0b7fDDD7cOHTpYmzZtChJJt23bZn//+9/tkEMOsVatWtldd921Xbv33nvPqlatak8//XSpxs+mxDmz7PIn+JK5ZJM/mZBwmtLMh6SDcUsEm/2UfgfgETNbU2ZRUSCQpZRU3RScJsjgwYPp1atXBVoeCAQC6SHV3S7P4JYMmgP/Bg6kGLEsn/RX7ki6UtKeae7zZUlrJD2fzn6TjDVOUm9Jx/nE0k8iap6TdqK/MlVMldQ/geroJEm/itS52C8vFfjnr8dIih329vdixm2fYJyZ/lnC34+kpl4VdZGk8ZKq+/Ld/f0i/7xJmt6elCipuinAPffcw5lnnhmUTQOBQFaQavCxzdxWyTOAe8xsEE4TokKQOx49GVfiNDRK0l9xM0AjgXNL0qfvtyRJpNthZq+YWT0za21mOf51xs72V1aY2UMR+3LMLAeX7/KrSJ37zeyRBG0vMLMF/rbI4MPM5sWPY4WKp8l+PzcDd5hZc+AHIHZ67fnAD778Dl+vXNm6dSs5OTnsu+++HHvssdupm3bu3JkTTjiBzz//HIClS5cyadIkBgwYUN5mBgKBQJmQasJpvqTf45IYT/FlKe2UkNtucQtxip8+CXAUThDrWyAfGGtefTNBP4txuzyOBW6R9D1OlXN3nC5Ff+A8oCEwXdIqM+sRU/H0ffQGTjazfl7tcxNwKG4raF3gR5xC5i9wiaITAcxsml9uSsXfYu00s/WSrsO9l3sAbwMX+fWxaF95OPXShsD1vngPoLqZNfXKnrfjtpiuwgl2LfPlY339VykClU4x9RTcSbTVgdW43TZ74JJQt0r6I253zNE4QbNbk/jXG9jD64fM9+/T92Z2p6/3L2CFmd1FAhL9fvzfXU+cpD04VdmhwH24k26H+vKJOO0Sxb//UdItr161alVmz57NmjVrOOOMM/j444/ZvHkzNWrUYNasWTz77LOcd955vPnmm1x55ZXcfPPNBTMigUAgUNlJNfjoj/tA+Ze5bYtN8YJWKfAbIAenZ1EfeF/SG8CROPGqNsC+wCcUfmAmY7WZHSapPvAscIyZbZA0GPiLmV0v6S9ADzNblYJtBwC/MrOtPhjZHyfW1QoneJUwEEqBIu3EBRKjzOx6AEmPAifjVEF3wMymeHvwu0Vel1QNp2p6mrmdHWcD/8IFYA/h5NrfkDSyGFvH48TKhkjaH9jfzGZJugf4yMxOl9QTeAT3e4zyFk73wyRdgAvY/irpfiLBhqSjizLA3NbYy/ysCX4Z5FngTh+k/g7oUowf8dQD1lihuNkS3HZn/M9v/dhbJK319bf7m1E5yKsDNGnShNGjR1O3bl0aNmxIXl4e++yzDx999BF5eXm89dZbvPnmmwCsXbuWyZMn8+mnn9KtW7edGj8jpJXTSDb5E3zJXLLJn0zwJaXgw8wW+A/Oxv7+K1Kfqi5Q/ASWS4opfnYDnjazbcD/JE1Poa/x/ucRuKBlhvuCS3XgnRTtifK0tyvGc96eBZL224n+SmJnD0lX45aI6uK+8ScMPmL4+hvNbLSkdkA7YKrvuyqwTE4JtI6ZxQ5ZexQ365SMCbjZkSHsqJh6JjjFVEmJFFMPAMb7oKU6bvtsqTGzxT7f5VCcRP1HZrY6HX2X0I5/43KcaNmypV1+zmlp6XflypVUq1aNOnXqsHHjRv75z38yePBgateuzcaNG8nNzSUvL4/WrVuTm5vLsmXLCtr269ePk08+md69excxQtHk5eWRm5ubBk8yg2zyJ/iSuWSTP5ngS6q7XU4BbsV9wDSVlANcb2anlqFtiYjpUgiYama/T6FNdCo9XlEzXuciqqopdp4i7ZRUA6fz0dnMvpU0NIFtxLU5BvgtTrQs1vd8M+saV69OSQy10imm3gPcbmZT/LLH0JKMXQxjcGe1/ILiZ8QSsRqoI2k3P/txAO7MGvzPA4ElPt+ntq9fLpRU3TQQCASyjVSXXYbipr3zAMxstqRmKbZNpvi5O9DXlzfAHTqW6nHz7+LErpqb2SJJewGNzOwzCtVGY1PoyyW1BhbiEmbXpThGOkhoJ07wCmCV3NkovSliiUfSQcBo4Dgz2+iLFwINJHU1s3f8MkwLM5vvd350M7O3cHkYxbGziqm1KfxAj4qarcOJeJWEfEnVzCzf30/CLU9VozBvI2X8UtB03Hv7lLdvsn88xd+/45+/VlS+R7rZGXXTGOPGjSsjqwKBQKD8SDWDLd/M1saVbUux7SQKFT9fwyt+4rbvLgEWAI8BHwLxYyTE3GFj/YAn5dQ238HlaYCbJn85soxzDfA8LqlzGTuBpDdxh78dLWmJ3DklO22n10d5AKf0+QrwfjFd9cPlJDwnt8X0RTP7GffBebOcYulsCneY9McFPbNJbQZnIi6vYkKkbCjQyds9gsSKqUOBpyV9wPb5Ev8BzvC2HpXC+OB+b3MlPQ7g/ZsOTIhbGtuBIn4/g4G/SFqEe/8e9OUP4g7eW4TLwbkmRRsDgUAgkAaUyhc+SQ8C03D/SZ+Jk9muZmYlmaJP1G9Nv/OjHvAe7gj2/5Wmz0B24BNNPwR+a2afV7Q9LVu2tIULF1a0GWkhE9Z700k2+RN8yVyyyZ/y9EXSB2bWOb481ZmPy4G2uJyIJ3AzFFemwa7n/bfzN4EbQuARAJATHlsETMuEwCPdbNq0iS5dutCxY0fatm3LkCFDAJdM2rRpU3JycsjJyWH27NkA/PDDD5xxxhl06NCBLl268PHHadOMCwQCgQqh2JwPOaGsF8ysB/CPdA5uZrkJxpsENI0rHmxmr6Rz7NJSWeyM4Zci4ncofZWhwmULcPoiBUhqz47buzdHhMYqDcnk1QFGjhy5w06WG2+8kZycHCZNmsSnn37KpZdeyrRp0yrC9EAgEEgLxQYfXgNjm6TaCfI+0k55fhh6PYnnzaxdSduWxk5JDYG7zSzl/ZIxQS4zm5Vi/Vxf/2Rwiqm4/JJKiZnNY0edEQAkHY875bcqMMbMRiTrxy/xTcRt9x5nZpel39qiURJ59WQsWLCAa65xaSmtWrVi8eLFLF++nP32K81u8EAgEKg4Ut3tsh6YJ2kqke2pZjawTKzKcszsO1yyaKCU+Jm50ThF2SU4EbspVijbHs8m4J84jZSUg850KZwuHnES4OTVO3XqxKJFi7j00kv55S9/yX333cc//vEPrr/+eo4++mhGjBjB7rvvTseOHXn22Wc56qijeO+99/j6669ZsmRJCD4CgUClJdWE00Q7HTCzh9NuUSmRNAL41sxG+/uhuIBpX3aUeG+Cn/mQ1A+nu3GZb/c8cKuZ5ckdkncfcCJux8zfcZLxjYErvc5FVdyukFzcNuLRZvZ/SWyMH/d0YC/gEAr1VM7F5dicaGbf+5mPOcCvcUHjeWb2nqQuuG/9NYCNOPn2hdGZjyLq9ANOxQmdHQxMMrOrvY3HAzfiZhNWmdnRfqvwPbgP7WrAUDOLbV+N97EtTmm1Oi636EychH7BTJOkq4CaZjbU+/cRcJR/L/oAfwPaA+PN7Nok43T1dhzn7/8GYGY3STrc+72Xfy+PNrN1vl4/Ir/vJH1HFU47XXfnA8mqpkz7RrW3u1+/fj3//Oc/GThwILVq1aJu3brk5+dz22230bBhQ/r27cuGDRsYNWoUn3/+Oc2aNeObb77hqquuonnz5jtlw/r16wtmXrKBbPIn+JK5ZJM/5elLjx49EiacYmZZ9cKd1fJ65H4BbpvoVNwH6X7ANzgp9SbAx75eP5zkeazd80CuvzbgBH89CacIWg0nGT/bl18IXOuvdwdmAU2T2Bg/7iKcNkkDXDLvxf7ZHbjgBpzGygP+unukfS1gN399DPCMv87FfdAXVacf7gyX2rjA5Guc+FYDnPx4U1+vrv95I/BHf10H+AzYK4mP9wDn+OvquDNfCvz25VfhAoeYfzf76yuA7/zvaHfcjEa9JOP0xi21xO7PxZ0ZVN37dnj8e5Do913cq0WLFlZWDBs2zEaOHLld2fTp0+2kk07aoe62bdvsoIMOsrVr1+70eNOnT9/ptplINvkTfMlcssmf8vQFmGUJ/k9NVeH0K7ZXCgXAzFIVGis3zOwjSfv6vIoGuNNMc0gs8T43eU/b8TPwsr+eh0t0zJc0D/eBCtAL6CB/XDzuA/0QUpMcn27uG/k6f85ITGZ9HtAhUu9J7+Mbkmp5NdO9gYclHYL7HSU68K92EXWmmc/lkbQAOAjYB3jDnIw+ZvZ9xMdT/YwFuIClMe5cnnjeAf4h6QDgWTP7vKi8Bs+UiN/zzWyZt+tLXFBUEhXSlsAyM3vf+/BjCdqWKfHy6lOnTmXw4MEsW7aM/fffHzPjueeeo107tyq0Zs0a9txzT6pXr86YMWPo3r07tWqVVMMtEAgEModUcz6iUyY1cDLfddNvTtp4GveN+Bc49c74XSmJ2ML2W4+jcuf5PoIDJ662GcDMtnl5bnBiXpfbzu12icq6b4vcb2P731F8AGjADbjg5Qy/nJOXoP+i6kTH3krRfxMCzjSzYgUvzOwJSTOBk4AXJV2EmylJ9h5HbYm+B7H7ZHbFpNJjRGXUM5Jk8uo9e/Zk5cqVmBk5OTncf//9AHzyySf07dsXSbRt25YHH3ywmBECgUAgs0n1YLn4b5x3elXL69JvUloYj1MQrY/LkehKYon36IffYuASL27ViJKfovoKMEDSa35WpAWw1Mziz48pDWcD0yV1A9aa2VpJUYnzfknapVInyrvAvZKamjvFuK6f/XgFuFzS5WZmkg41sx11wgEvv/+lmd0tqTFuBudNYF+/42Q97iTflxO1LwHvA4fInbS8FKfU+gfgc2B/SYeb2fuS9sYdypeeo2lLQTJ59ddeey1h/a5du/LZZ5+VtVmBQCBQbqS67HJY5LYKbiYk1VmTcsfc+SZ74z78l3lNjq64hE3DS7z7WYAYM3BLJAtwywgflnDYMbglmA/l1hdW4hJJ08kmSR/hlk3O82W34JZUrgWSbcdIpU4BZrbSJ1s+64OxFbjdJDcAd+Jk0Kvg3q+Tk3RzFnCupHzgf8CNPii7HqdmuxT4tDhbUrB1i6TLcIFRVWCsmc0HkHQ2cI+kPXCJtscA6yUtxuWAVJd0OtDLku+OCQQCgUCaSXW3S/S4+y24D53bUpl+DwSygXTJq2/atInu3buzefNmtmzZQu/evRk2bBjnn38+s2bNiiW3Mm7cOGrWrMnXX3/Neeedx8qVK6lbty6PPfYYBxxwQKlsyCaZaMguf4IvmUs2+VOZ5NXPN7Me/nWsmV2IS8IMBAIlIKZuOmfOHGbPns3LL7/Mu+++yx133MGcOXOYO3cujRs3ZtSoUQBcddVV9OnTh7lz53Ldddfxt7/9rYI9CAQCgdKTavCR6Lj3pEfAA3htjHJH0pWS9kxzny/LHVP/fAnbtZc72TX6mllMm3GxHTOSxvhzTkpjexNJaTsMRFI/SaMi98cl8PFNSb+K1LlYUh9/ndA/SX8vZtx6CcaZLenXkt6RNF/SXL/UEmvTVNJMSYskjZdU3Zfv7u8X+edN0vX+FIeSqJvGdq+YGRs3bixQPF2wYAE9e/YEoEePHkyenFBWJRAIBCoVReZtSGqFO1CutqTfRB7VYsedCuWGpKqW/Jj1K4HHgJ9K0N9uxSQijsQJcV2UspGOBWaWU8I2BZjZBTvbtrywBLLtcsJuvwLe9nXuT9I26t/fcToiycZZTQJ5dZ/Y28dv5W0IfCDpFTNbgzvL5g4ze0rS/cD5OLG484EfzKy5pN/5emfH9x0lHQqnRambAvTv358XX3yRNm3acNtttwEUqJteccUVTJo0iXXr1rF69Wrq1atXKlsCgUCgIiky50PSabikyVMp1GAAWAc8ZWZvF9F2vZnV9MmXt7CjumgVnBhUT5ygVT4uWTDhjIpPEhyPS3y8BfgeGIYTofoC6I9LwrwVWIhT5ewRs8P30Rs42cz6SRqHk9o+FJdsWhf4EZdM+wtcUurEyPi5RM5KKcLvYu00s/WSrgNOwYlvvQ1c5HeQjMOJg02UP88FaAhc74fYA6huZk0ldQJuB2oCq4B+PsG2EzDW138VJ5CWUEpc0ru4ZbVYkmZszC99H81wgdyFZjZXEWVQSacA1+IEvVYD53j73sVt212JOxH5aGC9md2axL/euN1H84D5/n363szu9Db9C1hhZncV9d77unN8f4v8+L/wSakFSqiSXvHX78htlf4f0MDi/jEozQqnRambNm3qdoNv3bqVu+++m1atWnHCCSewatUq7r77bpYtW0aHDh144403eOihh0qlTphNSo2QXf4EXzKXbPKn0iicAl1TqRfXZr3/eSaJ1UV7Ay/iln5+gRMD611Ef4txAQG4LbRv4NU1gcHAdZF69ePtsEI1zHH+ehxOxbRq5P5pb08bYFHc+Ll4xdBi/E7VzrqRNo8Cp0Ts6G2Fqp+d4/qfAFyK2/HyNu5DE9w397H+ei7Q3V+PJKIqmsDePwPD/PX+wEIrVCgd4q97Uqjk2g+vDIoTI4sFsBfgkpABhuICNeLvk/kX93tqAnzor6vggpGECqdxvnTB7VSq4t/7RZFnB1KoCvsxcEDk2RfRv5lEr7JSOE2kbvr6668nVDddt26dNWrUqNRjZpNSo1l2+RN8yVyyyZ9Ko3AKfCTpUtwSTMFyi5mdl7xJAd1IrC7aDXjazLYB/4vbUZOM8f7nEbgAYYZfG6+OU9QsKU/b9ss3z3l7FkgqzaldqdjZQ9LVuOWcurhv/P+hCHz9jWY2WlLsYLSpvu+qwDI51dM6ZvaGb/YobtYpGRNwsyNDcNtjY7M93XCBI2b2ms+5iJfVPAAYL2l/71sqaq7FYmaLJa2WdCguYP3IdtSa2Q5vw6NAX3Pib+kwJe0kUje9+uqrWbRoEc2bN8fMmDJlCq1atQJg1apV1K1blypVqnDTTTdx3nmp/JMLBAKBzCbV4ONRnCbDcbjp/3NILKld1sQEuwRMNbPfp9AmOpUen6cSLwAWVdUszadXkXZKqgHci/vW/63PkSgyh0bSMThl2e6RvuebWde4enVKYqiZLfUf9B1wsycXl6D5PcDt5g7Wy8XNcKSLMbhZll9QuISUEB8UvQD8w8ze9cWrgTqRfJ6o8mlMFXWJX3apTcmk23eaROqmJ510EkcddRQ//vgjZkbHjh257777ALcl7m9/+xuS6N69O6NHjy4PMwOBQKBMSTX4aG5mv5V0mpk9LOkJnFplKrxJYnXR3YG+vrwBblnjiRT7fBcYLam5mS2SO221kZl9hstH2RuXAwFutqU1Lg/kDP+8vEhoJ060C2CVpJq45aCku4ckHYQ7Nv44M9voixcCDSR1NZe7UA1oYU5gbY2kbmb2Fi5QLI7xwNVAbTOLnXfzpm97gw8sVpnZj3EzClHl1OjJx+twScklIV9SNTPL9/eTcIFuNZxiaUL8DpZJwCMWydExM/Ozab2Bp7x9sa0iU/z9O/75a356sMxJpm46Y8aMhPV79+5N7969Ez4LBAKBykqqW21jHwhr/HR/bdwR9akwCZeDMAd4Da8uCjyDO610AW53yoe4E12LxcxW4r4VPylpLu5DpJV//G/g5cgyzjW43I63gWUp2rwdkt7E5YMcLWmJpONKY6e5nRgP4HIPXsFJhBdFP6Ae8JzfXvqimf2M++C82SdZzsbtMAGXfDta0mxSm8GZiJMlnxApGwp08naPYPvgIlrnaTmp/VWR8v8AZ3hbj0phfHC/t7mSHgfw/k0HJljynU3gloq6A/1UuP02xz8bDPxF0iLc+xc7FOVBoJ4v/wvubyQQCAQC5UWiRJD4Fy6ZcB/cOSlf4r65X5xK22L6rel/1sMl/f2itH2GV3a8cIHxbOCQirbFLH0Jpxs3brTDDz/cOnToYG3atLHrrrvOzMzOO+8869Chg7Vv397OPPNMW7duXUGb8ePHW+vWra1Nmzb2+9//vtQ2ZFPinFl2+RN8yVyyyZ9Kk3BqZmP85eu4rZfp4nmfo1AduMHcjEhgF0dOeOx5YJKZfV7R9qSTmMJpzZo1yc/Pp1u3bpxwwgnccccdBUJjf/nLXxg1ahTXXHMNn3/+OTfddBMzZsxgn332YcWKFcWMEAgEAplPqgfL7YcTgGpoZif4D4euZlaqs73NLDfBWJOApnHFg23njqovMyqLnTH8UtHNccVfmdkZ/vkYXPJoqQ9Y81ogr5rZdzvZRSPc1utefklnEE6z49G4epvN7JeRcacAzSyJpkmk3su4nUhvWTG6LelGJVQ4feCBB7j00kvZZ599ANh331RXOwOBQCBzSTXhdBzwEPAPf/8ZLkmxVMFHImIfhplOZbEzhiVQIo17nk411X64fJadDT5W4XRPvvM5Rq+YWSMSKJzGkFPgTVXSf2cVa9NCSRROP/vsMwCOPPJItm7dytChQzn++OMrwuxAIBBIG6kGH/XNbIKkv0HBMeZFJQEGMhi/62YCbvtpVeAGYAA7oaaaoO/eOJXYxyVtBLriZi4Sqbnm4cTHZkmqj1sbbGJm0e0g84E9JO1uZptJgN8x9BecGumESHlz4H7cbqqtwG/N7Aszm+Z38KRMOuXVq1atyuzZs1mzZg1nnHEGH3/8Me3ateOhhx5i69atXH755YwfP57+/fuzZcsWPv/8c/Ly8liyZAndu3dn3rx51KlTp1S2BAKBQEWSavCxQVI9vGaGpCNIcWdKICM5HvjOzE4CkFQbF3xgZlPwUvqSJgCv+2289wCnmdlKucPb/oWTs98Oc7Lpl+GDCt/PKDO73l8/CpxMMYJqEc7EqZ0mDDw8NwC3seN5Po8DI8xsktdWSXV3F97WqLw617Uv6vif4snLy9uhrEmTJowePZqzzy48WqZly5b8+9//pmnTplSpUoUWLVoUbMVt0KABTz31VIEI2c6wfv36hLZUVrLJn+BL5pJN/mSEL4myUONfwGG480/W+p+fAR1SaRtemfcCWuBk4G8GjvJleUSk3HG6Hw/763a4c29m+9c8XE5Hsv7j+zoTmOnbLQWuia+Hk0NfHNdPW9wuqIOLGCsHmOKvm1Aoob43sKSIdrmkIJcfe6Vrt8uKFSvshx9+MDOzn376ybp162ZTpkyxzz//3MzMtm3bZn/961/tr3/9q5mZvfTSS9anTx8zM1u5cqUdcMABtmrVqlLZkE1Z+2bZ5U/wJXPJJn8yfreLpMZm9o2ZfSjp10BLnG7EQisUgwpUMszsM0mHAScCwyVNiz5PVU01FYpRc91C4WxEjbh2B+A0YvqY2RdFDNEV6Cx3oN9uwL5+OeeUktpaHpRU4fS4447j1VdfpU2bNlStWpWRI0eGE20DgUClp7hll+dwsx4A483szLI1J1AeyB09/72ZPSZpDU7HJfasRGqqSYaIqcxCYVCRSM11MdAJeM+Xx2yog5NLv8bMEkt/eszsPuA+364JbjYj198vkXS6mT0naXfcIYLxSzPlSkkVTiVx++23c/vtt5e1aYFAIFBuFLcGHlXHTKe+R6BiaQ+85xVQhwDDI8/6UTI11USMA+73/W8muZrrrcAASR/hll1iXAY0B66LqJbuzB7Tc4GBXqX1bdw5MTutWBsIBAKB9FDczIcluQ5UYizxtttc/3MWMCxBm9kULsMU1/8zOPn8GNf6V3y9T4EOcfUws+FsHxClhJktxuWnxO4/B3omqJeq5HsgEAgEyoDiZj46SvpR0jqgg7/+UdI6ST+Wh4GBQDaxadMmunTpQseOHWnbti1DhgwB4Pzzz6djx4506NCB3r17s369kyy5/fbbadOmDR06dODoo4/m66+/rkjzA4FAIC0UGXyYWVUzq2Vme5vZbv46dl/SU0sDWYak0ZFlkdirfxmONzPBeO3LaryyICavPmfOHGbPns3LL7/Mu+++yx133MGcOXOYO3cujRs3ZtSoUQAceuihzJo1i7lz59K7d2+uvvrqCvYgEAgESk+JdA+yDUlNJH1cAeM2lDSx+JrbtcmT1LkE9XMlPV9y61LHzC41s5y410NlON4vE4w3T1IdSRMlfSrpE0lF7sqR9LKkNWX9/iQZu0Ty6j169GDPPfcE4IgjjmDJkiXlbXIgEAiknVRFxgJpxNyZJ72LrRhIlbuAl82st6TqOOn0oiixvHo6FU5LIq8e5cEHH+SEE04olQ2BQCCQCchpgGQPkkYA35rZaH8/FNgA7AucgEucHW5m4yNbM9v5w9A6m9llvt3zwK1mlidpPW4754nAMuDvwC1AY+BKM5siqSowApe4uTsw2sz+L4mN8eOeDuwFHILbAVIdt1NjM3CimX3vtSvmAL/GBY3nmdl7krrgPnxrABuB/ma20MuHX2VmJxdRpx9wKu6D+GDcKbJXexuPxx0mWBVYZWZHe1n2e3BJndWAoWY2OYmPbXHnAVXHzbCdCeTH/PZ1rgJqmtlQ799HwFH+vegD/A23M2e8me2QsOr7qI3bfdPM4v6Yk8mr+2cF70+ifn2dqMJpp+vufCBZ1ZRo36j2dvfr16/nn//8JwMHDqRpU3dG4datW7n77rtp1arVdoHG1KlTmTRpEnfeeSfVq1cvlR3r168vmH3JBrLJn+BL5pJN/pSnLz169PjAzHactU+kPFaZX8ChwOuR+wVAX2Aq7oN0P+AbYH+2V8TsB4yKtHseyPXXBpzgrycBr+I+fDsCs335hcC1/np33K6RpklsjB93EU4XowFORfZi/+wOXHADTg30AX/dPdK+FrCbvz4GeMbiFDyLqNMP+BKojQtMvgYO9HZ8G7MfqOt/3gj80V/XwSnd7pXEx3uAc/x1ddy5LgV++/KrcAFMzL+b/fUVuEPp9vfv5RKgXpJxcnA6IeNwwcuYmE04VdUz/HUNYM9Iu4L3J5VXuhRO4xk2bJiNHDlyu7LXX3/dTjrppIL7qVOnWqtWrWz58uVpGTOblBrNssuf4Evmkk3+ZILCadblfJg7lGxfn1fREXc0ew7wpJltNbPlwOvA4SXo9mfgZX89Dxfc5PvrJr68F9DHa1vMxGllHJJi/9PNbJ2ZrcQFH7FzT6L9AzzpfXwDqOXFuGoDT/vclTtwkuTxFFVnmpmtNbNNuEDtINxx82+Y2Vd+vO8jPl7jfczDfaA3TuLTO8DfJQ0GDrJCwbKimBLxe76ZLTN3psuXuKAoEbvhhPDuM7NDcbNc10jaG2hkZpO8D5usggXGAFauXMmaNWsA2LhxI1OnTqVly5YsWrQIcF8GpkyZUnB2y0cffcRFF13ElClT2HffnZE6CQQCgcwjW3M+nsblVPwCGA80TaFNVOobtpf7zvcRHMA23HIIZrZNUuw9FHC5OQ2NkhI9NG1b5H4b2/+O4tfIDHeo2nQzO8Mv5+Ql6L+oOtGxt1L034SAM81sYRF1nGFmT0iaCZwEvCjpItxMSbL3OGpL9D2I3SezawnuDJeZ/n4icE1x9lUUJZVXHzRoEOvXr+e3v/0tAI0bN2bKlClFDREIBAIZT7YGH+Nxqpr1cTkSXYGLJD0M1MUtWwxi+w+/xcAlkqoAjYAuJRzzFZxa52tmli+pBbDUzDaUypPtORuYLqkbsNbM1vqch6X+eb8k7VKpE+Vd4F5JTc3sK0l1/ezHK8Dlki43M5N0qJ9p2gFJzYAvzexuSY1xYmJv4mal6gHrcafbvpyofaqY2f8kfSuppQ+KjgYWmNm6bJBX/+9//1vWJgUCgUC5k3XLLgDmzhzZG/fhvwyXpzEXl7D5GnC1mf0vrtkM4Cvc0sPdwIclHHaMb/uhX974P9If3G3yUuT3A+f7sluAm3x5svFSqVOAX/65EHjWy6mP949uwOW6zJU0398n4yzgY79E0w54xC9VXY/L0ZgKfFqcLSlyOfC4l1HPweWmQJBXDwQCgYwk63a7BAJlQcuWLW3hwmJXm4pl06ZNdO/enc2bN7NlyxZ69+7NsGHDOP/885k1a1YsuZVx48ZRs2ZNNm/eTJ8+ffjggw+oV68e48ePp0mTJqWyIS8vj9zc3FL7kilkkz/Bl8wlm/wpT18kJdztkpUzH4FAplJShdMHH3yQffbZh0WLFvHnP/+ZwYMHV7AHgUAgUHpC8FGGSGqfQA58ZvEtyx9JYyS12Yl2xyXw8QNJDUthy7G+j3n+Z09J9RKMM9uXd/J1F0m6WzF50OT9VxqF08mTJ9O3b18AevfuzbRp0wizlYFAoLKTrQmnGYGZzcPlIGQ8ZnbBTrbb4YRcLxjWEKfVsTOsAk4xs+8ktQNeMbNGJHkvJb0E/Am3xflF4HjgpSL6rzQKp0uXLuXAA90u4912243atWuzevVq6tevXypbAoFAoCIJOR+7IF6pdAJwAE547QZgAE70qyEuKRScMFh1M2sqqRNwO1ATFxz088m88X33xgl+LcWpqXbF7Sw6xff3NnCR3y2Th1MZnSWpPk6MpklcfwJWA/t7zY/48fbHbSNu5e9/jxOHuygbFE779+/PLbfcQoMGDQA455xzuPfee6ldu/YO/adKNik1Qnb5E3zJXLLJn6BwGl4V8sJJnT8Qua+N0/7oHFdvAnApbofL20ADX342MLaI/rfrC6+Q6q8fxc1qbFcPty16cYK+egP/LWKsztHnOHn2mLJrpVc47dWrl7399ttmZpafn2/16tWzbdu2lWrMbFJqNMsuf4IvmUs2+RMUTgMVxTzgWEk3SzrKzNbGV5B0NbDR3Bk5LXHbZaf6rbPX4mZNUqWHpJmS5gE9SazCugP+fJibKcHySKRtViicnnrqqTz88MMATJw4kZ49exbkgwQCgUBlJeR87IKY2WeSDsMdlDdc0rToc0nHAL/FibGBUzadb2ZFHlWfCEk1gHtxMxzfyh30FxN3i6rK1ohrdwBOn6WP+aWSJCxl+0DoAAoF1TKOkiqcnn/++Zx77rk0b96cunXr8tRTT1WwB4FAIFB6QvCxC+J3onxvZo9JWgNcEHl2EDAaOM4Kz2NZCDSQ1NXM3pFUDWhhTswtEetwIm9QGFSsklQTt4wy0ZctBjrhRMd6R2yoA7wAXGNmiaU/PWa2TNKPko7ALbP0Ae6xLFE4rVGjBk8//XRZmxUIBALlSlh22TVpD7znl1CGAMMjz/rhDsV7zm9lfdHMfsYFBzd7xdPZwK+K6H8ccL/vfzNO6v5j3K6Y9yP1bsVJ0n+Ey/mIcRnQHLgusqW2qFPVLsEpzC4CvqBwp0tQOA0EAoEMJMx87IJYgu2xuARMgFnAsARtZlO4DFNc/88Az0SKrvWv+Hqf4s58idbDzIazfUBU3HizcDkp8eWf43JM4suPSrXvQCAQCKSfMPMRCJQTmzZtokuXLnTs2JG2bdsyZMgQwG2fbdmyJe3ateO8884jPz+/oE1eXh45OTm0bduWX//61xVleiAQCKSVEHxUIJLqSLqklH30kzQqTfY0lDSx+JoF9UcnUBztnw5bkow3M8F47SU9KGmOpLmSJvrckqL6GStphdwBgOVGMmn1c845h08//ZR58+axceNGxowZA8CaNWu45JJLmDJlCvPnzw+5H4FAIGsIyy4VSx1cvsK90UJJu5nZlvI2xsy+I5L4mUL9S8vQnETj/TJRuaQ/m9mP/vp2XM7IiCK6GgeMAh5Jt41FkUxa/cQTTyyo06VLF5YsWQLAE088wW9+8xsaN24MwL77FpX2EggEApWHEHxULCOAg31iZj6wCfgBaAW0kPQccCBux8hdZvZvAD+78DdgDTAHl9SJpAY4Rc/Gvv8rk+0WkfRr4C5/a7h8jno44a12ksbgBLwAGgGjzGyYpEHAWcDuwCQzG5Kk/x1UVM1svKTFuG23qyR1Bm41s1y/Bbcp0Mzb/2fgCOAE3NbZU8wsf8eRIBJ4CKeiav5+P/9+NPNVB5jZ22b2hqQmifpKRrrk1ZNJq4MLSB599FHuusv9Wj777DPy8/PJzc1l3bp1XHHFFfTp06dUNgQCgUAmEIKPiuUaoJ2Z5Xi57xf8/Vf++Xlm9r2kPYD3JT0DVMclhHYC1gLTgdjezbuAO8zsLUmNcUmlrZOMfRVwqZnN8MsUm6IPzZ/14rfevgyMk9QLOATogtP+mCKpu5m9kaD/44HvzOwk308qeuAHAz2ANsA7wJlmdrWkScBJwHPJGkp6CKdbsgD4qy++G3jdzM6QVBUnDZ8ycfLqXNe+dJNReXl5ANx5550F0uqtWrUqkFa/9dZbadasGVu3biUvL4+vv/6ahQsXctttt/Hzzz9z6aWXIqngrJedZf369QW2ZAPZ5E/wJXPJJn8ywZcQfGQW70UCD3DbRM/w1wfiPvh/AeSZ2UoASeOBFr7OMUCbiAJmLUk1zWx9grFmALdLehx41syWxCtneoGwp4HLzexrSZcDvSgMdmp6mxIFH/OA2yTdjJtNeTMF/18ys3yvhFoVF/TE+mpSVEMz6+8DjHtw8u8P4Xa69PHPt+KCtZTxM03/BmjZsqVdfs5pJWleLB9++CGrV6+mf//+DBs2jN12240JEyZQpYpLxXr33Xfp0KEDJ5xwAgBTpkyhRo0a5ObmlmrcvLy8UveRSWSTP8GXzCWb/MkEX0LCaWaxIXbhZ0KOAbqaWUfcB36NxM0KqAIcYWY5/tUoSeCBmY3AiYvtAcyQ1CpBtftxgcl/Y2YBN0X6b25mDybp/zPgMFzgMFzSdf5RUlVT/PKRmW0D8v25AADbSCFQ9gHGU7izazKORNLqrVq1YsyYMbzyyis8+eSTBYEHwGmnncZbb73Fli1b+Omnn5g5cyatWyebyAoEAoHKQwg+KpaoEmg8tYEfzOwnHxgc4ctnAr+WVM8rjf420uZV4PLYjaScZANLOtjM5pnZzTjhr1Zxzy8F9vZBSoxXgPNiu0kkNUom/uVVVH8ys8dwR9gf5h8txi0ZQRqCBDmax66BU4FP/eNpuNN6kVQ1xaWfMmPZsmX06NGDDh06cPjhh3Psscdy8sknc/HFF7N8+XK6du1KTk4O11/vDhVu3bo1xx9/PB06dKBLly5ccMEFtGu3g5xJIBAIVDrCsksFYmarJc3wWz43Assjj18GLpb0CU7e/F3fZplPznwHl3A6O9JmIDDaK3ruhlsOuTjJ8FdK6oGbVZiPUwXdP/L8KiDfJ8MC3G9m90tqDbzjl2jWA38EViTovz0wUtI2XDLtAF8+DHhQ0g24U21Li4CHJdXy13MiY10B/FvS+cBWX/6OpCdxomr1JS0BhiSbwUknyaTVt2xJnksyaNAgBg0aVJZmBQKBQLkTgo8Kxsz+kKR8M26nR6JnD+FyGuLLV+HyHVIZ9/IExYvxSqFm1jRJu7so3CVTVP+JVFTxuR8tEpQPjbuvmexZXL1twJFJni0HdkjUMLPfJ7c8EAgEAmVNWHYJBMqJkiqcTp48mQ4dOpCTk0Pnzp156623KtL8QCAQSBsh+MhyJPVPoAo6Oo3910vQ/2xJ9dI1RmSsSQnGqTSHwpVU4fToo48uqDt27FguuOCCYkYIBAKBykFYdqlA5I6O/4OZ3Vtc3SL66IcT7bos0fNkSzRJ+moI3G1mJVE5XQ3kpFq/NJjZGYnKJTXF7XKpB3wAnOtP4k2IpLHAycAKMyu3DM6SKpzG6gJs2LCB+K3QgUAgUFkJwUfFUodKLK+eQdyME1d7StL9wPnAfUXUH0cJ5dUrQuEUYNKkSfztb39jxYoVvPBC6cYPBAKBTEGFUgqB8kbSU7iEyIXEyaubWYnk1c3ssl1RXt1vr10J/MLMtkjqCgw1s+OSyav7dk1iviay39eJKpx2uu7OB5JVTYn2jQp3+sYUTgcOHLidwmmNGjW47LIdJ7HmzJnDI488wm233VYqG2JjR2dVKjvZ5E/wJXPJJn/K05cePXp8YGadd3hgZuFVQS+caufH/joXJzLWNPK8rv+5B/AxLjjYH/gGaICTWp+BCwwAngC6+evGwCdFjP0f4Eh/XRM3C1ZgT6TeQcAn/mcvnOKncPlCzwPdk/R/JvBA5L62/7kYqO+vO+PUWgGGAm8B1YCOwE/ACf7ZJOD0JOPUBxZF7g+MvKfjcQEYuACodqL3PpVXixYtLN0MGzbMRo4caWZmQ4cOtdNOO822bt2atH7Tpk1t5cqVpR53+vTppe4jk8gmf4IvmUs2+VOevgCzLMH/qSHhNLNIJK8+B6fxEZNX/yVeXt1cXsP4SP1jgFFem2MKXl49yVgxefWBQB1LsMwTL6+OCz5i8uof4oTJDknS/zzgWEk3SzrKzFKRNn/J3OxGieXVk9ATv/xiZltTtKHMKKnC6aJFi2KBEh9++CGbN2+mXr205/EGAoFAuRNyPjKLZPLqP0nKI3V59U3F1MPMRkh6AXcY2wy/ayS+XTJ59f9Lof/PJB3m+x8uaZqZXU+K8uqSUpVXXw3UieTJHIBbpsk4li1bRt++fdm6dSvbtm3jrLPO4uSTT2a33XbjoIMOomvXrgD85je/4brrruOZZ57hkUceoVq1auyxxx6MHz8+JJ0GAoGsIAQfFcvOyqvf5bey/oiTV5/jn8Xk1UeCk1c3s9mJOo/JqwPzJB2Om8WYHXmeTF79BkmPm9l6SY1wZ7DsoHDqd858b2aPSVqDO0cGCuXVXyIN8upmZpKm4xJlnwL6ApP945i8+p3+0LmaFTn7UVKF08GDBzN48OCyNisQCATKnbDsUoGY26Yak1cfGff4ZWA3L68+goi8Oi4/4h3c0sknkTYDgc6S5kpaQHJpdXDy6h97KfZ8XDAQ5SqgfURP42IzexWXV/KOP3l2IsmDp/bAe34JaAgw3JcPwwVPs3CS5+lgMPAXSYtweTExqfQrgB7e1g+ANgBeXv0doKWkJV5+PRAIBALlRJj5qGAsyKvHyofG3ackr+6ffwl0SVCeUfLqmzZtonv37mzevJktW7bQu3dvhg0bxjnnnMOsWbOoVq0aXbp04f/+7/+oVq0an376Kf379+fDDz/kX//6F1dddVVFmB0IBAJpJ8x8BALlREkVTuvWrcvdd98dgo5AIJB1hOAjDUiqI+mSYuo0kZRwliNBvY/TaNsYSSuzWF79XEkf+uv5kopaakJSK0nvSNosqVw/1YtSOJWEpO0UTvfdd18OP/xwqlWrVp5mBgKBQJkTll3SQx0SKJXG0QT4Ay5nojx5C9hkSeTXS4tVsLy6pOrABDPb7LcVfyxpijm11kR8j8uNOb0kY5dW4XTxiJMASqxwGggEAtlICD7SwwjgYJ9cOdWXnYBTDh1uZuN9nda+zsM44axHgb18/cvMq28WhaR3gfPNbL6/z8Mlh34JjMWpef4EXGhmc+PajsOpek709+vNrKbf1jsMp5jaHqdMOg+XsLkHTuDrizQoqHYCrjKzk32dUTgBmnFe+fRJ/75twSmL3gQ0B0aa2f2JxrHtz3DZnchsnqTjgRtxmiGrzOxovzNnhaSTEvUX50NU4ZTr2u+84n1eXl7B9Z133lmgcNqqVavtFE6bNWvG1q1bt6u/ePFi9thjj+3KSsP69evT1lcmkE3+BF8yl2zyJyN8SaQ8Fl6lUio9ExeAVAX2w6mR7o9TMH0+0mZPoIa/PgSvAkcxyps42fFh/np/YKG/vgcY4q97ArP9dT8KFVDHAb0jfa33P3Nxgcf+uA/wpZExrgDu9NelVVCNfw9GAf389WKc/DnAHcBc3E6aBsDyYt7/A339n4BLfVkD4Fu8YixeLTbSZiguEErpd1yRCqdDhgwpqJcOskmp0Sy7/Am+ZC7Z5E9QOM1OugFPmlPUXA68DhyeoF414AG/DfRp/DbQFJhA4eFvZ+G2u8bGfRTAzF4D6kmqVQK73zezZeZ22XyB0wyB7dVF06qgmoApkTFnmtk6M1sJbJY7ATghZvatmXXAzZL09We6HAG8YV4x1sy+T2H8MqWkCqeBQCCQrYRll4rjz8By3DkmVdhRXTQhZrZU0mpJHXDbaotMsIyjQF1UUhXc2TAxNkeut0Xuo+qipVVQjaqbQhKF07jx420oaszvfLLuUXHtM4KSKpz+73//o3Pnzvz4449UqVKFO++8kwULFlCrVkliykAgEMg8QvCRHqJKpW8CF0l6GKiLy3UYhDsZNirIVRtYYk5KvC9umSZVxgNX4w5Ki+V1vAmcg1MgzcXlOPwYJ8e9GJd3MQE4FTf7UhJKq6D6AdBG0u64XJKjcQmxO42kA4DVZrZR0j64GaA7gP8B90pqamZfSapb0bMfJVU4/cUvflGw8yUQCASyiRB8pAEzWy0pplT6Ei7/YA4u0fJqM/ufpNXAVrmD4sbhdsY8I6kPTs10Q+LeEzIRl8x5Q6RsKDDWK5b+hJMZj+cBYLK3oaRjgtslMtqPsRvwBslnXq6U1AM3azEfd2jcZkkTcCf0foU7oK60tAZuk2S4s2du9UFPLGH0WT/LswJ30N0vgFlALWCbpCuBNmb2YxpsCQQCgUAKhOAjTdiOSqWD4p7n4xJBo3SIXA/29RbjVUaLGGs5cb87/63+9AR1x+GCnVi7IyKPY2PmAXmRNrmR64JnVnoFVczsatysTXx5k0Q2xz9L0G4q27+P0WcvEScbb2b/wx0+FwgEAoEKImS3BQLlxKZNm+jSpQsdO3akbdu2DBkyBIBRo0bRvHlzJLFq1aqC+pMnT6ZDhw7k5OTQuXNn3nqrVCtUgUAgkDHs0sFHutVESzBuQ0kTi6lzXJyS53p/emuqY+RKer701hY7Tv8EqqNpU1CNjNM+wTgzI8+rSvqoOJ+9Iut0/36OSredRZFMXv3II4/kv//9LwcddNB29Y8++uiCumPHjuWCCy5I0nMgEAhULsKySwVgTn2zdzF1tjuYzYuJDUraoIKwJIfclcE48yhaSfUK3Am/xW0F2QT8E7e0VeTyVrpJJq9+6KGHJqwfqwuwYcMG4pKHA4FAoNKSdcGHpBHAt2Y22t8PxSVW7suOqqPRdv2AzuZlyP036FvNLE/SeuA+3LbRZcDfgVtwQltXmtkUSVVxKqa5OKGu0Wb2f0lsbIIT22rnxz0dp3R6CHArbgvsubjtoidGdmmcK2kM7vd2npm9J6kLLvm0BrAR6G9mC+PGS1jHj30qTvDsYGCSz8lIqA4qaS+cmFk73E6ZoWY2OYmPbXFBSXXcDNuZQH7Mb1/nKqCmmQ31wdVHuG2yewF9gL/hFFfHm9m1icbx/RwAnAT8C/hLpPxw7/de/r082szWAW9Jap6sv0SUh7x6IiZNmsTf/vY3VqxYwQsv7Pz4gUAgkElkXfCB24Z6JxCb+j8LuBnohdPUqA+8L+mNEvS5F/CamQ2SNAkYDhyLEwZ7GCeOdT6w1swO91tJZ0h6NSZyVQztgENxwcEiYLCZHSrpDtyH8J2+3p5mliOpO05KvR3wKXCUmW2RdAwuYDgzrv+i6uT4sTcDCyXdg5sdeADoHtum6uv+w78P53nRr/ck/dfMEu2auRi4y8we9+evxBRfi+JnM+ss6QpgMm5b8PfAF5LuMHeOTCLuxCWxFmxl9mOOB842s/e94NrGYsbfjvKWV9+0aRMzZsygdu3aBXX32Wcf7r//fubMmcNll13GbbfdttM2xMgIaeU0kk3+BF8yl2zyJxN8ybrgw8w+krSvpIY4ie0fcB+wT5rZVmC5pJjq6NzkPW3Hz7itqeDUNzebWb5XJ23iy3sBHSTFllNq42YyUgk+pvtv5OskrcVJk8fGiu7keNL7+IakWj4A2Bt4WNIhuFmdRNodtYuoM83M1gJIWgAcBOxDYnXQXsCpKjwNtgZeZj3BmO8A//CzEs+a2ecpLBtEFU7nm9kyb9eXOAn1HYIPSScDK8zsA69vEqMlsMzM3vc+lHgrrZn9G/g3QMuWLe3yc04raRdF8uGHH7J69Wr69+8PQI0aNTjyyCOpX7/+DnVzc3O56667aNeuXcLnJSEvL4/c3NxS9ZFJZJM/wZfMJZv8yQRfsjXh9GlcTsXZuG+/qVCU+ma+16iHiPqmmUWVNwVcbmY5/tXUzF4lNVJRFwUXOBB3fwMueGkHnMKOqqEUUyc69laKDkgFnBnxsbGZJQo8MLMncEs6G4EXJfWkbBROj8QFRIuBp4Cekh4rwocKI5m8ejIWLVoUO4eGDz/8kM2bN1OvXr3yMDUQCATKlGwNPsYDv8MFIE/j1D/P9jsiGuBUR9+La7MYyJFURdKBQJcSjvkKMEBSNQBJLXyORDo52/fdDbfEsxY3q7HUP++XpF0qdaK8C3SX1NSPF1t2eQW4XH4KQ1LiTEn3rBnwpZndjVtC6YCTk9/X7zjZHTg5BVuKxMz+ZmYHeC2Q3+GWhf4ILAT293kfSNpbUoXO9C1btowePXrQoUMHDj/8cI499lhOPvlk7r77bg444ACWLFlChw4dCna1PPPMM7Rr146cnBwuvfRSxo8fH5JOA4FAVpB1yy4AZjZf0t7AUjNb5vM0urKj6miTSLMZuCWSBbhlhA9LOOwY3BLMh/7DeSUJRL9KySZJH+GWTc7zZbfgllSuBZJlJKZSpwAzW6kE6qC4GZQ7gbm+/CuSBxBn4RJk83FS5zf6parrcYHfUlwuSplgZj9LOhu4R9IeuBmYY4D1fpakFlBd0ulALzNbUFa2xEgmrz5w4EAGDhy4Q/ngwYMZPHhwWZsVCAQC5Y4KVxMCgUAyWrZsaQsXLiy+YiUgE9Z700k2+RN8yVyyyZ/y9EXSB2bWOb48W5ddAoGMo6QKp2bGwIEDad68OR06dODDD0s6GRcIBAKZSVYuu2QKktoDj8YVbzazosUdKhGSjsNtZY7ylZmdkeZx6gHTEjw6uogtuBlFTOG0Zs2a5Ofn061bN0444QSOPPJITj755B2+ibz00kt8/vnnfP7558ycOZMBAwYwc+bMxJ0HAoFAJWKXDj6iYl9l0X8yVU6/DfhuMytS5TSuTR5wlZnNSrF+rq9f6qTOoohXYi3DcVaTROFU0lhc7smKVH6Xkl7GHbD3Vlm/P3HjlkjhdPLkyfTp0wdJHHHEEaxZs4Zly5ax//77l5fJgUAgUCbs0sFHRZGKvHqgRIwDRgGPpFh/JE7V9aJUB6gIhdOlS5dy4IEHFtwfcMABLF26NAQfgUCg0pN1wUeQV9/15NW96FqTBDY0B+7Hic1tBX5rZl+Y2bQ4QbKEVLTC6erVq/noo4/YssWN+8MPP/DBBx+wfv36nbYDMkPdMJ1kkz/Bl8wlm/zJCF/MLKteOKnw1yP3C4C+wFQKJb6/AfbHbY392NfrB4yKtHseyPXXBpzgrycBr+I+fDsCs335hcC1/np3YBbQNImN8eMuwimVNgDWAhf7Z3fgghuAPOABf9090r4WsJu/PgZ4xl/n4j7oi6rTD/gSpwNSA/gapyTaAPg2Zj9Q1/+8Efijv64DfAbslcTHe4Bz/HV1YI+o3778KlwAE/PvZn99BfCd/x3tDiwB6hXze9+ub182EzjDX9fAydMT//6k8mrRooWlm2HDhtnIkSML7g866CBbuXJlwf2FF15oTzzxRMF9ixYt7Lvvviv1uNOnTy91H5lENvkTfMlcssmf8vQFmGUJ/k/Nut0uZvYRTsiqoaSOxMmrm9lyICavnirx8uqvm1m+v27iy3sBfSTNxn3o1cPNZKTCdDNbZ2YrccFHVF69SaRegbw6EJNXrw08LeljXLDSNkH/RdWZZmZrzWwTLlA7CJcPkUxe/RrvYx6F8uqJeAf4u6TBwEFmlsq5KjvIq5vZZlyAdGDyZjvidV4amdkk78MmM/upJH2km5IqnJ566qk88sgjmBnvvvsutWvXDksugUAgK8i64MMT5NW3J5vl1SsNJVU4PfHEE2nWrBnNmzfnT3/6E/fee28FexAIBALpodL/h56E8bhTWesDv8apm14k6WGgLm7ZYhDbf/gtBi7xyp2N2Hl59dfMKXm2wCmsJjrxdWc5G5gelVeXVFby6vdKamr+VFs/+xGTV7/czEzSoX6maQei8uqSGuPk1d/Ey6sD63E7VF5O1L60mNk6SUsknW5mz3k596oVOftRUoVTSYwePXqH8kAgEKjsZOXMh5nNx+VQLDV3Muok3Am2c4DX8PLqcc2i8up3s3Py6gtw8uofA/9H+oO7mLz6/cD5vuwW4CZfnmy8VOoU4Jd/YvLqcyicPboBl+syV9J8f5+Ms4CP/RJNO+ARv1QVk1efSprk1SU9iVvmaekDjth7cy4wUNJc4G3gF77+m7jZsaN9/ePSYUcgEAgEUiPIqwcCKZAOefVNmzbRvXt3Nm/ezJYtW+jduzfDhg3jq6++4ne/+x2rV6+mU6dOPProo1SvXp1vvvmGvn37smbNGrZu3cqIESM48cQTS+1LNslEQ3b5E3zJXLLJnyCvHgjsQsQUTufMmcPs2bN5+eWXeffddxk8eDB//vOfWbRoEfvssw8PPvggAMOHD+ess87io48+4qmnnuKSSy6pYA8CgUAgPYTgIw1IqiNph08GSe0lzfavBZIWSypSH1tSE79sky7b+kkala7+EvR/XMTH2GtSGYxTL8E4s315Y0mvSvrEv89NiulnuqT1Zfm+JBk7ocLpa6+9Ru/eTnOub9++PPfccwX1f/zxRwDWrl1Lw4YNy9PcQCAQKDOyNeG0vKkDXAJstx3BIvLq5SV3Xt5YZsirPwP8y8ymSqqJ2x2TjE3AP3F5KCnL6peVwunBBx9MnTp12G03908xpmIKMHToUHr16sU999zDhg0b+O9//7vT4wcCgUAmEYKP9DACONgnV071ZfFqqiOA1r7Ow7gk2Edxap4Al5nZ28UNJOld4HyfVFtw5gtOC2Ms0Az4CbjQzObGtR2HE9aa6O/Xm1lNHxgNA9bgFEUn4LQ2rsCJg51uZl9IaoBLdo1pe1xpZjOS2PlrnKoq/n3oDnQiEoD5mYdZZjZO0mKcjskJuC25FwI3Ac2BkWZ2f5Jx2uAE1KYCmNn6yLPDvQ174bbuHm1m64C3vPppkZSHwukBBxzAxo0bC56vWLGCDRs2kJeXx4QJEzjqqKM466yzmD9/PmeeeSZjx46lSpXSTVhmhLphGskmf4IvmUs2+ZMRviRSHguvEquqNqFQcfRMEqup5hJR1MRJmtfw14fgVeBIoNQZN9afgWH+en9gob++Bxjir3tSqLzaD6/cijsDpXekr/VWqPa5hkJF0aWRMa4A7vTXTwDd/HVj4JMi7PwPcKS/rokLdOPfg1FAP3+9GBjgr+/A7U6Kqb4uL2Kc03FqtM/i5NlH+ve+Oi4gO9zXK1B5jX9fUnmVlcLpLbfcYvXq1bP8/HwzM3v77betV69eZmbWpk0b++abbwrqN23a1JYvX17qcbNJqdEsu/wJvmQu2eRPUDjNTrqRmppqNeABSfNw2z7bpNj/BAoPpTsLmBgZ91EAM3sNqCepVgnsft8KFUW/wEnIw/Yqq8cAo/zszRScymrNJP3NAG6XNBCoY2apTBtEFU5nWqHq62av5pqI3XDnwVyFe5+b4QKLlsAyM3sfwMx+TNGGMiORwmnr1q3p0aMHEye6X+PDDz/MaaedBkDjxo2ZNm0aAJ988gmbNm2iQYMGFWJ7IBAIpJOw7FJx/BlYjjsfpgouF6FYzGyppNWSOuBExy4uwZgFCqNeTK165FkqKqtVgCPMSbEXZ+cISS/gDuOb4bU0ykLhdAluludL79dzOHn494qzsbxZtmwZffv2ZevWrWzbto2zzjqLk08+mTZt2vC73/2Oa6+9lkMPPZTzz3cyJbfddht/+tOfuOOOO5DEuHHjkFTBXgQCgUDpCcFHeliHWyIAp+KZSE21UaQOONXRJWa2TVJf3FJBqowHrgZqW2Fex5vAOcANPodjlZn9GPdhtRiXdzEBJ31erQRjgpsNuRy3tIGkHDObnaiipIPNJdzO87kXrYAPgDZebXQP4GjgrRLaEM/7QB1JDfwsSU/coX4Lgf0lHW5m7/uzXjZW5OxHMoXTZs2a8d57O8ZKbdq0YcaMhCk1gUAgUKkJwUcaMLPVkmb4LbIvUaimang1VUmrga1eMXQcbmfMM5L64CTGSyLDPhGXSBlVGB0KjPVqnj/hTvKN5wFgsrehpGMCDARG+zF2A94g+czLlZJ64GYt5gMvmdlmSROAj3Fqsgml2UuCmW2VdBUwTS7S+gB3+u/Pks4G7pG0B+6MmWOA9T65tRZQXdLpQC8zW1BaWwKBQCCQGiH4SBNm9oe4okFxz/Nx38qjdIhcD/b1FlPMFlCfS7JbXNn3uOTL+LrjcMFOrN0RCcbMw51SG2uTG7kueGZmq3BLPcViZpcnKb8aN2sTX94kkc3xz5L0OZXt38tY+fts729K/QUCgUCgbAkJp4FAIBAIBMqVMPORofgEzZvjir8yszMqwp5kSOqP244bZYaZXZrmcdrjd/NE2Gxmv0znOIFAIBAoe0LwkaFYOSmHlhYzewh4qBzGKVCLDQQCgUDlJiy7BAKBQCAQKFfkBMgCgUBRSFqH276bDdQHVlW0EWkkm/wJvmQu2eRPefpykJntoI4Yll0CgdRYaGadK9qIdCBpVrb4AtnlT/Alc8kmfzLBl7DsEggEAoFAoFwJwUcgEAgEAoFyJQQfgUBq/LuiDUgj2eQLZJc/wZfMJZv8qXBfQsJpIBAIBAKBciXMfAQCgUAgEChXQvARCAQCgUCgXAnBRyBQBJKOl7RQ0iJJ11S0PcmQNFbSCn+ycqysrqSpkj73P/fx5ZJ0t/dprqTDIm36+vqfS0p0MnJ5+HKgpOmSFkiaL+mKyuqPpBqS3pM0x/syzJc3lTTT2zxeUnVfvru/X+SfN4n09TdfvtAfv1AhSKoq6SNJz/v7yuzLYknzJM2WNMuXVbq/M29DHUkTJX0q6RNJXTPaFzMLr/AKrwQvoCrwBdAMqA7MAdpUtF1JbO0OHAZ8HCm7BbjGX18D3OyvTwReAoQ79XemL68LfOl/7uOv96kAX/YHDvPXewOfAW0qoz/eppr+uhow09s4AfidL78fGOCvLwHu99e/A8b76zb+7293oKn/u6xaQX9rfwGeAJ7395XZl8VA/biySvd35u14GLjAX1cH6mSyL2HmIxBIThdgkZl9aWY/A08Bp1WwTQkxszeA7+OKT8P9h4T/eXqk/BFzvAvUkbQ/cBww1cy+N7MfgKnA8WVufBxmtszMPvTX64BPgEZUQn+8Tev9bTX/MqAnMNGXx/sS83EicLQk+fKnzGyzmX0FLML9fZYrkg4ATgLG+HtRSX0pgkr3dyapNu4LyIMAZvazma0hg30JwUcgkJxGwLeR+yW+rLKwn5kt89f/A/bz18n8yjh//VT9obgZg0rpj1+mmA2swP1n/gWwxsy2JLCrwGb/fC1QjwzxBbgTuBrY5u/rUXl9ARcIvirpA0kX+rLK+HfWFFgJPOSXxMZI2osM9iUEH4HALoC5OdVKta9eUk3gGeBKM/sx+qwy+WNmW80sBzgA9w2/VcVatHNIOhlYYWYfVLQtaaSbmR0GnABcKql79GEl+jvbDbfsep+ZHQpswC2zFJBpvoTgIxBIzlLgwMj9Ab6ssrDcT6Xif67w5cn8yhh/JVXDBR6Pm9mzvrjS+gPgp8GnA11x09yxs7WidhXY7J/XBlaTGb4cCZwqaTFuCbIncBeV0xcAzGyp/7kCmIQLDivj39kSYImZzfT3E3HBSMb6EoKPQCA57wOH+Gz+6rikuSkVbFNJmALEstX7ApMj5X18xvsRwFo/NfsK0EvSPj4rvpcvK1d8XsCDwCdmdnvkUaXzR1IDSXX89R7AsbgclulAb18t3peYj72B1/w31inA7/wOkqbAIcB75eKEx8z+ZmYHmFkT3L+F18zsHCqhLwCS9pK0d+wa9/fxMZXw78zM/gd8K6mlLzoaWEAm+1IWWazhFV7Z8sJlhX+GW6f/R0XbU4SdTwLLgHzct6Dzcevr04DPgf8CdX1dAaO9T/OAzpF+zsMlAC4C+leQL91w08Nzgdn+dWJl9AfoAHzkffkYuM6XN8N94C4CngZ29+U1/P0i/7xZpK9/eB8XAidU8N9bLoW7XSqlL97uOf41P/bvuzL+nXkbcoBZ/m/tOdxulYz1JcirBwKBQCAQKFfCsksgEAgEAoFyJQQfgUAgEAgEypUQfAQCgUAgEChXQvARCAQCgUCgXAnBRyAQCAQCgXIlBB+BQGCXRtJWf6pp7NVkJ/o4XVKbMjAPSQ0lTSy+ZlrHzJF0YnmOGdi12K34KoFAIJDVbDQnf14aTgeexwk7pYSk3azwTJSkmNl3FIp4lTlejTQH6Ay8WF7jBnYtwsxHIBAIxCGpk6TX/YFjr0Qkqv8k6X1JcyQ9I2lPSb8CTgVG+pmTgyXlSers29T3kuRI6idpiqTXgGleZXOspPf8gWA7nJosqYmkjyPtn5M0VdJiSZdJ+otv+66kur5enqS7vD0fS+riy+v69nN9/Q6+fKikRyXNAB4FrgfO9u3PltRF0jt+nLdjSprenmclvSzpc0m3ROw+XtKH/r2a5suK9TewaxBmPgKBwK7OHnKnzgJ8BZwF3AOcZmYrJZ0N/Aun/PismT0AIGk4cL6Z3SNpCk7xc6J/VtR4hwEdzOx7STfiZMfP8zLs70n6r5ltKKJ9O9xJvzVwKpSDzexQSXcAfXAnzwLsaWY5coeljfXthgEfmdnpknoCj+BmOQDa4A5a2yipH0718jLvTy3gKDPbIukY4EbgTN8ux9uzGVgo6R5gE/AA0N3MvooFRThl05L6G8hCQvARCAR2dbZbdpHUDvdBPdUHEVVx0vUA7XzQUQeoyc6dezHVzL73171wh7Vd5e9rAI1x578kY7qZrQPWSVoL/MeXz8PJucd4EsDM3pBUy3/Yd8MHDWb2mqR6PrAAmGJmG5OMWRt4WNIhOOn7apFn08xsLYCkBcBBOGnvN8zsKz9WafwNZCEh+AgEAoHtETDfzLomeDYOON3M5vjZgdwkfWyhcFm7Rtyz6Ld8AWea2cIS2Lc5cr0tcr+N7f9Pjz87o7izNIqafbgBF/Sc4RNy85LYs5WiP1d2xt9AFhJyPgKBQGB7FgINJHUFkFRNUlv/bG9gmaRqwDmRNuv8sxiLgU7+uqhk0VeAy+WnWCQdWnrzCzjb99kNd2rpWuBNvN2ScoFVZvZjgrbx/tSm8Gj1fimM/S7QXe7UWiLLLmXpb6ASEYKPQCAQiGBmP+MChpslzcGdqvsr//ifwExgBvBppNlTwCCfRHkwcCswQNJHQP0ihrsBt4QxV9J8f58uNvnx78edcgwwFOgkaS4wgsLj1uOZDrSJJZwCtwA3+f6KnTE3s5XAhcCz/j0c7x+Vpb+BSkQ41TYQCASyDEl5wFVmNquibQkEEhFmPgKBQCAQCJQrYeYjEAgEAoFAuRJmPgKBQCAQCJQrIfgIBAKBQCBQroTgIxAIBAKBQLkSgo9AIBAIBALlSgg+AoFAIBAIlCv/D108ug60/K3+AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "import lightgbm as lgb\n",
    "\n",
    "seed0=2021\n",
    "params0 = {\n",
    "    'objective': 'rmse',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'max_depth': -1,\n",
    "    'max_bin':100,\n",
    "    'min_data_in_leaf':500,\n",
    "    'learning_rate': 0.05,\n",
    "    'subsample': 0.72,\n",
    "    'subsample_freq': 4,\n",
    "    'feature_fraction': 0.5,\n",
    "    'lambda_l1': 0.5,\n",
    "    'lambda_l2': 1.0,\n",
    "    'categorical_column':[0],\n",
    "    'seed':seed0,\n",
    "    'feature_fraction_seed': seed0,\n",
    "    'bagging_seed': seed0,\n",
    "    'drop_seed': seed0,\n",
    "    'data_random_seed': seed0,\n",
    "    'n_jobs':-1,\n",
    "    'verbose': -1}\n",
    "seed1=42\n",
    "params1 = {\n",
    "        'learning_rate': 0.1,        \n",
    "        'lambda_l1': 2,\n",
    "        'lambda_l2': 7,\n",
    "        'num_leaves': 800,\n",
    "        'min_sum_hessian_in_leaf': 20,\n",
    "        'feature_fraction': 0.8,\n",
    "        'feature_fraction_bynode': 0.8,\n",
    "        'bagging_fraction': 0.9,\n",
    "        'bagging_freq': 42,\n",
    "        'min_data_in_leaf': 700,\n",
    "        'max_depth': 4,\n",
    "        'categorical_column':[0],\n",
    "        'seed': seed1,\n",
    "        'feature_fraction_seed': seed1,\n",
    "        'bagging_seed': seed1,\n",
    "        'drop_seed': seed1,\n",
    "        'data_random_seed': seed1,\n",
    "        'objective': 'rmse',\n",
    "        'boosting': 'gbdt',\n",
    "        'verbosity': -1,\n",
    "        'n_jobs':-1,\n",
    "    }\n",
    "# Function to early stop with root mean squared percentage error\n",
    "def rmspe(y_true, y_pred):\n",
    "    return np.sqrt(np.mean(np.square((y_true - y_pred) / y_true)))\n",
    "\n",
    "def feval_rmspe(y_pred, lgb_train):\n",
    "    y_true = lgb_train.get_label()\n",
    "    return 'RMSPE', rmspe(y_true, y_pred), False\n",
    "\n",
    "def train_and_evaluate_lgb(train, test, params):\n",
    "    # Hyperparammeters (just basic)\n",
    "    \n",
    "    features = [col for col in train.columns if col not in {\"time_id\", \"target\", \"row_id\"}]\n",
    "    # Create out of folds array\n",
    "    y = train['target']\n",
    "    oof_predictions = np.zeros(train.shape[0])\n",
    "    # Create test array to store predictions\n",
    "    test_predictions = np.zeros(test.shape[0])\n",
    "    # Create a KFold object\n",
    "    kfold = KFold(n_splits = 5, random_state = 2021, shuffle = True)\n",
    "    # Iterate through each fold\n",
    "    for fold, (trn_ind, val_ind) in enumerate(kfold.split(train)):\n",
    "        print(f'Training fold {fold + 1}')\n",
    "        x_train, x_val = train.iloc[trn_ind], train.iloc[val_ind]\n",
    "        y_train, y_val = y.iloc[trn_ind], y.iloc[val_ind]\n",
    "        # Root mean squared percentage error weights\n",
    "        train_weights = 1 / np.square(y_train)\n",
    "        val_weights = 1 / np.square(y_val)\n",
    "        train_dataset = lgb.Dataset(x_train[features], y_train, weight = train_weights)\n",
    "        val_dataset = lgb.Dataset(x_val[features], y_val, weight = val_weights)\n",
    "        model = lgb.train(params = params,\n",
    "                          num_boost_round=1300,\n",
    "                          train_set = train_dataset, \n",
    "                          valid_sets = [train_dataset, val_dataset], \n",
    "                          verbose_eval = 250,\n",
    "                          early_stopping_rounds=50,\n",
    "                          feval = feval_rmspe)\n",
    "        # Add predictions to the out of folds array\n",
    "        oof_predictions[val_ind] = model.predict(x_val[features])\n",
    "        # Predict the test set\n",
    "        test_predictions += model.predict(test[features]) / 5\n",
    "    rmspe_score = rmspe(y, oof_predictions)\n",
    "    print(f'Our out of folds RMSPE is {rmspe_score}')\n",
    "    lgb.plot_importance(model,max_num_features=20)\n",
    "    # Return test predictions\n",
    "    return test_predictions\n",
    "# Traing and evaluate\n",
    "predictions_lgb_1= train_and_evaluate_lgb(train, test,params0)\n",
    "# predictions_lgb_2= train_and_evaluate_lgb(train, test,params1)\n",
    "# test['target'] = predictions_lgb_1\n",
    "# test[['row_id', 'target']].to_csv('submission.csv',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2d107b06",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-16T18:29:27.671642Z",
     "iopub.status.busy": "2023-12-16T18:29:27.670861Z",
     "iopub.status.idle": "2023-12-16T18:29:27.674641Z",
     "shell.execute_reply": "2023-12-16T18:29:27.674060Z",
     "shell.execute_reply.started": "2023-12-15T12:59:44.071696Z"
    },
    "papermill": {
     "duration": 0.060966,
     "end_time": "2023-12-16T18:29:27.674774",
     "exception": false,
     "start_time": "2023-12-16T18:29:27.613808",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "280"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape[1]\n",
    "#0.19065"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a3d93bff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-16T18:29:27.787822Z",
     "iopub.status.busy": "2023-12-16T18:29:27.787142Z",
     "iopub.status.idle": "2023-12-16T18:29:33.317782Z",
     "shell.execute_reply": "2023-12-16T18:29:33.317112Z",
     "shell.execute_reply.started": "2023-12-15T12:59:45.754100Z"
    },
    "papermill": {
     "duration": 5.589246,
     "end_time": "2023-12-16T18:29:33.317932",
     "exception": false,
     "start_time": "2023-12-16T18:29:27.728686",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "from numpy.random import seed\n",
    "seed(42)\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(42)\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "from keras import backend as K\n",
    "def root_mean_squared_per_error(y_true, y_pred):\n",
    "         return K.sqrt(K.mean(K.square( (y_true - y_pred)/ y_true )))\n",
    "    \n",
    "es = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', patience=20, verbose=0,\n",
    "    mode='min',restore_best_weights=True)\n",
    "\n",
    "plateau = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss', factor=0.195, patience=7, verbose=0,\n",
    "    mode='min')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "97319361",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-16T18:29:33.444581Z",
     "iopub.status.busy": "2023-12-16T18:29:33.443729Z",
     "iopub.status.idle": "2023-12-16T18:29:42.227946Z",
     "shell.execute_reply": "2023-12-16T18:29:42.227166Z",
     "shell.execute_reply.started": "2023-12-15T13:00:09.454704Z"
    },
    "papermill": {
     "duration": 8.853962,
     "end_time": "2023-12-16T18:29:42.228096",
     "exception": false,
     "start_time": "2023-12-16T18:29:33.374134",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# kfold based on the knn++ algorithm\n",
    "\n",
    "out_train = pd.read_csv('../input/optiver-realized-volatility-prediction/train.csv')\n",
    "out_train = out_train.pivot(index='time_id', columns='stock_id', values='target')\n",
    "\n",
    "#out_train[out_train.isna().any(axis=1)]\n",
    "out_train = out_train.fillna(out_train.mean())\n",
    "out_train.head()\n",
    "\n",
    "# code to add the just the read data after first execution\n",
    "\n",
    "# data separation based on knn ++\n",
    "nfolds = 5 # number of folds\n",
    "index = []\n",
    "totDist = []\n",
    "values = []\n",
    "# generates a matriz with the values of \n",
    "mat = out_train.values\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "mat = scaler.fit_transform(mat)\n",
    "\n",
    "nind = int(mat.shape[0]/nfolds) # number of individuals\n",
    "\n",
    "# adds index in the last column\n",
    "mat = np.c_[mat,np.arange(mat.shape[0])]\n",
    "\n",
    "\n",
    "lineNumber = np.random.choice(np.array(mat.shape[0]), size=nfolds, replace=False)\n",
    "\n",
    "lineNumber = np.sort(lineNumber)[::-1]\n",
    "\n",
    "for n in range(nfolds):\n",
    "    totDist.append(np.zeros(mat.shape[0]-nfolds))\n",
    "\n",
    "# saves index\n",
    "for n in range(nfolds):\n",
    "    \n",
    "    values.append([lineNumber[n]])    \n",
    "\n",
    "\n",
    "s=[]\n",
    "for n in range(nfolds):\n",
    "    s.append(mat[lineNumber[n],:])\n",
    "    \n",
    "    mat = np.delete(mat, obj=lineNumber[n], axis=0)\n",
    "\n",
    "for n in range(nind-1):    \n",
    "\n",
    "    luck = np.random.uniform(0,1,nfolds)\n",
    "    \n",
    "    for cycle in range(nfolds):\n",
    "         # saves the values of index           \n",
    "\n",
    "        s[cycle] = np.matlib.repmat(s[cycle], mat.shape[0], 1)\n",
    "\n",
    "        sumDist = np.sum( (mat[:,:-1] - s[cycle][:,:-1])**2 , axis=1)   \n",
    "        totDist[cycle] += sumDist        \n",
    "                \n",
    "        # probabilities\n",
    "        f = totDist[cycle]/np.sum(totDist[cycle]) # normalizing the totdist\n",
    "        j = 0\n",
    "        kn = 0\n",
    "        for val in f:\n",
    "            j += val        \n",
    "            if (j > luck[cycle]): # the column was selected\n",
    "                break\n",
    "            kn +=1\n",
    "        lineNumber[cycle] = kn\n",
    "        \n",
    "        # delete line of the value added    \n",
    "        for n_iter in range(nfolds):\n",
    "            \n",
    "            totDist[n_iter] = np.delete(totDist[n_iter],obj=lineNumber[cycle], axis=0)\n",
    "            j= 0\n",
    "        \n",
    "        s[cycle] = mat[lineNumber[cycle],:]\n",
    "        values[cycle].append(int(mat[lineNumber[cycle],-1]))\n",
    "        mat = np.delete(mat, obj=lineNumber[cycle], axis=0)\n",
    "\n",
    "\n",
    "for n_mod in range(nfolds):\n",
    "    values[n_mod] = out_train.index[values[n_mod]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a7392a24",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-16T18:29:42.345845Z",
     "iopub.status.busy": "2023-12-16T18:29:42.345179Z",
     "iopub.status.idle": "2023-12-16T18:30:10.974549Z",
     "shell.execute_reply": "2023-12-16T18:30:10.973896Z",
     "shell.execute_reply.started": "2023-12-15T13:00:46.820602Z"
    },
    "papermill": {
     "duration": 28.689179,
     "end_time": "2023-12-16T18:30:10.974720",
     "exception": false,
     "start_time": "2023-12-16T18:29:42.285541",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#colNames.remove('row_id')\n",
    "train.replace([np.inf, -np.inf], np.nan,inplace=True)\n",
    "test.replace([np.inf, -np.inf], np.nan,inplace=True)\n",
    "qt_train = []\n",
    "train_nn=train[colNames].copy()\n",
    "test_nn=test[colNames].copy()\n",
    "for col in colNames:\n",
    "    #print(col)\n",
    "    qt = QuantileTransformer(random_state=21,n_quantiles=2000, output_distribution='normal')\n",
    "    train_nn[col] = qt.fit_transform(train_nn[[col]])\n",
    "    test_nn[col] = qt.transform(test_nn[[col]])    \n",
    "    qt_train.append(qt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "914ca0d2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-16T18:30:11.094178Z",
     "iopub.status.busy": "2023-12-16T18:30:11.093561Z",
     "iopub.status.idle": "2023-12-16T18:30:11.101727Z",
     "shell.execute_reply": "2023-12-16T18:30:11.101069Z",
     "shell.execute_reply.started": "2023-12-15T13:01:19.727543Z"
    },
    "papermill": {
     "duration": 0.070875,
     "end_time": "2023-12-16T18:30:11.101854",
     "exception": false,
     "start_time": "2023-12-16T18:30:11.030979",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_nn[['stock_id','time_id','target']]=train[['stock_id','time_id','target']]\n",
    "test_nn[['stock_id','time_id']]=test[['stock_id','time_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9baaa230",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-16T18:30:11.222460Z",
     "iopub.status.busy": "2023-12-16T18:30:11.221809Z",
     "iopub.status.idle": "2023-12-16T18:30:13.158190Z",
     "shell.execute_reply": "2023-12-16T18:30:13.157575Z",
     "shell.execute_reply.started": "2023-12-15T13:01:22.489203Z"
    },
    "papermill": {
     "duration": 2.001811,
     "end_time": "2023-12-16T18:30:13.158356",
     "exception": false,
     "start_time": "2023-12-16T18:30:11.156545",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 4 2 1 1 2 4 6 2 1 0 4 4 1 1 1 2 4 4 4 0 1 1 3 1 1 4 3 4 3 4 4 1 3 3 4\n",
      " 3 4 1 4 1 4 4 1 0 4 4 1 0 0 3 3 3 2 0 2 4 1 4 4 1 4 1 0 3 3 0 3 0 6 5 3 3\n",
      " 0 1 2 0 3 3 3 4 1 1 0 2 3 3 1 0 1 4 4 4 4 4 1 3 1 0 1 4 1 0 1 4 1 0 4 0 4\n",
      " 0]\n",
      "[1, 11, 22, 50, 55, 56, 62, 73, 76, 78, 84, 87, 96, 101, 112, 116, 122, 124, 126]\n",
      "[0, 4, 5, 10, 15, 16, 17, 23, 26, 28, 29, 36, 42, 44, 48, 53, 66, 69, 72, 85, 94, 95, 100, 102, 109, 111, 113, 115, 118, 120]\n",
      "[3, 6, 9, 18, 61, 63, 86, 97]\n",
      "[27, 31, 33, 37, 38, 40, 58, 59, 60, 74, 75, 77, 82, 83, 88, 89, 90, 98, 99, 110]\n",
      "[2, 7, 13, 14, 19, 20, 21, 30, 32, 34, 35, 39, 41, 43, 46, 47, 51, 52, 64, 67, 68, 70, 93, 103, 104, 105, 107, 108, 114, 119, 123, 125]\n",
      "[81]\n",
      "[8, 80]\n"
     ]
    }
   ],
   "source": [
    "# making agg features\n",
    "from sklearn.cluster import KMeans\n",
    "train_p = pd.read_csv('../input/optiver-realized-volatility-prediction/train.csv')\n",
    "train_p = train_p.pivot(index='time_id', columns='stock_id', values='target')\n",
    "\n",
    "corr = train_p.corr()\n",
    "\n",
    "ids = corr.index\n",
    "\n",
    "kmeans = KMeans(n_clusters=7, random_state=0).fit(corr.values)\n",
    "print(kmeans.labels_)\n",
    "\n",
    "l = []\n",
    "for n in range(7):\n",
    "    l.append ( [ (x-1) for x in ( (ids+1)*(kmeans.labels_ == n)) if x > 0] )\n",
    "    \n",
    "\n",
    "mat = []\n",
    "matTest = []\n",
    "\n",
    "n = 0\n",
    "for ind in l:\n",
    "    print(ind)\n",
    "    newDf = train_nn.loc[train_nn['stock_id'].isin(ind) ]\n",
    "    newDf = newDf.groupby(['time_id']).agg(np.nanmean)\n",
    "    newDf.loc[:,'stock_id'] = str(n)+'c1'\n",
    "    mat.append ( newDf )\n",
    "    \n",
    "    newDf = test_nn.loc[test_nn['stock_id'].isin(ind) ]    \n",
    "    newDf = newDf.groupby(['time_id']).agg(np.nanmean)\n",
    "    newDf.loc[:,'stock_id'] = str(n)+'c1'\n",
    "    matTest.append ( newDf )\n",
    "    \n",
    "    n+=1\n",
    "    \n",
    "mat1 = pd.concat(mat).reset_index()\n",
    "mat1.drop(columns=['target'],inplace=True)\n",
    "\n",
    "mat2 = pd.concat(matTest).reset_index()\n",
    "mat2 = pd.concat([mat2,mat1.loc[mat1.time_id==5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "215f8dfb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-16T18:30:13.276021Z",
     "iopub.status.busy": "2023-12-16T18:30:13.275191Z",
     "iopub.status.idle": "2023-12-16T18:30:13.278004Z",
     "shell.execute_reply": "2023-12-16T18:30:13.277413Z",
     "shell.execute_reply.started": "2023-12-15T13:01:28.257291Z"
    },
    "papermill": {
     "duration": 0.063884,
     "end_time": "2023-12-16T18:30:13.278121",
     "exception": false,
     "start_time": "2023-12-16T18:30:13.214237",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "nnn = ['time_id',\n",
    "     'log_return1_realized_volatility_0c1',\n",
    "     'log_return1_realized_volatility_1c1',     \n",
    "     'log_return1_realized_volatility_3c1',\n",
    "     'log_return1_realized_volatility_4c1',     \n",
    "     'log_return1_realized_volatility_6c1',\n",
    "     'total_volume_sum_0c1',\n",
    "     'total_volume_sum_1c1', \n",
    "     'total_volume_sum_3c1',\n",
    "     'total_volume_sum_4c1', \n",
    "     'total_volume_sum_6c1',\n",
    "     'trade_size_sum_0c1',\n",
    "     'trade_size_sum_1c1', \n",
    "     'trade_size_sum_3c1',\n",
    "     'trade_size_sum_4c1', \n",
    "     'trade_size_sum_6c1',\n",
    "     'trade_order_count_sum_0c1',\n",
    "     'trade_order_count_sum_1c1',\n",
    "     'trade_order_count_sum_3c1',\n",
    "     'trade_order_count_sum_4c1',\n",
    "     'trade_order_count_sum_6c1',      \n",
    "     'price_spread_sum_0c1',\n",
    "     'price_spread_sum_1c1',\n",
    "     'price_spread_sum_3c1',\n",
    "     'price_spread_sum_4c1',\n",
    "     'price_spread_sum_6c1',   \n",
    "     'bid_spread_sum_0c1',\n",
    "     'bid_spread_sum_1c1',\n",
    "     'bid_spread_sum_3c1',\n",
    "     'bid_spread_sum_4c1',\n",
    "     'bid_spread_sum_6c1',       \n",
    "     'ask_spread_sum_0c1',\n",
    "     'ask_spread_sum_1c1',\n",
    "     'ask_spread_sum_3c1',\n",
    "     'ask_spread_sum_4c1',\n",
    "     'ask_spread_sum_6c1',   \n",
    "     'volume_imbalance_sum_0c1',\n",
    "     'volume_imbalance_sum_1c1',\n",
    "     'volume_imbalance_sum_3c1',\n",
    "     'volume_imbalance_sum_4c1',\n",
    "     'volume_imbalance_sum_6c1',       \n",
    "     'bid_ask_spread_sum_0c1',\n",
    "     'bid_ask_spread_sum_1c1',\n",
    "     'bid_ask_spread_sum_3c1',\n",
    "     'bid_ask_spread_sum_4c1',\n",
    "     'bid_ask_spread_sum_6c1',\n",
    "     'size_tau2_0c1',\n",
    "     'size_tau2_1c1',\n",
    "     'size_tau2_3c1',\n",
    "     'size_tau2_4c1',\n",
    "     'size_tau2_6c1'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "63aeee20",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-16T18:30:13.395524Z",
     "iopub.status.busy": "2023-12-16T18:30:13.394905Z",
     "iopub.status.idle": "2023-12-16T18:30:13.579410Z",
     "shell.execute_reply": "2023-12-16T18:30:13.580013Z",
     "shell.execute_reply.started": "2023-12-15T13:01:31.866397Z"
    },
    "papermill": {
     "duration": 0.246608,
     "end_time": "2023-12-16T18:30:13.580200",
     "exception": false,
     "start_time": "2023-12-16T18:30:13.333592",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:2: FutureWarning: Index.ravel returning ndarray is deprecated; in a future version this will return a view on self.\n",
      "  \n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:6: FutureWarning: Index.ravel returning ndarray is deprecated; in a future version this will return a view on self.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "mat1 = mat1.pivot(index='time_id', columns='stock_id')\n",
    "mat1.columns = [\"_\".join(x) for x in mat1.columns.ravel()]\n",
    "mat1.reset_index(inplace=True)\n",
    "\n",
    "mat2 = mat2.pivot(index='time_id', columns='stock_id')\n",
    "mat2.columns = [\"_\".join(x) for x in mat2.columns.ravel()]\n",
    "mat2.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2828a847",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-16T18:30:13.708552Z",
     "iopub.status.busy": "2023-12-16T18:30:13.707627Z",
     "iopub.status.idle": "2023-12-16T18:30:27.555721Z",
     "shell.execute_reply": "2023-12-16T18:30:27.555046Z",
     "shell.execute_reply.started": "2023-12-15T13:01:33.854254Z"
    },
    "papermill": {
     "duration": 13.910579,
     "end_time": "2023-12-16T18:30:27.555868",
     "exception": false,
     "start_time": "2023-12-16T18:30:13.645289",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "train_nn = pd.merge(train_nn,mat1[nnn],how='left',on='time_id')\n",
    "test_nn = pd.merge(test_nn,mat2[nnn],how='left',on='time_id')\n",
    "del mat1,mat2\n",
    "del train,test\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fecbad6f",
   "metadata": {
    "papermill": {
     "duration": 0.056235,
     "end_time": "2023-12-16T18:30:27.667339",
     "exception": false,
     "start_time": "2023-12-16T18:30:27.611104",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## FFNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0c0c9bae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-16T18:30:27.790332Z",
     "iopub.status.busy": "2023-12-16T18:30:27.789684Z",
     "iopub.status.idle": "2023-12-16T18:30:27.805171Z",
     "shell.execute_reply": "2023-12-16T18:30:27.804580Z",
     "shell.execute_reply.started": "2023-12-15T13:01:50.087197Z"
    },
    "papermill": {
     "duration": 0.081628,
     "end_time": "2023-12-16T18:30:27.805308",
     "exception": false,
     "start_time": "2023-12-16T18:30:27.723680",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#https://bignerdranch.com/blog/implementing-swish-activation-function-in-keras/\n",
    "from keras.backend import sigmoid\n",
    "def swish(x, beta = 1):\n",
    "    return (x * sigmoid(beta * x))\n",
    "\n",
    "from keras.utils.generic_utils import get_custom_objects\n",
    "from keras.layers import Activation\n",
    "get_custom_objects().update({'swish': Activation(swish)})\n",
    "\n",
    "hidden_units = (128,64,32)\n",
    "stock_embedding_size = 24\n",
    "\n",
    "cat_data = train_nn['stock_id']\n",
    "\n",
    "def base_model():\n",
    "    \n",
    "    # Each instance will consist of two inputs: a single user id, and a single movie id\n",
    "    stock_id_input = keras.Input(shape=(1,), name='stock_id')\n",
    "    num_input = keras.Input(shape=(276,), name='num_data')\n",
    "\n",
    "\n",
    "    #embedding, flatenning and concatenating\n",
    "    stock_embedded = keras.layers.Embedding(max(cat_data)+1, stock_embedding_size, \n",
    "                                           input_length=1, name='stock_embedding')(stock_id_input)\n",
    "    stock_flattened = keras.layers.Flatten()(stock_embedded)\n",
    "    out = keras.layers.Concatenate()([stock_flattened, num_input])\n",
    "    \n",
    "    # Add one or more hidden layers\n",
    "    for n_hidden in hidden_units:\n",
    "\n",
    "        out = keras.layers.Dense(n_hidden, activation='swish')(out)\n",
    "        \n",
    "\n",
    "    #out = keras.layers.Concatenate()([out, num_input])\n",
    "\n",
    "    # A single output: our predicted rating\n",
    "    out = keras.layers.Dense(1, activation='linear', name='prediction')(out)\n",
    "    \n",
    "    model = keras.Model(\n",
    "    inputs = [stock_id_input, num_input],\n",
    "    outputs = out,\n",
    "    )\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "059f5c3f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-16T18:30:27.923074Z",
     "iopub.status.busy": "2023-12-16T18:30:27.922480Z",
     "iopub.status.idle": "2023-12-16T18:30:27.925490Z",
     "shell.execute_reply": "2023-12-16T18:30:27.924868Z",
     "shell.execute_reply.started": "2023-12-15T13:01:59.993683Z"
    },
    "papermill": {
     "duration": 0.063451,
     "end_time": "2023-12-16T18:30:27.925616",
     "exception": false,
     "start_time": "2023-12-16T18:30:27.862165",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to calculate the root mean squared percentage error\n",
    "def rmspe(y_true, y_pred):\n",
    "    return np.sqrt(np.mean(np.square((y_true - y_pred) / y_true)))\n",
    "\n",
    "# Function to early stop with root mean squared percentage error\n",
    "def feval_rmspe(y_pred, lgb_train):\n",
    "    y_true = lgb_train.get_label()\n",
    "    return 'RMSPE', rmspe(y_true, y_pred), False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7869c33d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-16T18:30:28.060752Z",
     "iopub.status.busy": "2023-12-16T18:30:28.051507Z",
     "iopub.status.idle": "2023-12-16T18:35:46.268824Z",
     "shell.execute_reply": "2023-12-16T18:35:46.268150Z",
     "shell.execute_reply.started": "2023-12-15T13:02:01.638835Z"
    },
    "papermill": {
     "duration": 318.286636,
     "end_time": "2023-12-16T18:35:46.269011",
     "exception": false,
     "start_time": "2023-12-16T18:30:27.982375",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV 1/5\n",
      "Epoch 1/1000\n",
      "168/168 [==============================] - 4s 10ms/step - loss: 22.7472 - val_loss: 0.8286\n",
      "Epoch 2/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.7889 - val_loss: 0.7669\n",
      "Epoch 3/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.7077 - val_loss: 0.5741\n",
      "Epoch 4/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.6528 - val_loss: 0.6530\n",
      "Epoch 5/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.6178 - val_loss: 0.5130\n",
      "Epoch 6/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.5455 - val_loss: 0.5842\n",
      "Epoch 7/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.5228 - val_loss: 0.5194\n",
      "Epoch 8/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.8056 - val_loss: 0.3246\n",
      "Epoch 9/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2714 - val_loss: 0.2376\n",
      "Epoch 10/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2472 - val_loss: 0.5042\n",
      "Epoch 11/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.4817 - val_loss: 0.4359\n",
      "Epoch 12/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.4338 - val_loss: 0.4146\n",
      "Epoch 13/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.3859 - val_loss: 0.3938\n",
      "Epoch 14/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.3641 - val_loss: 0.3725\n",
      "Epoch 15/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.3458 - val_loss: 0.3467\n",
      "Epoch 16/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.3532 - val_loss: 0.2739\n",
      "Epoch 17/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2203 - val_loss: 0.2114\n",
      "Epoch 18/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2113 - val_loss: 0.2102\n",
      "Epoch 19/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2096 - val_loss: 0.2114\n",
      "Epoch 20/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2096 - val_loss: 0.2139\n",
      "Epoch 21/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2102 - val_loss: 0.2115\n",
      "Epoch 22/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2097 - val_loss: 0.2114\n",
      "Epoch 23/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2095 - val_loss: 0.2102\n",
      "Epoch 24/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2081 - val_loss: 0.2165\n",
      "Epoch 25/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2177 - val_loss: 0.2118\n",
      "Epoch 26/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2072 - val_loss: 0.2090\n",
      "Epoch 27/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2065 - val_loss: 0.2091\n",
      "Epoch 28/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2065 - val_loss: 0.2090\n",
      "Epoch 29/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2064 - val_loss: 0.2089\n",
      "Epoch 30/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2060 - val_loss: 0.2093\n",
      "Epoch 31/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2064 - val_loss: 0.2094\n",
      "Epoch 32/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2062 - val_loss: 0.2093\n",
      "Epoch 33/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2061 - val_loss: 0.2091\n",
      "Epoch 34/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2051 - val_loss: 0.2087\n",
      "Epoch 35/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2048 - val_loss: 0.2087\n",
      "Epoch 36/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2057 - val_loss: 0.2089\n",
      "Epoch 37/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2051 - val_loss: 0.2090\n",
      "Epoch 38/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2051 - val_loss: 0.2090\n",
      "Epoch 39/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2049 - val_loss: 0.2089\n",
      "Epoch 40/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2045 - val_loss: 0.2090\n",
      "Epoch 41/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2052 - val_loss: 0.2089\n",
      "Epoch 42/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2050 - val_loss: 0.2088\n",
      "Epoch 43/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2054 - val_loss: 0.2086\n",
      "Epoch 44/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2055 - val_loss: 0.2088\n",
      "Epoch 45/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2052 - val_loss: 0.2088\n",
      "Epoch 46/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2059 - val_loss: 0.2087\n",
      "Epoch 47/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2053 - val_loss: 0.2086\n",
      "Epoch 48/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2041 - val_loss: 0.2087\n",
      "Epoch 49/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2049 - val_loss: 0.2087\n",
      "Epoch 50/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2048 - val_loss: 0.2088\n",
      "Epoch 51/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2059 - val_loss: 0.2088\n",
      "Epoch 52/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2052 - val_loss: 0.2087\n",
      "Epoch 53/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2049 - val_loss: 0.2087\n",
      "Epoch 54/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2054 - val_loss: 0.2088\n",
      "Epoch 55/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2052 - val_loss: 0.2088\n",
      "Epoch 56/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2048 - val_loss: 0.2087\n",
      "Epoch 57/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2043 - val_loss: 0.2087\n",
      "Epoch 58/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2049 - val_loss: 0.2087\n",
      "Epoch 59/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2038 - val_loss: 0.2087\n",
      "Epoch 60/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2052 - val_loss: 0.2087\n",
      "Epoch 61/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2045 - val_loss: 0.2087\n",
      "Epoch 62/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2048 - val_loss: 0.2087\n",
      "Epoch 63/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2048 - val_loss: 0.2087\n",
      "Epoch 64/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2044 - val_loss: 0.2087\n",
      "Epoch 65/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2049 - val_loss: 0.2087\n",
      "Epoch 66/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2050 - val_loss: 0.2087\n",
      "Epoch 67/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2042 - val_loss: 0.2087\n",
      "Fold 1 NN: 0.20864\n",
      "CV 2/5\n",
      "Epoch 1/1000\n",
      "168/168 [==============================] - 2s 6ms/step - loss: 26.5246 - val_loss: 0.5258\n",
      "Epoch 2/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.9617 - val_loss: 0.9190\n",
      "Epoch 3/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.7861 - val_loss: 1.6192\n",
      "Epoch 4/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.5811 - val_loss: 0.2467\n",
      "Epoch 5/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.9222 - val_loss: 0.6399\n",
      "Epoch 6/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.6707 - val_loss: 0.5843\n",
      "Epoch 7/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.5969 - val_loss: 0.4231\n",
      "Epoch 8/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.5533 - val_loss: 0.4660\n",
      "Epoch 9/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.5265 - val_loss: 0.7639\n",
      "Epoch 10/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.6082 - val_loss: 0.2655\n",
      "Epoch 11/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2639 - val_loss: 0.2337\n",
      "Epoch 12/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2716 - val_loss: 0.2650\n",
      "Epoch 13/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2687 - val_loss: 0.2441\n",
      "Epoch 14/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2717 - val_loss: 0.3416\n",
      "Epoch 15/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.3597 - val_loss: 0.5037\n",
      "Epoch 16/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 1.1082 - val_loss: 0.2793\n",
      "Epoch 17/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2581 - val_loss: 0.2253\n",
      "Epoch 18/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2356 - val_loss: 0.2533\n",
      "Epoch 19/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2455 - val_loss: 0.3288\n",
      "Epoch 20/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2432 - val_loss: 0.2389\n",
      "Epoch 21/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2428 - val_loss: 0.2807\n",
      "Epoch 22/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2404 - val_loss: 0.2308\n",
      "Epoch 23/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2531 - val_loss: 0.2335\n",
      "Epoch 24/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2519 - val_loss: 0.2655\n",
      "Epoch 25/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2138 - val_loss: 0.2172\n",
      "Epoch 26/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2076 - val_loss: 0.2151\n",
      "Epoch 27/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2069 - val_loss: 0.2165\n",
      "Epoch 28/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2071 - val_loss: 0.2149\n",
      "Epoch 29/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2055 - val_loss: 0.2132\n",
      "Epoch 30/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2056 - val_loss: 0.2156\n",
      "Epoch 31/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2059 - val_loss: 0.2137\n",
      "Epoch 32/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2064 - val_loss: 0.2148\n",
      "Epoch 33/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2069 - val_loss: 0.2154\n",
      "Epoch 34/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2063 - val_loss: 0.2176\n",
      "Epoch 35/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2047 - val_loss: 0.2135\n",
      "Epoch 36/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2066 - val_loss: 0.2182\n",
      "Epoch 37/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2041 - val_loss: 0.2128\n",
      "Epoch 38/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2031 - val_loss: 0.2147\n",
      "Epoch 39/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2028 - val_loss: 0.2123\n",
      "Epoch 40/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2023 - val_loss: 0.2131\n",
      "Epoch 41/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2025 - val_loss: 0.2129\n",
      "Epoch 42/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2024 - val_loss: 0.2115\n",
      "Epoch 43/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2030 - val_loss: 0.2134\n",
      "Epoch 44/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2023 - val_loss: 0.2128\n",
      "Epoch 45/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2023 - val_loss: 0.2130\n",
      "Epoch 46/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2026 - val_loss: 0.2145\n",
      "Epoch 47/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2017 - val_loss: 0.2117\n",
      "Epoch 48/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2024 - val_loss: 0.2129\n",
      "Epoch 49/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2023 - val_loss: 0.2143\n",
      "Epoch 50/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2009 - val_loss: 0.2121\n",
      "Epoch 51/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2008 - val_loss: 0.2122\n",
      "Epoch 52/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2009 - val_loss: 0.2117\n",
      "Epoch 53/1000\n",
      "168/168 [==============================] - 1s 4ms/step - loss: 0.2004 - val_loss: 0.2114\n",
      "Epoch 54/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2016 - val_loss: 0.2116\n",
      "Epoch 55/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2008 - val_loss: 0.2122\n",
      "Epoch 56/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2005 - val_loss: 0.2121\n",
      "Epoch 57/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2011 - val_loss: 0.2120\n",
      "Epoch 58/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2008 - val_loss: 0.2124\n",
      "Epoch 59/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2007 - val_loss: 0.2118\n",
      "Epoch 60/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2007 - val_loss: 0.2126\n",
      "Epoch 61/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2003 - val_loss: 0.2123\n",
      "Epoch 62/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2000 - val_loss: 0.2123\n",
      "Epoch 63/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2006 - val_loss: 0.2122\n",
      "Epoch 64/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.1999 - val_loss: 0.2123\n",
      "Epoch 65/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2005 - val_loss: 0.2120\n",
      "Epoch 66/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2009 - val_loss: 0.2124\n",
      "Epoch 67/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2002 - val_loss: 0.2122\n",
      "Epoch 68/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2006 - val_loss: 0.2122\n",
      "Epoch 69/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2004 - val_loss: 0.2119\n",
      "Epoch 70/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2001 - val_loss: 0.2119\n",
      "Epoch 71/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2000 - val_loss: 0.2118\n",
      "Epoch 72/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2000 - val_loss: 0.2119\n",
      "Epoch 73/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.1999 - val_loss: 0.2118\n",
      "Fold 2 NN: 0.2114\n",
      "CV 3/5\n",
      "Epoch 1/1000\n",
      "168/168 [==============================] - 2s 7ms/step - loss: 26.7937 - val_loss: 0.4444\n",
      "Epoch 2/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.5057 - val_loss: 0.9192\n",
      "Epoch 3/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.7141 - val_loss: 0.6022\n",
      "Epoch 4/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.6121 - val_loss: 0.6478\n",
      "Epoch 5/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.7159 - val_loss: 0.4603\n",
      "Epoch 6/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.5201 - val_loss: 0.5817\n",
      "Epoch 7/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.5257 - val_loss: 0.4728\n",
      "Epoch 8/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.5267 - val_loss: 0.8730\n",
      "Epoch 9/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.3308 - val_loss: 0.2295\n",
      "Epoch 10/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2212 - val_loss: 0.2174\n",
      "Epoch 11/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2175 - val_loss: 0.2189\n",
      "Epoch 12/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2173 - val_loss: 0.2166\n",
      "Epoch 13/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2173 - val_loss: 0.2248\n",
      "Epoch 14/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2154 - val_loss: 0.2154\n",
      "Epoch 15/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2151 - val_loss: 0.2147\n",
      "Epoch 16/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2148 - val_loss: 0.2186\n",
      "Epoch 17/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2148 - val_loss: 0.2164\n",
      "Epoch 18/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2149 - val_loss: 0.2157\n",
      "Epoch 19/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2161 - val_loss: 0.2169\n",
      "Epoch 20/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2133 - val_loss: 0.2152\n",
      "Epoch 21/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2127 - val_loss: 0.2322\n",
      "Epoch 22/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2178 - val_loss: 0.2145\n",
      "Epoch 23/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2240 - val_loss: 0.2332\n",
      "Epoch 24/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2212 - val_loss: 0.2176\n",
      "Epoch 25/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2186 - val_loss: 0.2204\n",
      "Epoch 26/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2152 - val_loss: 0.2126\n",
      "Epoch 27/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2148 - val_loss: 0.2156\n",
      "Epoch 28/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2221 - val_loss: 0.2303\n",
      "Epoch 29/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2347 - val_loss: 0.2209\n",
      "Epoch 30/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2170 - val_loss: 0.2248\n",
      "Epoch 31/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2239 - val_loss: 0.2238\n",
      "Epoch 32/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2238 - val_loss: 0.2237\n",
      "Epoch 33/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2263 - val_loss: 0.2194\n",
      "Epoch 34/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2086 - val_loss: 0.2113\n",
      "Epoch 35/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2055 - val_loss: 0.2122\n",
      "Epoch 36/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2049 - val_loss: 0.2106\n",
      "Epoch 37/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2038 - val_loss: 0.2134\n",
      "Epoch 38/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2041 - val_loss: 0.2116\n",
      "Epoch 39/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2032 - val_loss: 0.2110\n",
      "Epoch 40/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2041 - val_loss: 0.2118\n",
      "Epoch 41/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2036 - val_loss: 0.2130\n",
      "Epoch 42/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2055 - val_loss: 0.2124\n",
      "Epoch 43/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2038 - val_loss: 0.2121\n",
      "Epoch 44/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2019 - val_loss: 0.2112\n",
      "Epoch 45/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2019 - val_loss: 0.2111\n",
      "Epoch 46/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2019 - val_loss: 0.2109\n",
      "Epoch 47/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2021 - val_loss: 0.2113\n",
      "Epoch 48/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2018 - val_loss: 0.2111\n",
      "Epoch 49/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2016 - val_loss: 0.2116\n",
      "Epoch 50/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2021 - val_loss: 0.2110\n",
      "Epoch 51/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2015 - val_loss: 0.2109\n",
      "Epoch 52/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2015 - val_loss: 0.2109\n",
      "Epoch 53/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2023 - val_loss: 0.2111\n",
      "Epoch 54/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2010 - val_loss: 0.2110\n",
      "Epoch 55/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2010 - val_loss: 0.2109\n",
      "Epoch 56/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2012 - val_loss: 0.2111\n",
      "Fold 3 NN: 0.2106\n",
      "CV 4/5\n",
      "Epoch 1/1000\n",
      "168/168 [==============================] - 2s 6ms/step - loss: 36.1687 - val_loss: 1.7290\n",
      "Epoch 2/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.9452 - val_loss: 0.6192\n",
      "Epoch 3/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.5791 - val_loss: 0.2728\n",
      "Epoch 4/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.3644 - val_loss: 0.6626\n",
      "Epoch 5/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.7535 - val_loss: 0.6480\n",
      "Epoch 6/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.6418 - val_loss: 0.5573\n",
      "Epoch 7/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.6352 - val_loss: 0.4839\n",
      "Epoch 8/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.5614 - val_loss: 0.4846\n",
      "Epoch 9/1000\n",
      "168/168 [==============================] - 1s 4ms/step - loss: 0.4977 - val_loss: 0.4059\n",
      "Epoch 10/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.4669 - val_loss: 0.4346\n",
      "Epoch 11/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2411 - val_loss: 0.2320\n",
      "Epoch 12/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2158 - val_loss: 0.2227\n",
      "Epoch 13/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2137 - val_loss: 0.2230\n",
      "Epoch 14/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2138 - val_loss: 0.2300\n",
      "Epoch 15/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2137 - val_loss: 0.2253\n",
      "Epoch 16/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2126 - val_loss: 0.2287\n",
      "Epoch 17/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2141 - val_loss: 0.2239\n",
      "Epoch 18/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2125 - val_loss: 0.2213\n",
      "Epoch 19/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2105 - val_loss: 0.2244\n",
      "Epoch 20/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2119 - val_loss: 0.2247\n",
      "Epoch 21/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2142 - val_loss: 0.2229\n",
      "Epoch 22/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2117 - val_loss: 0.2590\n",
      "Epoch 23/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2242 - val_loss: 0.2207\n",
      "Epoch 24/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2127 - val_loss: 0.2217\n",
      "Epoch 25/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2108 - val_loss: 0.2268\n",
      "Epoch 26/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2110 - val_loss: 0.2218\n",
      "Epoch 27/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2117 - val_loss: 0.2292\n",
      "Epoch 28/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2156 - val_loss: 0.2441\n",
      "Epoch 29/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2166 - val_loss: 0.2254\n",
      "Epoch 30/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2174 - val_loss: 0.2197\n",
      "Epoch 31/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2180 - val_loss: 0.2176\n",
      "Epoch 32/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2118 - val_loss: 0.2219\n",
      "Epoch 33/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2137 - val_loss: 0.2224\n",
      "Epoch 34/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2183 - val_loss: 0.2214\n",
      "Epoch 35/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2167 - val_loss: 0.2350\n",
      "Epoch 36/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2259 - val_loss: 0.2220\n",
      "Epoch 37/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2174 - val_loss: 0.2186\n",
      "Epoch 38/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2165 - val_loss: 0.2220\n",
      "Epoch 39/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2040 - val_loss: 0.2165\n",
      "Epoch 40/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2031 - val_loss: 0.2148\n",
      "Epoch 41/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2027 - val_loss: 0.2162\n",
      "Epoch 42/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2023 - val_loss: 0.2165\n",
      "Epoch 43/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2025 - val_loss: 0.2157\n",
      "Epoch 44/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2022 - val_loss: 0.2165\n",
      "Epoch 45/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2016 - val_loss: 0.2156\n",
      "Epoch 46/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2028 - val_loss: 0.2171\n",
      "Epoch 47/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2024 - val_loss: 0.2161\n",
      "Epoch 48/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2007 - val_loss: 0.2161\n",
      "Epoch 49/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2003 - val_loss: 0.2155\n",
      "Epoch 50/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2005 - val_loss: 0.2161\n",
      "Epoch 51/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.1999 - val_loss: 0.2158\n",
      "Epoch 52/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2000 - val_loss: 0.2160\n",
      "Epoch 53/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2007 - val_loss: 0.2156\n",
      "Epoch 54/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2005 - val_loss: 0.2159\n",
      "Epoch 55/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2008 - val_loss: 0.2159\n",
      "Epoch 56/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.1997 - val_loss: 0.2159\n",
      "Epoch 57/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.1996 - val_loss: 0.2158\n",
      "Epoch 58/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2001 - val_loss: 0.2157\n",
      "Epoch 59/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.1997 - val_loss: 0.2161\n",
      "Epoch 60/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2006 - val_loss: 0.2162\n",
      "Fold 4 NN: 0.21479\n",
      "CV 5/5\n",
      "Epoch 1/1000\n",
      "168/168 [==============================] - 2s 7ms/step - loss: 26.8871 - val_loss: 0.7082\n",
      "Epoch 2/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.6453 - val_loss: 0.3986\n",
      "Epoch 3/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.3851 - val_loss: 0.6807\n",
      "Epoch 4/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.6310 - val_loss: 0.6810\n",
      "Epoch 5/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.6115 - val_loss: 0.6245\n",
      "Epoch 6/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.5666 - val_loss: 0.4766\n",
      "Epoch 7/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.4760 - val_loss: 0.4548\n",
      "Epoch 8/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.4631 - val_loss: 0.5555\n",
      "Epoch 9/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.4303 - val_loss: 0.2458\n",
      "Epoch 10/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2413 - val_loss: 0.2234\n",
      "Epoch 11/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.8841 - val_loss: 0.2267\n",
      "Epoch 12/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2433 - val_loss: 0.2270\n",
      "Epoch 13/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2269 - val_loss: 0.2572\n",
      "Epoch 14/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2698 - val_loss: 0.2267\n",
      "Epoch 15/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2440 - val_loss: 0.3089\n",
      "Epoch 16/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2529 - val_loss: 0.2414\n",
      "Epoch 17/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2441 - val_loss: 0.2216\n",
      "Epoch 18/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2491 - val_loss: 0.2559\n",
      "Epoch 19/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2468 - val_loss: 0.3028\n",
      "Epoch 20/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2500 - val_loss: 0.2741\n",
      "Epoch 21/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.4284 - val_loss: 0.2389\n",
      "Epoch 22/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2466 - val_loss: 0.2954\n",
      "Epoch 23/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2720 - val_loss: 0.2348\n",
      "Epoch 24/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2371 - val_loss: 0.2364\n",
      "Epoch 25/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2138 - val_loss: 0.2177\n",
      "Epoch 26/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2066 - val_loss: 0.2151\n",
      "Epoch 27/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2069 - val_loss: 0.2198\n",
      "Epoch 28/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2062 - val_loss: 0.2152\n",
      "Epoch 29/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2052 - val_loss: 0.2142\n",
      "Epoch 30/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2050 - val_loss: 0.2151\n",
      "Epoch 31/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2043 - val_loss: 0.2161\n",
      "Epoch 32/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2075 - val_loss: 0.2142\n",
      "Epoch 33/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2057 - val_loss: 0.2146\n",
      "Epoch 34/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2051 - val_loss: 0.2137\n",
      "Epoch 35/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2050 - val_loss: 0.2136\n",
      "Epoch 36/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2042 - val_loss: 0.2169\n",
      "Epoch 37/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2042 - val_loss: 0.2180\n",
      "Epoch 38/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2073 - val_loss: 0.2144\n",
      "Epoch 39/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2040 - val_loss: 0.2143\n",
      "Epoch 40/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2058 - val_loss: 0.2201\n",
      "Epoch 41/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2093 - val_loss: 0.2141\n",
      "Epoch 42/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2045 - val_loss: 0.2233\n",
      "Epoch 43/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2036 - val_loss: 0.2141\n",
      "Epoch 44/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2011 - val_loss: 0.2130\n",
      "Epoch 45/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2009 - val_loss: 0.2127\n",
      "Epoch 46/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2009 - val_loss: 0.2131\n",
      "Epoch 47/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2010 - val_loss: 0.2134\n",
      "Epoch 48/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2012 - val_loss: 0.2130\n",
      "Epoch 49/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2016 - val_loss: 0.2146\n",
      "Epoch 50/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2009 - val_loss: 0.2137\n",
      "Epoch 51/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2006 - val_loss: 0.2131\n",
      "Epoch 52/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2009 - val_loss: 0.2130\n",
      "Epoch 53/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2000 - val_loss: 0.2127\n",
      "Epoch 54/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.1995 - val_loss: 0.2127\n",
      "Epoch 55/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2005 - val_loss: 0.2137\n",
      "Epoch 56/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.1994 - val_loss: 0.2129\n",
      "Epoch 57/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.1993 - val_loss: 0.2128\n",
      "Epoch 58/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2003 - val_loss: 0.2132\n",
      "Epoch 59/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.1987 - val_loss: 0.2130\n",
      "Epoch 60/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.1991 - val_loss: 0.2129\n",
      "Epoch 61/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.1991 - val_loss: 0.2125\n",
      "Epoch 62/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.1996 - val_loss: 0.2125\n",
      "Epoch 63/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.1987 - val_loss: 0.2126\n",
      "Epoch 64/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.1988 - val_loss: 0.2126\n",
      "Epoch 65/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.1992 - val_loss: 0.2125\n",
      "Epoch 66/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.1991 - val_loss: 0.2128\n",
      "Epoch 67/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.1992 - val_loss: 0.2128\n",
      "Epoch 68/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.1995 - val_loss: 0.2128\n",
      "Epoch 69/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.1994 - val_loss: 0.2128\n",
      "Epoch 70/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.1988 - val_loss: 0.2127\n",
      "Epoch 71/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.1991 - val_loss: 0.2128\n",
      "Epoch 72/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.1989 - val_loss: 0.2127\n",
      "Epoch 73/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2005 - val_loss: 0.2129\n",
      "Epoch 74/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.1992 - val_loss: 0.2127\n",
      "Epoch 75/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.1994 - val_loss: 0.2128\n",
      "Epoch 76/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.1994 - val_loss: 0.2128\n",
      "Epoch 77/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.1994 - val_loss: 0.2127\n",
      "Epoch 78/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.1988 - val_loss: 0.2128\n",
      "Epoch 79/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.1989 - val_loss: 0.2128\n",
      "Epoch 80/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.1993 - val_loss: 0.2127\n",
      "Epoch 81/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.1986 - val_loss: 0.2128\n",
      "Epoch 82/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.1990 - val_loss: 0.2128\n",
      "Epoch 83/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2000 - val_loss: 0.2128\n",
      "Epoch 84/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.1994 - val_loss: 0.2128\n",
      "Epoch 85/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.1992 - val_loss: 0.2128\n",
      "Fold 5 NN: 0.21252\n"
     ]
    }
   ],
   "source": [
    "plateau = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss', factor=0.19, patience=7, verbose=0,\n",
    "    mode='min')\n",
    "\n",
    "es = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', patience=20, verbose=0,\n",
    "    mode='min',restore_best_weights=True)\n",
    "\n",
    "target_name='target'\n",
    "scores_folds = {}\n",
    "model_name = 'NN'\n",
    "pred_name = 'pred_{}'.format(model_name)\n",
    "\n",
    "n_folds = 5\n",
    "kf = model_selection.KFold(n_splits=n_folds, shuffle=True, random_state=2020)\n",
    "scores_folds[model_name] = []\n",
    "counter = 1\n",
    "\n",
    "features_to_consider = list(train_nn)\n",
    "\n",
    "features_to_consider.remove('time_id')\n",
    "features_to_consider.remove('target')\n",
    "try:\n",
    "    features_to_consider.remove('pred_NN')\n",
    "except:\n",
    "    pass\n",
    "\n",
    "\n",
    "train_nn[features_to_consider] = train_nn[features_to_consider].fillna(train_nn[features_to_consider].mean())\n",
    "test_nn[features_to_consider] = test_nn[features_to_consider].fillna(train_nn[features_to_consider].mean())\n",
    "\n",
    "train_nn[pred_name] = 0\n",
    "test_nn[target_name] = 0\n",
    "test_predictions_nn = np.zeros(test_nn.shape[0])\n",
    "\n",
    "for n_count in range(n_folds):\n",
    "    print('CV {}/{}'.format(counter, n_folds))\n",
    "    \n",
    "    indexes = np.arange(nfolds).astype(int)    \n",
    "    indexes = np.delete(indexes,obj=n_count, axis=0) \n",
    "    \n",
    "    indexes = np.r_[values[indexes[0]],values[indexes[1]],values[indexes[2]],values[indexes[3]]]\n",
    "    \n",
    "    X_train = train_nn.loc[train_nn.time_id.isin(indexes), features_to_consider]\n",
    "    y_train = train_nn.loc[train_nn.time_id.isin(indexes), target_name]\n",
    "    X_test = train_nn.loc[train_nn.time_id.isin(values[n_count]), features_to_consider]\n",
    "    y_test = train_nn.loc[train_nn.time_id.isin(values[n_count]), target_name]\n",
    "    \n",
    "    #############################################################################################\n",
    "    # NN\n",
    "    #############################################################################################\n",
    "    \n",
    "    model = base_model()\n",
    "    \n",
    "    model.compile(\n",
    "        keras.optimizers.Adam(learning_rate=0.006),\n",
    "        loss=root_mean_squared_per_error\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        features_to_consider.remove('stock_id')\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    num_data = X_train[features_to_consider]\n",
    "    \n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1))         \n",
    "    num_data = scaler.fit_transform(num_data.values)    \n",
    "    \n",
    "    cat_data = X_train['stock_id']    \n",
    "    target =  y_train\n",
    "    \n",
    "    num_data_test = X_test[features_to_consider]\n",
    "    num_data_test = scaler.transform(num_data_test.values)\n",
    "    cat_data_test = X_test['stock_id']\n",
    "\n",
    "    model.fit([cat_data, num_data], \n",
    "              target,               \n",
    "              batch_size=2048,\n",
    "              epochs=1000,\n",
    "              validation_data=([cat_data_test, num_data_test], y_test),\n",
    "              callbacks=[es, plateau],\n",
    "              validation_batch_size=len(y_test),\n",
    "              shuffle=True,\n",
    "             verbose = 1)\n",
    "\n",
    "    preds = model.predict([cat_data_test, num_data_test]).reshape(1,-1)[0]\n",
    "    \n",
    "    score = round(rmspe(y_true = y_test, y_pred = preds),5)\n",
    "    print('Fold {} {}: {}'.format(counter, model_name, score))\n",
    "    scores_folds[model_name].append(score)\n",
    "    \n",
    "    tt =scaler.transform(test_nn[features_to_consider].values)\n",
    "    #test_nn[target_name] += model.predict([test_nn['stock_id'], tt]).reshape(1,-1)[0].clip(0,1e10)\n",
    "    test_predictions_nn += model.predict([test_nn['stock_id'], tt]).reshape(1,-1)[0].clip(0,1e10)/n_folds\n",
    "    #test[target_name] += model.predict([test['stock_id'], test[features_to_consider]]).reshape(1,-1)[0].clip(0,1e10)\n",
    "       \n",
    "    counter += 1\n",
    "    features_to_consider.append('stock_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "220d94db",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-16T18:35:49.728526Z",
     "iopub.status.busy": "2023-12-16T18:35:49.727174Z",
     "iopub.status.idle": "2023-12-16T18:35:49.748887Z",
     "shell.execute_reply": "2023-12-16T18:35:49.748235Z",
     "shell.execute_reply.started": "2023-12-15T13:07:49.143151Z"
    },
    "papermill": {
     "duration": 1.671539,
     "end_time": "2023-12-16T18:35:49.749018",
     "exception": false,
     "start_time": "2023-12-16T18:35:48.077479",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSPE NN: 1.0 - Folds: [0.20864, 0.2114, 0.2106, 0.21479, 0.21252]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0-4</td>\n",
       "      <td>0.004207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0-32</td>\n",
       "      <td>0.002332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0-34</td>\n",
       "      <td>0.002332</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  row_id    target\n",
       "0    0-4  0.004207\n",
       "1   0-32  0.002332\n",
       "2   0-34  0.002332"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_nn[\"row_id\"] = test_nn[\"stock_id\"].astype(str) + \"-\" + test_nn[\"time_id\"].astype(str) \n",
    "#test_nn[target_name] = (test_predictions_nn*0.455+predictions_lgb_1*0.265+preds_tab*0.3)\n",
    "test_nn[target_name] = test_predictions_nn\n",
    "\n",
    "\n",
    "score = round(rmspe(y_true = train_nn[target_name].values, y_pred = train_nn[pred_name].values),5)\n",
    "print('RMSPE {}: {} - Folds: {}'.format(model_name, score, scores_folds[model_name]))\n",
    "\n",
    "display(test_nn[['row_id', target_name]].head(3))\n",
    "test_nn[['row_id', target_name]].to_csv('submission.csv',index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfdbca2c",
   "metadata": {
    "papermill": {
     "duration": 1.657891,
     "end_time": "2023-12-16T18:35:53.078832",
     "exception": false,
     "start_time": "2023-12-16T18:35:51.420941",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "It's a fork from https://www.kaggle.com/alexioslyon/lgbm-baseline and other great kaggles.thanks a lot!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "489001e8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-16T18:35:56.444246Z",
     "iopub.status.busy": "2023-12-16T18:35:56.443332Z",
     "iopub.status.idle": "2023-12-16T18:35:56.446292Z",
     "shell.execute_reply": "2023-12-16T18:35:56.445675Z"
    },
    "papermill": {
     "duration": 1.674676,
     "end_time": "2023-12-16T18:35:56.446424",
     "exception": false,
     "start_time": "2023-12-16T18:35:54.771748",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# total time is around 7:30"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 2344753,
     "sourceId": 27233,
     "sourceType": "competition"
    },
    {
     "datasetId": 921302,
     "sourceId": 6182088,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30120,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 4434.192584,
   "end_time": "2023-12-16T18:36:01.167041",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-12-16T17:22:06.974457",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
