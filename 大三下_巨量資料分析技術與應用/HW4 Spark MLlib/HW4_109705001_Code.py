# -*- coding: utf-8 -*-
"""HW4_109705001_Code.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1eqXgvKCR6gJbFbh1q55P6RfrPo4p8CWC
"""

from google.colab import drive
drive.mount('/content/gdrive')

# copy all files from Google drive to current directory
!cp -r ./gdrive/MyDrive/BigData/HW4/* .

!apt-get -y install openjdk-8-jre-headless
!pip install pyspark

# 讀資料
import pandas as pd
import numpy as np
import pyspark.pandas as ps
from pyspark import SparkConf
from pyspark.sql import SparkSession
from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType

# label
from pyspark.sql.functions import when
from pyspark.sql.functions import col, floor

# one hot encodeing
from pyspark.ml.feature import StringIndexer
from pyspark.ml.feature import OneHotEncoder

from pyspark.ml.feature import MinMaxScaler

# feature
from pyspark.ml.feature import VectorAssembler

# model
from pyspark.ml.classification import LogisticRegression
from pyspark.ml.tuning import ParamGridBuilder, CrossValidator
from pyspark.ml.evaluation import BinaryClassificationEvaluator
from pyspark.mllib.evaluation import BinaryClassificationMetrics

# evaluate
from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score

conf=SparkConf()
spark = SparkSession.builder.config(conf = conf).getOrCreate()

"""## Load Data ============================================================="""

customSchema = StructType([
    StructField("Year", IntegerType(), True),  
    StructField("Month", IntegerType(), True),  
    StructField("DayofMonth", IntegerType(), True),  
    StructField("DayOfWeek", IntegerType(), True),  
    StructField("DepTime", IntegerType(), True),  #
    StructField("CRSDepTime", IntegerType(), True),  
    StructField("ArrTime", IntegerType(), True),  #
    StructField("CRSArrTime", IntegerType(), True),  
    StructField("UniqueCarrier", StringType(), True), # 
    StructField("FlightNum", IntegerType(), True),
    StructField("TailNum", StringType(), True), 
    StructField("ActualElapsedTime", DoubleType(), True),  
    StructField("CRSElapsedTime", IntegerType(), True),  
    StructField("AirTime", DoubleType(), True),  
    StructField("ArrDelay", IntegerType(), True),  
    StructField("DepDelay", IntegerType(), True),  
    StructField("Origin", StringType(), True),  #
    StructField("Dest", StringType(), True),   #      
    StructField("Distance", DoubleType(), True), #
    StructField("TaxiIn", IntegerType(), True),
    StructField("TaxiOut", IntegerType(), True),         
    StructField("Cancelled", IntegerType(), True), 
    StructField("CancellationCode", StringType(), True),         
    StructField("Diverted", IntegerType(), True),
    StructField("CarrierDelay", IntegerType(), True),
    StructField("WeatherDelay", IntegerType(), True),         
    StructField("NASDelay", StringType(), True),
    StructField("SecurityDelay", IntegerType(), True),  
    StructField("LateAircraftDelay", IntegerType(), True)
])

df3=spark.read.csv('./data/2003.csv', header=True, schema=customSchema)
df4=spark.read.csv('./data/2004.csv', header=True, schema=customSchema)
df5=spark.read.csv('./data/2005.csv', header=True, schema=customSchema)

# df5.show()



"""## Filter features

"""

# Cancelled 就不會有arrdelay
df3.filter(df3.Cancelled==1).show(3)

# 公司號 會顯示在機尾號
# 同公司 同航線 不同機delay 不同
df3.filter(df3.UniqueCarrier=="UA").show(3)

# CRSElapsedTime = CRSArrTime - CRSDepTime
# ActualElapsedTime = ArrTime - SDepTime
# ArrDelay = ArrTime - CRSArrTime 不能同時放
df3.show(3)

# 同公司同行班 表定時間會不同
df3.filter((df3.FlightNum==1017) & (df3.UniqueCarrier=="UA")).groupby('CRSArrTime').count().show(10)

# 出發的delay不完全表示抵達會delay
total_df = total_df.withColumn("ArrDelay_vs_DepDelay", when(((total_df.ArrDelay>0) &  (total_df.DepDelay > 0)) , 'both delay')\
                            .when(((total_df.ArrDelay>0) &  (total_df.DepDelay <= 0)) , 'arr delay but dep not')\
                            .when(((total_df.ArrDelay<=0) &  (total_df.DepDelay <= 0)) , 'both not')\
                            .when(((total_df.ArrDelay<=0) &  (total_df.DepDelay > 0)) , 'arr not delay but dep delay'))
total_df.groupby(total_df.ArrDelay_vs_DepDelay).count().orderBy("count", ascending=False).show()

"""## Labeled"""

total_df = df3.union(df4).union(df5)
total_df = total_df.filter(total_df.Cancelled==0)
total_df = total_df.withColumn("IsDelay", when((total_df.ArrDelay>0) , 1).otherwise(0))

# total_df.show()

total_df.groupBy("IsDelay").count().show()

# Year DayOfWeek > cat CRSDepTime> cat CRSArrTime> cat TailNum> cat DepDelay> normalize Origin> cat Dest> cat

total_df = total_df.withColumn('ArrHour', floor(col('CRSArrTime') / 100).cast("integer"))
total_df = total_df.withColumn('DepHour', floor(col('CRSDepTime') / 100).cast("integer"))

total_df = total_df.select("Year","DayOfWeek",\
              "DepHour","ArrHour",\
              "TailNum",\
              "Origin","Dest",\
              "Distance","DepDelay",
              "IsDelay")

total_df = total_df.na.drop()

"""## Normalize"""

DepDelay_assembler = VectorAssembler().setInputCols(['DepDelay']).setOutputCol('DepDelay_vec')
Distance_assembler = VectorAssembler().setInputCols(['Distance']).setOutputCol('Distance_vec')
total_df = DepDelay_assembler.transform(total_df)
total_df = Distance_assembler.transform(total_df)
DepDelay_scaler = MinMaxScaler(inputCol="DepDelay_vec", outputCol="DepDelayStd")
Distance_scaler = MinMaxScaler(inputCol="Distance_vec", outputCol="DistanceStd")
total_df = DepDelay_scaler.fit(total_df).transform(total_df)
total_df = Distance_scaler.fit(total_df).transform(total_df)

"""## One Hot Encoder

"""

DayOfWeek_indexer  = StringIndexer(inputCol = "DayOfWeek",outputCol = "DayOfWeekIndex")
TailNum_indexer = StringIndexer(inputCol = "TailNum",outputCol = "TailNumIndex")
Dest_indexer = StringIndexer(inputCol = "Dest",outputCol = "DestIndex")
Origin_indexer = StringIndexer(inputCol = "Origin",outputCol = "OriginIndex")
DepHour_indexer = StringIndexer(inputCol = "DepHour",outputCol = "DepHourIndex")
ArrHour_indexer = StringIndexer(inputCol = "ArrHour",outputCol = "ArrHourIndex")

OHE = OneHotEncoder(inputCols =["DayOfWeekIndex","TailNumIndex","DestIndex","OriginIndex","DepHourIndex","ArrHourIndex"],\
           outputCols = ["DayOfWeekOHE","TailNumOHE","DestOHE","OriginOHE","DepHourOHE","ArrHourOHE"])

total_df = DayOfWeek_indexer.fit(total_df).transform(total_df)
total_df = TailNum_indexer.fit(total_df).transform(total_df)
total_df = Dest_indexer.fit(total_df).transform(total_df)
total_df = Origin_indexer.fit(total_df).transform(total_df)
total_df = DepHour_indexer.fit(total_df).transform(total_df)
total_df = ArrHour_indexer.fit(total_df).transform(total_df)

total_df = OHE.fit(total_df).transform(total_df)

total_df = total_df.select("Year","DayOfWeekOHE",\
              "DepHourOHE","ArrHourOHE",\
              "TailNumOHE",\
              "OriginOHE","DestOHE",\
              "DistanceStd","DepDelayStd","IsDelay")

total_df = total_df.fillna(0)

total_df.groupBy("IsDelay").count().show()

"""## Featuer Assemble"""

featureAssembler = VectorAssembler(inputCols=["Year","DayOfWeekOHE",\
                        "DepHourOHE","ArrHourOHE",\
                        "TailNumOHE",\
                        "OriginOHE","DestOHE",\
                        "DistanceStd","DepDelayStd"\
                        ],outputCol="features")

feature_df = featureAssembler.transform(total_df)
final_total_df = feature_df.select(["Year","features","IsDelay"])
final_total_df = final_total_df.withColumnRenamed("IsDelay", "label")

final_total_df.show(1)

"""## Train df & Test df"""

train = final_total_df.filter(final_total_df.Year != 2005).select(["features","label"])
test = final_total_df.filter(final_total_df.Year == 2005).select(["features","label"])

test.groupBy("label").count().show()

"""## Training

## DT================
"""

lr = LogisticRegression(labelCol="label", featuresCol="features")

lrParamGrid = (ParamGridBuilder()
        .addGrid(lr.regParam, [0.01, 0.1, 0.2])
        .addGrid(lr.elasticNetParam, [0, 0.25, 0.5, 0.75, 1.0])
        .addGrid(lr.maxIter, [5, 10, 15])
        .build())

lrEval = BinaryClassificationEvaluator(rawPredictionCol="rawPrediction")

lr_5fold = CrossValidator(estimator = lr,
              estimatorParamMaps = lrParamGrid,
              evaluator = lrEval,
              numFolds = 5)

lrModel = lr_5fold.fit(train)

lrPred = lrModel.transform(test)

y_true = lrPred.select("label")
y_true = y_true.toPandas()

y_pred = lrPred.select("prediction")
y_pred = y_pred.toPandas()

acc = accuracy_score(y_true, y_pred)
cnf_matrix = confusion_matrix(y_true, y_pred)
cnf_matrix

print(f"Accuracy: {acc:.4f}")



